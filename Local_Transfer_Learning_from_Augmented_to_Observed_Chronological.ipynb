{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pLfutwr-LKwV",
   "metadata": {
    "id": "pLfutwr-LKwV"
   },
   "source": [
    "# Transfer Learning to Observed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9456ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:24:26.067813Z",
     "start_time": "2023-01-26T00:24:26.062835Z"
    }
   },
   "outputs": [],
   "source": [
    "local_root_path= \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3z3YjNaVvZ0",
   "metadata": {
    "id": "w3z3YjNaVvZ0"
   },
   "source": [
    "## Define hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b6344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:24:26.083743Z",
     "start_time": "2023-01-26T00:24:26.070800Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Transfer_Learning_from_Augmented_to_Observed_Chronological.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "# Transfer Learning to Observed Data\n",
    "\n",
    "## Define hyper-parameters\n",
    "\"\"\"\n",
    "\n",
    "####################################################\n",
    "############## Hyper-param definitions #############\n",
    "####################################################\n",
    "\n",
    "''' \n",
    "Select NN architecture from:\n",
    "MLP, LSTM, GRU, ResNet, Residual_LSTM, Residual_GRU, Transformer\n",
    "'''\n",
    "model_type ='Residual_LSTM'\n",
    "\n",
    "''' \n",
    "Name of pre-trained models (to be transferred)\n",
    "'''\n",
    "pretrain_model_path_prefix='mtl_i118_residual_lstm_8_2_Tune_RSAC_RSAN'\n",
    "\n",
    "''' \n",
    "Numbers of neurons in the main branch\n",
    " - Provide two numbers for MLP, ResNet, Res-LSTM, Res-GRU, \n",
    " - Provide one number for LSTM, GRU.\n",
    " '''\n",
    "num_neurons_multiplier = [8,2]\n",
    "\n",
    "'''\n",
    "Number of training epochs (Note: training will stop when reaching this number \n",
    "or test loss doesn't decrease for 50 epochs)\n",
    "'''\n",
    "epochs = 500\n",
    "\n",
    "''' \n",
    "Whether to (True) train models from scratch or (False) evaluate pre-trained models\n",
    "'''\n",
    "train_models = True\n",
    "\n",
    "''' \n",
    "Whether to augment inputs (jittering and dropout)\n",
    "'''\n",
    "augmentation = False\n",
    "\n",
    "''' \n",
    "Training and test set, in years\n",
    "'''\n",
    "calib_slice = slice('2000', '2016')\n",
    "valid_slice = slice('2017', '2020')\n",
    "\n",
    "''' \n",
    "Learning rate multiplier\n",
    "Defined as ratio of learning rate between front layers and output layer\n",
    "'''\n",
    "lr_mult=0.1\n",
    "\n",
    "''' \n",
    "Model resolution\n",
    "- '1D': daily\n",
    "- '1h': hourly\n",
    "'''\n",
    "interval = '1D'\n",
    "\n",
    "''' \n",
    "Salinity forecasting lead time, unit is given by 'interval'\n",
    "'''\n",
    "lead_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RZffFrVYeP6C",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:24:27.684223Z",
     "start_time": "2023-01-26T00:24:26.085734Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3625,
     "status": "ok",
     "timestamp": 1670876317805,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "RZffFrVYeP6C",
    "outputId": "5c3996cf-8db4-48a0-a535-913fb24b94bb"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "lead_freq = re.split('(\\d+)',interval)[-1]\n",
    "\n",
    "if lead_time > 0:\n",
    "    forecast_marker = '_forecast_%dx%s' % (lead_time,interval)\n",
    "else:\n",
    "    forecast_marker = ''\n",
    "\n",
    "model_type = model_type.lower()\n",
    "assert (model_type in pretrain_model_path_prefix) or (model_type=='mlp' and 'd' in pretrain_model_path_prefix) or (model_type=='gru' and 'g' in pretrain_model_path_prefix)\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "given_name = '%s%s%s_hist' % (model_type, (('_'+interval) if interval.lower() != '1d' else ''), forecast_marker)\n",
    "\n",
    "\n",
    "\n",
    "if augmentation:\n",
    "    noise_sigma=0.03\n",
    "    dropout_ratio = 0.05\n",
    "else:\n",
    "    noise_sigma=0.\n",
    "    dropout_ratio = 0.\n",
    "\n",
    "group_stations = False\n",
    "initial_lr = 0.001\n",
    "data_file = \"observed_data_daily.xlsx\"\n",
    "\n",
    "print('Dataset: %s' % data_file)\n",
    "\n",
    "data_path = os.path.join(local_root_path,data_file)\n",
    "num_sheets = {\"observed_data_daily.xlsx\":9,\n",
    "              \"observed_data_hourly.xlsx\":9}\n",
    "sys.path.append(local_root_path)\n",
    "\n",
    "\n",
    "'''\n",
    "Define parameters for input pre-processing\n",
    "- ndays: number of daily values in inputs\n",
    "- window_size: length of averaging windows\n",
    "- nwindows: number of moving averages\n",
    "'''\n",
    "if model_type =='mlp':\n",
    "    # apply pre-defined average windowing:\n",
    "    ndays=8\n",
    "    window_size=11\n",
    "    nwindows=10\n",
    "else:\n",
    "    # directly use daily measurements as inputs\n",
    "    ndays=118\n",
    "    window_size=0\n",
    "    nwindows=0\n",
    "\n",
    "# percentile thresholds for ranged results\n",
    "percentiles = [0,0.75,0.95]  \n",
    "model_size = 'x'.join([str(n) for n in num_neurons_multiplier if n > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2voO3aN2LSsP",
   "metadata": {
    "id": "2voO3aN2LSsP"
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l9U0JyrTcWj6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:24:35.000821Z",
     "start_time": "2023-01-26T00:24:27.687210Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17094,
     "status": "ok",
     "timestamp": 1670876334893,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "l9U0JyrTcWj6",
    "outputId": "3c6f6c67-4dcd-4f95-c160-5c92fe24b39f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras import layers\n",
    "#import keras\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zi-vqBqpLhiU",
   "metadata": {
    "id": "Zi-vqBqpLhiU"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KhOaZzmkam28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:24:44.795885Z",
     "start_time": "2023-01-26T00:24:35.003807Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 16527,
     "status": "ok",
     "timestamp": 1670876351605,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "KhOaZzmkam28",
    "outputId": "cf4ff54f-5d31-452e-c958-71e8e97ef522"
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(local_root_path,data_file)\n",
    "dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets[data_file])]\n",
    "def load_data(data_file):\n",
    "    if data_file in [\"dsm2_ann_observed_1h.xlsx\",\"dsm2_ann_observed_15min.xlsx\"]:\n",
    "        dfouts=dflist[-1]\n",
    "        df2 = pd.pivot_table(dfouts, index=dfouts.index.date, columns=dfouts.index.hour)\n",
    "        df2.columns=df2.columns.get_level_values(0)\n",
    "        output_col_list = df2.columns\n",
    "        df_inpout = pd.concat(dflist[0:(num_sheets[data_file]-1)]+[df2],axis=1).dropna(axis=0)\n",
    "        dfinps = df_inpout.loc[:,~df_inpout.columns.isin(output_col_list)]\n",
    "        dfouts = df_inpout.loc[:,df_inpout.columns.isin(output_col_list)]\n",
    "    else:\n",
    "        df_inpout = pd.concat(dflist[0:(num_sheets[data_file])],axis=1).dropna(axis=0)\n",
    "        dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets[data_file]-1].columns)]\n",
    "        dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets[data_file]-1].columns)]\n",
    "    return dfinps, dfouts\n",
    "\n",
    "dfinps, dfouts = load_data(data_file)\n",
    "dfinps.head()\n",
    "\n",
    "dfouts\n",
    "\n",
    "output_locations_w_duplicates_for_time = dfouts.columns[~dfouts.columns.str.contains('_dup')]\n",
    "output_locations = set(output_locations_w_duplicates_for_time)\n",
    "print('Found %d stations' % len(output_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b598dae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:24:44.843187Z",
     "start_time": "2023-01-26T00:24:44.797876Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670876351606,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "3b598dae",
    "outputId": "3260da39-dd45-4545-8914-464570931521"
   },
   "outputs": [],
   "source": [
    "dfouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmDR0VennE5I",
   "metadata": {
    "id": "mmDR0VennE5I"
   },
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803eca3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:24:44.875047Z",
     "start_time": "2023-01-26T00:24:44.846174Z"
    },
    "id": "803eca3b"
   },
   "outputs": [],
   "source": [
    "\"\"\"# Custom loss function\"\"\"\n",
    "def mse_loss_masked(y_true, y_pred):\n",
    "    squared_diff = tf.reduce_sum(tf.math.squared_difference(y_pred[y_true>=0],y_true[y_true>=0]))\n",
    "    return squared_diff/(tf.reduce_sum(tf.cast(y_true>=0, tf.float32))+0.01)\n",
    "\n",
    "# Define Sequential model\n",
    "# LSTM model has one LSTM layer\n",
    "# MLP model has 3 Dense layers\n",
    "NFEATURES = (ndays + nwindows) * (num_sheets[data_file]-1)\n",
    "\n",
    "def build_lstm_model(lstm_units=8,lstm_units2=None, output_shape=1, act_func='sigmoid',layer_type='lstm'):\n",
    "    rnn_layer = layers.LSTM if layer_type == 'lstm' else layers.GRU\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(NFEATURES)),\n",
    "            layers.Reshape((ndays+nwindows,num_sheets[data_file]-1)),\n",
    "            layers.Permute((2,1)),\n",
    "            rnn_layer(units = lstm_units, activation=act_func,return_sequences=True),\n",
    "            # rnn_layer(units = lstm_units, activation=act_func),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(output_shape, activation=keras.activations.linear,name='output')\n",
    "        ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "def build_residual_lstm_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid',layer_type='lstm',conv_init=None):\n",
    "    rnn_layer = layers.LSTM if layer_type == 'lstm' else layers.GRU\n",
    "    inputs = layers.Input(shape=NFEATURES)\n",
    "    x = layers.Reshape((ndays+nwindows,num_sheets[data_file]-1))(inputs)\n",
    "    x = layers.Permute((2,1))(x)\n",
    "\n",
    "    y = tf.keras.layers.Conv1D(8+10,1, activation='relu',\n",
    "                            kernel_initializer=conv_init,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),\n",
    "                            trainable=False)(x)\n",
    "\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(nhidden1, activation=act_func)(y)\n",
    "    y = layers.Dense(nhidden2, activation=act_func)(y)\n",
    "    y = layers.Dense(output_shape, activation=keras.activations.linear,name='mlp_output')(y)\n",
    "\n",
    "    shortcut = x\n",
    "    shortcut = layers.Dense(nhidden1, activation=act_func)(shortcut)\n",
    "    shortcut = rnn_layer(units = output_shape*2, activation=act_func,return_sequences=True)(shortcut)\n",
    "    shortcut = layers.Flatten()(shortcut)\n",
    "    shortcut = layers.Dense(output_shape, activation=keras.activations.linear,name='lstm_output')(shortcut)\n",
    "\n",
    "    outputs = layers.Add(name=\"res_add\")([y, shortcut])\n",
    "    # outputs = layers.Activation(\"relu\",name=\"res_relu\")(outputs)\n",
    "    outputs = layers.LeakyReLU(alpha=0.3,name=\"res_relu\")(outputs)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid'):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(NFEATURES)),\n",
    "            layers.Dense(nhidden1, activation=act_func),\n",
    "            layers.Dense(nhidden2, activation=act_func),\n",
    "            layers.Dense(output_shape, activation=keras.activations.linear,name='output')\n",
    "        ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    #model.compile(optimizer=keras.optimizers.RMSprop(), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "def build_resnet_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid',filters=num_sheets[data_file]-1, kernel_size=3, stride=1):\n",
    "    inputs = layers.Input(shape=NFEATURES)\n",
    "    x = layers.Reshape((ndays+nwindows,num_sheets[data_file]-1))(inputs)\n",
    "\n",
    "    y = layers.ZeroPadding1D(padding=1,name=\"padding_branch2a\")(x)\n",
    "    y = layers.Conv1D(filters,kernel_size,strides=stride,use_bias=False,\n",
    "                                name=\"res_branch2a\")(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation(\"relu\", name=\"res_branch2a_relu\")(y)\n",
    "\n",
    "    y = layers.ZeroPadding1D(padding=1,name=\"padding_branch2b\")(y)\n",
    "    y = layers.Conv1D(filters,kernel_size,use_bias=False,\n",
    "                            name=\"res_branch2b\")(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(nhidden1, activation=act_func)(y)\n",
    "\n",
    "    shortcut = inputs\n",
    "    shortcut = layers.Dense(nhidden1, activation=act_func)(shortcut)\n",
    "\n",
    "    y = layers.Add(name=\"res\")([y, shortcut])\n",
    "    \n",
    "    y = layers.Activation(\"relu\",name=\"res_relu\")(y)\n",
    "\n",
    "\n",
    "    y = layers.Dense(nhidden2, activation=act_func)(y)\n",
    "    outputs= layers.Dense(output_shape, activation=keras.activations.linear,name='output')(y)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R0ix5jxYPi9k",
   "metadata": {
    "id": "R0ix5jxYPi9k"
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ac231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:26:42.961574Z",
     "start_time": "2023-01-26T00:24:44.877039Z"
    },
    "id": "d96ac231"
   },
   "outputs": [],
   "source": [
    "\"\"\"## Evaluation Metrics\"\"\"\n",
    "\n",
    "eval_metrics = ['MSE', 'Bias', 'R', 'RMSD', 'NSE']\n",
    "key_stations = ['RSAC064','CCWD_Rock','CHDMC006', 'CHSWP003','RSAC092','RSAN018']\n",
    "\n",
    "def evaluate_sequences(target, pred, metrics):\n",
    "    assert len(target) == len(pred), 'Target and predicted sequence length must equal.'\n",
    "    if len(target.shape)==1 or target.shape[-1]==1:                                     \n",
    "        valid_entries = target>=0\n",
    "    else:\n",
    "        valid_entries = (target>=0).all(axis=1)\n",
    "    sequence_length = np.sum(valid_entries)\n",
    "    if np.any(sequence_length == 0):\n",
    "        return {k: 0 for k in metrics}\n",
    "    target=target[valid_entries]\n",
    "    pred = pred[valid_entries]\n",
    "    SD_pred = np.sqrt( np.sum((pred-np.mean(pred)) ** 2) /(sequence_length-1))\n",
    "    SD_target = np.sqrt( np.sum((target-np.mean(target)) ** 2) /(sequence_length-1))\n",
    "\n",
    "    eval_results = defaultdict(float)\n",
    "    \n",
    "    for m in metrics:\n",
    "        if m =='MSE':\n",
    "            eval_results[m] = ((target - pred)**2).mean()\n",
    "        elif m =='Bias':\n",
    "            eval_results[m] = np.sum(pred - target)/np.sum(target) * 100\n",
    "        elif m == 'R':\n",
    "            eval_results[m] = np.sum(np.abs((pred-np.mean(pred)) * (target - np.mean(target)))) / (sequence_length * SD_pred * SD_target)\n",
    "        elif m == 'RMSD':\n",
    "            eval_results[m] = np.sqrt(np.sum( ( ( pred-np.mean(pred) ) * ( target - np.mean(target) ) ) ** 2 ) / sequence_length)\n",
    "        elif m == 'NSE':\n",
    "            eval_results[m] = 1 - np.sum( ( target - pred ) ** 2 ) / np.sum( (target - np.mean(target) ) ** 2 )\n",
    "    return eval_results\n",
    "\n",
    "import annutils\n",
    "result_folders = ['models','results','images']\n",
    "for result_folder in result_folders:\n",
    "    if not os.path.exists(os.path.join(local_root_path, result_folder)):\n",
    "        os.makedirs(os.path.join(local_root_path, result_folder))\n",
    "\n",
    "station_without_groups = {'all':output_locations}\n",
    "station_with_groups = {'G1':['SSS','RSAC101','RSMKL008'],\n",
    "                  'G2':['Old_River_Hwy_4','Middle_River_Intake','CCWD_Rock','SLTRM004','RSAN032','RSAN037','SLDUT007','ROLD024','RSAN058','RSAN072','OLD_MID','ROLD059','CHDMC006','CHSWP003','CHVCT000'],\n",
    "                  'G3':['SLCBN002','SLSUS012','SLMZU011','SLMZU025','RSAC064','RSAC075','RSAC081','RSAN007','RSAC092','RSAN018','Martinez']}\n",
    "final_groups = {False: station_without_groups, \n",
    "                True: station_with_groups}\n",
    "\n",
    "\"\"\"## Load Pre-Trained Models and Apply Transfer Learning\"\"\"\n",
    "\n",
    "for group_name, stations in final_groups[group_stations].items():\n",
    "    if not train_models:\n",
    "        break\n",
    "    selected_output_variables = []\n",
    "    for station in stations:\n",
    "        for output in output_locations:\n",
    "            if station in output:\n",
    "                selected_output_variables.append(output)\n",
    "\n",
    "\n",
    "    print('Training MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "    print([station.replace('target/','').replace('target','') for station in selected_output_variables])\n",
    "    \n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (xallc, yallc), (xallv, yallv), xscaler, yscaler = \\\n",
    "        annutils.create_training_sets([dfinps],\n",
    "                                      [dfouts[selected_output_variables]],\n",
    "                                      #train_frac=train_frac,\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                      noise_sigma=noise_sigma,dropout_ratio=dropout_ratio,\n",
    "                                      lead_time=lead_time,lead_freq=lead_freq)\n",
    "    if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "        try:\n",
    "            loaded_model = annutils.load_model(os.path.join(local_root_path,'models', pretrain_model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "            pretrained_model = loaded_model.model\n",
    "            print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "        except:\n",
    "            # only model weights are saved, build model then load\n",
    "            if model_type.lower() in ['lstm','gru']:\n",
    "                pretrained_model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                    num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                    output_shape=yallc.shape[1],\n",
    "                                                    act_func='sigmoid',\n",
    "                                                    layer_type=model_type.lower())\n",
    "            elif model_type.lower() == 'mlp':\n",
    "                pretrained_model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                               num_neurons_multiplier[1]*len(output_locations),\n",
    "                                               output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "            elif model_type.lower() =='resnet':\n",
    "                pretrained_model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                      num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                      output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "        \n",
    "            pretrained_model.load_weights(os.path.join(local_root_path,'models', pretrain_model_path_prefix+'.h5'))\n",
    "        \n",
    "        print('Transfer learning from pre-trained model %s' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "        o = keras.layers.Dense(yallc.shape[1], activation='linear',name='transfer_out')(pretrained_model.layers[-2].output)\n",
    "        model = keras.Model(inputs=pretrained_model.input, outputs=[o])\n",
    "\n",
    "        \n",
    "        optimizers = [\n",
    "            tf.keras.optimizers.Adam(learning_rate=lr_mult*initial_lr),\n",
    "            tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "        ]\n",
    "        optimizers_and_layers = [(optimizers[0], model.layers[0:-1]), (optimizers[1], model.layers[-1])]\n",
    "\n",
    "        model.compile(optimizer=tfa.optimizers.MultiOptimizer(optimizers_and_layers),\n",
    "                      loss=mse_loss_masked)\n",
    "    elif model_type.lower() in ['lstm','gru']:\n",
    "        model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                 num_neurons_multiplier[1]*len(output_locations),\n",
    "                                 output_shape=yallc.shape[1],\n",
    "                                 act_func='sigmoid',\n",
    "                                 layer_type=model_type.lower())\n",
    "    elif model_type.lower() == 'mlp':\n",
    "        model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                            num_neurons_multiplier[1]*len(output_locations),\n",
    "                            output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "    elif model_type.lower() =='resnet':\n",
    "        model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                   num_neurons_multiplier[1]*len(output_locations),\n",
    "                                   output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "    elif model_type.lower() in ['res-lstm','res-gru']:\n",
    "        conv_init = tf.constant_initializer(annutils.conv_filter_generator(ndays=8,window_size=11,nwindows=10))\n",
    "\n",
    "        model = build_residual_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                 num_neurons_multiplier[1]*len(output_locations),\n",
    "                                 output_shape=yallc.shape[1],\n",
    "                                 act_func='sigmoid',\n",
    "                                 layer_type=model_type.lower()[len('res-'):],\n",
    "                                 conv_init=conv_init)\n",
    "\n",
    "    model.summary()\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        xallc,\n",
    "        yallc,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        validation_data=(xallv, yallv),\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=50, mode=\"min\", restore_best_weights=True),\n",
    "        ],\n",
    "        verbose=0,\n",
    "    )\n",
    "    end_time=time.time()\n",
    "    print('Training time: %d min' % ((end_time-start_time)/60) )\n",
    "    model_save_path = os.path.join(local_root_path, 'models', (('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name)\n",
    "    if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "        joblib.dump((xscaler,yscaler),'%s_xyscaler.dump'%model_save_path)\n",
    "        model.save_weights('%s.h5'%model_save_path)\n",
    "    else:\n",
    "        annutils.save_model(model_save_path, model, xscaler, yscaler)\n",
    "    print('Model saved at %s.h5' % model_save_path)\n",
    "\n",
    "\"\"\"# Evaluation\n",
    "\n",
    "## Numerical Results\n",
    "\"\"\"\n",
    "\n",
    "full_results={}\n",
    "range_results=defaultdict(defaultdict)\n",
    "\n",
    "df_inpout = pd.concat(dflist[0:(num_sheets[data_file])],axis=1).dropna(axis=0)\n",
    "for group_name, stations in final_groups[group_stations].items():\n",
    "    # prepare dataset\n",
    "    selected_output_variables = []\n",
    "    for station in stations:\n",
    "        for output in output_locations:\n",
    "            if station in output:\n",
    "                selected_output_variables.append(output)\n",
    "    print('Testing MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "\n",
    "    print([station.replace('target/','').replace('target','') for station in selected_output_variables])\n",
    "\n",
    "    model_save_path = os.path.join(local_root_path, 'models', (('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name)\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (xallc, yallc), (xallv, yallv), xscaler, yscaler = \\\n",
    "        annutils.create_training_sets([dfinps],\n",
    "                                      [dfouts[selected_output_variables]],\n",
    "                                      #train_frac=train_frac,\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                      lead_time=lead_time,lead_freq=lead_freq)\n",
    "    try:\n",
    "        annmodel = annutils.load_model(model_save_path,\n",
    "                                       custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "        model = annmodel.model\n",
    "    except:\n",
    "        print('Unable to load saved model, rebuilding from pre-trained model...')\n",
    "        if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "            try:\n",
    "                loaded_model = annutils.load_model(os.path.join(local_root_path,'models', pretrain_model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "                pretrained_model = loaded_model.model\n",
    "                print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "            except:\n",
    "                # only model weights are saved, build model then load\n",
    "                if model_type.lower() in ['lstm','gru']:\n",
    "                    pretrained_model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                        num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                        output_shape=yallc.shape[1],\n",
    "                                                        act_func='sigmoid',\n",
    "                                                        layer_type=model_type.lower())\n",
    "                elif model_type.lower() == 'mlp':\n",
    "                    pretrained_model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                   num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                   output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "                elif model_type.lower() =='resnet':\n",
    "                    pretrained_model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations), \n",
    "                                                          num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                          output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "            \n",
    "            # remove last layer and attach new\n",
    "            o = keras.layers.Dense(yallc.shape[1], activation='linear',name='transfer_out')(pretrained_model.layers[-2].output)\n",
    "            model = keras.Model(inputs=pretrained_model.input, outputs=[o])\n",
    "\n",
    "        model.load_weights(model_save_path+'.h5')\n",
    "        print('Loaded model weights from %s.h5' % model_save_path)\n",
    "        xscaler,yscaler=joblib.load('%s_xyscaler.dump'%model_save_path)\n",
    "\n",
    "\n",
    "    train_pred = pd.DataFrame(np.clip(model.predict(xallc),0,1),yallc.index,columns=yallc.columns)\n",
    "    test_pred = pd.DataFrame(np.clip(model.predict(xallv),0,1),yallv.index,columns=yallv.columns)\n",
    "\n",
    "    for ii, location in enumerate(selected_output_variables):\n",
    "        \n",
    "        # compute training results\n",
    "        train_results = evaluate_sequences(yallc.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                           train_pred.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                           eval_metrics)\n",
    "        train_results['R^2'] = r2_score(train_pred.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                        yallc.loc[:,location].to_numpy().reshape(-1,1))\n",
    "        full_results['%s_train' %location] = train_results\n",
    "\n",
    "        # compute evaluation results\n",
    "        eval_results = evaluate_sequences(yallv.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                          test_pred.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                          eval_metrics)\n",
    "        eval_results['R^2'] = r2_score(test_pred.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                       yallv.loc[:,location].to_numpy().reshape(-1,1))\n",
    "\n",
    "        full_results['%s_test' %location] = eval_results\n",
    "\n",
    "        all_target = np.concatenate((yallc[location].to_numpy().reshape(-1,1),\n",
    "                                    yallv[location].to_numpy().reshape(-1,1)),axis=0)\n",
    "        all_pred = np.concatenate((train_pred[location].to_numpy().reshape(-1,1),\n",
    "                                    test_pred[location].to_numpy().reshape(-1,1)),axis=0)\n",
    "\n",
    "        # compute results at different EC ranges\n",
    "        for (lower_quantile, upper_quantile) in zip(percentiles,percentiles[1:]+[1,]):\n",
    "            lower_threshold = np.quantile(all_target, lower_quantile)\n",
    "            upper_threshold = np.quantile(all_target, upper_quantile)\n",
    "            eval_results = evaluate_sequences(all_target[(all_target > lower_threshold) & (all_target <= upper_threshold)],\n",
    "                                              all_pred[(all_target > lower_threshold) & (all_target <= upper_threshold)],\n",
    "                                              eval_metrics)\n",
    "            range_results[location][lower_quantile*100] = eval_results\n",
    "\n",
    "def add_subplot_axes(ax,rect,facecolor='w'): # matplotlib 2.0+\n",
    "    fig = plt.gcf()\n",
    "    box = ax.get_position()\n",
    "    width = box.width\n",
    "    height = box.height\n",
    "    inax_position  = ax.transAxes.transform(rect[0:2])\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    infig_position = transFigure.transform(inax_position)    \n",
    "    x = infig_position[0]\n",
    "    y = infig_position[1]\n",
    "    width *= rect[2]\n",
    "    height *= rect[3]\n",
    "    subax = fig.add_axes([x,y,width,height],facecolor=facecolor)  # matplotlib 2.0+\n",
    "    x_labelsize = subax.get_xticklabels()[0].get_size()\n",
    "    y_labelsize = subax.get_yticklabels()[0].get_size()\n",
    "    x_labelsize *= rect[2]**0.5\n",
    "    y_labelsize *= rect[3]**0.5\n",
    "    subax.xaxis.set_tick_params(labelsize=x_labelsize)\n",
    "    subax.yaxis.set_tick_params(labelsize=y_labelsize)\n",
    "    return subax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7244215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:26:42.976508Z",
     "start_time": "2023-01-26T00:26:42.965556Z"
    },
    "id": "b7244215"
   },
   "outputs": [],
   "source": [
    "import annutils\n",
    "result_folders = ['models','results','images']\n",
    "for result_folder in result_folders:\n",
    "    if not os.path.exists(os.path.join(local_root_path, result_folder)):\n",
    "        os.makedirs(os.path.join(local_root_path, result_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AuQjo5zt3Q12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:26:42.991489Z",
     "start_time": "2023-01-26T00:26:42.978499Z"
    },
    "id": "AuQjo5zt3Q12"
   },
   "outputs": [],
   "source": [
    "station_without_groups = {'all':output_locations}\n",
    "station_with_groups = {'G1':['SSS','RSAC101','RSMKL008'],\n",
    "                  'G2':['Old_River_Hwy_4','Middle_River_Intake','CCWD_Rock','SLTRM004','RSAN032','RSAN037','SLDUT007','ROLD024','RSAN058','RSAN072','OLD_MID','ROLD059','CHDMC006','CHSWP003','CHVCT000'],\n",
    "                  'G3':['SLCBN002','SLSUS012','SLMZU011','SLMZU025','RSAC064','RSAC075','RSAC081','RSAN007','RSAC092','RSAN018','Martinez']}\n",
    "final_groups = {False: station_without_groups, \n",
    "                True: station_with_groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H5l-Ym2VaoDh",
   "metadata": {
    "id": "H5l-Ym2VaoDh"
   },
   "source": [
    "## Load Pre-Trained Models and Apply Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2383d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:28:35.035218Z",
     "start_time": "2023-01-26T00:26:42.993990Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "executionInfo": {
     "elapsed": 12314,
     "status": "ok",
     "timestamp": 1670876364232,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "2be2383d",
    "outputId": "06dfadc3-6a78-4052-be8e-ee780b53f9d6"
   },
   "outputs": [],
   "source": [
    "for group_name, stations in final_groups[group_stations].items():\n",
    "    if not train_models:\n",
    "        break\n",
    "    selected_output_variables = []\n",
    "    for station in stations:\n",
    "        for output in output_locations:\n",
    "            if station in output:\n",
    "                selected_output_variables.append(output)\n",
    "\n",
    "\n",
    "    print('Training MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "    print([station.replace('target/','').replace('target','') for station in selected_output_variables])\n",
    "    \n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (xallc, yallc), (xallv, yallv), xscaler, yscaler = \\\n",
    "        annutils.create_training_sets([dfinps],\n",
    "                                      [dfouts[selected_output_variables]],\n",
    "                                      #train_frac=train_frac,\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                      noise_sigma=noise_sigma,dropout_ratio=dropout_ratio,\n",
    "                                      lead_time=lead_time,lead_freq=lead_freq)\n",
    "    if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "        try:\n",
    "            loaded_model = annutils.load_model(os.path.join(local_root_path,'models', pretrain_model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "            pretrained_model = loaded_model.model\n",
    "            print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "        except:\n",
    "            # only model weights are saved, build model then load\n",
    "            if model_type.lower() in ['lstm','gru']:\n",
    "                pretrained_model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                    num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                    output_shape=yallc.shape[1],\n",
    "                                                    act_func='sigmoid',\n",
    "                                                    layer_type=model_type.lower())\n",
    "            elif model_type.lower() == 'mlp':\n",
    "                pretrained_model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                               num_neurons_multiplier[1]*len(output_locations),\n",
    "                                               output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "            elif model_type.lower() =='resnet':\n",
    "                pretrained_model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                      num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                      output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "        \n",
    "            pretrained_model.load_weights(os.path.join(local_root_path,'models', pretrain_model_path_prefix+'.h5'))\n",
    "        \n",
    "        print('Transfer learning from pre-trained model %s' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "        o = keras.layers.Dense(yallc.shape[1], activation='linear',name='transfer_out')(pretrained_model.layers[-2].output)\n",
    "        model = keras.Model(inputs=pretrained_model.input, outputs=[o])\n",
    "\n",
    "        \n",
    "        optimizers = [\n",
    "            tf.keras.optimizers.Adam(learning_rate=lr_mult*initial_lr),\n",
    "            tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "        ]\n",
    "        optimizers_and_layers = [(optimizers[0], model.layers[0:-1]), (optimizers[1], model.layers[-1])]\n",
    "\n",
    "        model.compile(optimizer=tfa.optimizers.MultiOptimizer(optimizers_and_layers),\n",
    "                      loss=mse_loss_masked)\n",
    "    elif model_type.lower() in ['lstm','gru']:\n",
    "        model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                 num_neurons_multiplier[1]*len(output_locations),\n",
    "                                 output_shape=yallc.shape[1],\n",
    "                                 act_func='sigmoid',\n",
    "                                 layer_type=model_type.lower())\n",
    "    elif model_type.lower() == 'mlp':\n",
    "        model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                            num_neurons_multiplier[1]*len(output_locations),\n",
    "                            output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "    elif model_type.lower() =='resnet':\n",
    "        model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                   num_neurons_multiplier[1]*len(output_locations),\n",
    "                                   output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "    elif model_type.lower() in ['res-lstm','res-gru']:\n",
    "        conv_init = tf.constant_initializer(annutils.conv_filter_generator(ndays=8,window_size=11,nwindows=10))\n",
    "\n",
    "        model = build_residual_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                 num_neurons_multiplier[1]*len(output_locations),\n",
    "                                 output_shape=yallc.shape[1],\n",
    "                                 act_func='sigmoid',\n",
    "                                 layer_type=model_type.lower()[len('res-'):],\n",
    "                                 conv_init=conv_init)\n",
    "\n",
    "    display(model.summary())\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        xallc,\n",
    "        yallc,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        validation_data=(xallv, yallv),\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=50, mode=\"min\", restore_best_weights=True),\n",
    "        ],\n",
    "        verbose=0,\n",
    "    )\n",
    "    end_time=time.time()\n",
    "    print('Training time: %d min' % ((end_time-start_time)/60) )\n",
    "    model_save_path = os.path.join(local_root_path, 'models', (('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name)\n",
    "    if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "        joblib.dump((xscaler,yscaler),'%s_xyscaler.dump'%model_save_path)\n",
    "        model.save_weights('%s.h5'%model_save_path)\n",
    "    else:\n",
    "        annutils.save_model(model_save_path, model, xscaler, yscaler)\n",
    "    print('Model saved at %s.h5' % model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19pX3dRUwLQ0",
   "metadata": {
    "id": "19pX3dRUwLQ0"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5j9ZPkdRwfTd",
   "metadata": {
    "id": "5j9ZPkdRwfTd"
   },
   "source": [
    "## Numerical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P8lFDBnQFkJx",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:28:35.080021Z",
     "start_time": "2023-01-26T00:28:35.038205Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5367,
     "status": "ok",
     "timestamp": 1670876369591,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "P8lFDBnQFkJx",
    "outputId": "6507209f-423d-4394-fe5d-bc018717b0a0"
   },
   "outputs": [],
   "source": [
    "\"\"\"## Evaluation Metrics\"\"\"\n",
    "\n",
    "eval_metrics = ['MSE', 'Bias', 'R', 'RMSD', 'NSE']\n",
    "key_stations = ['RSAC064','CCWD_Rock','CHDMC006', 'CHSWP003','RSAC092','RSAN018']\n",
    "\n",
    "def evaluate_sequences(target, pred, metrics):\n",
    "    assert len(target) == len(pred), 'Target and predicted sequence length must equal.'\n",
    "    if len(target.shape)==1 or target.shape[-1]==1:                                     \n",
    "        valid_entries = target>=0\n",
    "    else:\n",
    "        valid_entries = (target>=0).all(axis=1)\n",
    "    sequence_length = np.sum(valid_entries)\n",
    "    if np.any(sequence_length == 0):\n",
    "        return {k: 0 for k in metrics}\n",
    "    target=target[valid_entries]\n",
    "    pred = pred[valid_entries]\n",
    "    SD_pred = np.sqrt( np.sum((pred-np.mean(pred)) ** 2) /(sequence_length-1))\n",
    "    SD_target = np.sqrt( np.sum((target-np.mean(target)) ** 2) /(sequence_length-1))\n",
    "\n",
    "    eval_results = defaultdict(float)\n",
    "    \n",
    "    for m in metrics:\n",
    "        if m =='MSE':\n",
    "            eval_results[m] = ((target - pred)**2).mean()\n",
    "        elif m =='Bias':\n",
    "            eval_results[m] = np.sum(pred - target)/np.sum(target) * 100\n",
    "        elif m == 'R':\n",
    "            eval_results[m] = np.sum(np.abs((pred-np.mean(pred)) * (target - np.mean(target)))) / (sequence_length * SD_pred * SD_target)\n",
    "        elif m == 'RMSD':\n",
    "            eval_results[m] = np.sqrt(np.sum( ( ( pred-np.mean(pred) ) * ( target - np.mean(target) ) ) ** 2 ) / sequence_length)\n",
    "        elif m == 'NSE':\n",
    "            eval_results[m] = 1 - np.sum( ( target - pred ) ** 2 ) / np.sum( (target - np.mean(target) ) ** 2 )\n",
    "    return eval_results\n",
    "\n",
    "import annutils\n",
    "result_folders = ['models','results','images']\n",
    "for result_folder in result_folders:\n",
    "    if not os.path.exists(os.path.join(local_root_path, result_folder)):\n",
    "        os.makedirs(os.path.join(local_root_path, result_folder))\n",
    "\n",
    "station_without_groups = {'all':output_locations}\n",
    "station_with_groups = {'G1':['SSS','RSAC101','RSMKL008'],\n",
    "                  'G2':['Old_River_Hwy_4','Middle_River_Intake','CCWD_Rock','SLTRM004','RSAN032','RSAN037','SLDUT007','ROLD024','RSAN058','RSAN072','OLD_MID','ROLD059','CHDMC006','CHSWP003','CHVCT000'],\n",
    "                  'G3':['SLCBN002','SLSUS012','SLMZU011','SLMZU025','RSAC064','RSAC075','RSAC081','RSAN007','RSAC092','RSAN018','Martinez']}\n",
    "final_groups = {False: station_without_groups, \n",
    "                True: station_with_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d60cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:30:06.014525Z",
     "start_time": "2023-01-26T00:28:35.082012Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"## Load Pre-Trained Models and Apply Transfer Learning\"\"\"\n",
    "\n",
    "for group_name, stations in final_groups[group_stations].items():\n",
    "    if not train_models:\n",
    "        break\n",
    "    selected_output_variables = []\n",
    "    for station in stations:\n",
    "        for output in output_locations:\n",
    "            if station in output:\n",
    "                selected_output_variables.append(output)\n",
    "\n",
    "\n",
    "    print('Training MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "    print([station.replace('target/','').replace('target','') for station in selected_output_variables])\n",
    "    \n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (xallc, yallc), (xallv, yallv), xscaler, yscaler = \\\n",
    "        annutils.create_training_sets([dfinps],\n",
    "                                      [dfouts[selected_output_variables]],\n",
    "                                      #train_frac=train_frac,\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                      noise_sigma=noise_sigma,dropout_ratio=dropout_ratio,\n",
    "                                      lead_time=lead_time,lead_freq=lead_freq)\n",
    "    if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "        try:\n",
    "            loaded_model = annutils.load_model(os.path.join(local_root_path,'models', pretrain_model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "            pretrained_model = loaded_model.model\n",
    "            print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "        except:\n",
    "            # only model weights are saved, build model then load\n",
    "            if model_type.lower() in ['lstm','gru']:\n",
    "                pretrained_model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                    num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                    output_shape=yallc.shape[1],\n",
    "                                                    act_func='sigmoid',\n",
    "                                                    layer_type=model_type.lower())\n",
    "            elif model_type.lower() == 'mlp':\n",
    "                pretrained_model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                               num_neurons_multiplier[1]*len(output_locations),\n",
    "                                               output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "            elif model_type.lower() =='resnet':\n",
    "                pretrained_model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                      num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                      output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "        \n",
    "            pretrained_model.load_weights(os.path.join(local_root_path,'models', pretrain_model_path_prefix+'.h5'))\n",
    "        \n",
    "        print('Transfer learning from pre-trained model %s' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "        o = keras.layers.Dense(yallc.shape[1], activation='linear',name='transfer_out')(pretrained_model.layers[-2].output)\n",
    "        model = keras.Model(inputs=pretrained_model.input, outputs=[o])\n",
    "\n",
    "        \n",
    "        optimizers = [\n",
    "            tf.keras.optimizers.Adam(learning_rate=lr_mult*initial_lr),\n",
    "            tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "        ]\n",
    "        optimizers_and_layers = [(optimizers[0], model.layers[0:-1]), (optimizers[1], model.layers[-1])]\n",
    "\n",
    "        model.compile(optimizer=tfa.optimizers.MultiOptimizer(optimizers_and_layers),\n",
    "                      loss=mse_loss_masked)\n",
    "    elif model_type.lower() in ['lstm','gru']:\n",
    "        model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                 num_neurons_multiplier[1]*len(output_locations),\n",
    "                                 output_shape=yallc.shape[1],\n",
    "                                 act_func='sigmoid',\n",
    "                                 layer_type=model_type.lower())\n",
    "    elif model_type.lower() == 'mlp':\n",
    "        model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                            num_neurons_multiplier[1]*len(output_locations),\n",
    "                            output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "    elif model_type.lower() =='resnet':\n",
    "        model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                   num_neurons_multiplier[1]*len(output_locations),\n",
    "                                   output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "    elif model_type.lower() in ['res-lstm','res-gru']:\n",
    "        conv_init = tf.constant_initializer(annutils.conv_filter_generator(ndays=8,window_size=11,nwindows=10))\n",
    "\n",
    "        model = build_residual_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                 num_neurons_multiplier[1]*len(output_locations),\n",
    "                                 output_shape=yallc.shape[1],\n",
    "                                 act_func='sigmoid',\n",
    "                                 layer_type=model_type.lower()[len('res-'):],\n",
    "                                 conv_init=conv_init)\n",
    "\n",
    "    model.summary()\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        xallc,\n",
    "        yallc,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        validation_data=(xallv, yallv),\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=50, mode=\"min\", restore_best_weights=True),\n",
    "        ],\n",
    "        verbose=0,\n",
    "    )\n",
    "    end_time=time.time()\n",
    "    print('Training time: %d min' % ((end_time-start_time)/60) )\n",
    "    model_save_path = os.path.join(local_root_path, 'models', (('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name)\n",
    "    if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "        joblib.dump((xscaler,yscaler),'%s_xyscaler.dump'%model_save_path)\n",
    "        model.save_weights('%s.h5'%model_save_path)\n",
    "    else:\n",
    "        annutils.save_model(model_save_path, model, xscaler, yscaler)\n",
    "    print('Model saved at %s.h5' % model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5781dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:30:08.041356Z",
     "start_time": "2023-01-26T00:30:06.016516Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"# Evaluation\n",
    "\n",
    "## Numerical Results\n",
    "\"\"\"\n",
    "\n",
    "full_results={}\n",
    "range_results=defaultdict(defaultdict)\n",
    "\n",
    "df_inpout = pd.concat(dflist[0:(num_sheets[data_file])],axis=1).dropna(axis=0)\n",
    "for group_name, stations in final_groups[group_stations].items():\n",
    "    # prepare dataset\n",
    "    selected_output_variables = []\n",
    "    for station in stations:\n",
    "        for output in output_locations:\n",
    "            if station in output:\n",
    "                selected_output_variables.append(output)\n",
    "    print('Testing MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "\n",
    "    print([station.replace('target/','').replace('target','') for station in selected_output_variables])\n",
    "\n",
    "    model_save_path = os.path.join(local_root_path, 'models', (('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name)\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (xallc, yallc), (xallv, yallv), xscaler, yscaler = \\\n",
    "        annutils.create_training_sets([dfinps],\n",
    "                                      [dfouts[selected_output_variables]],\n",
    "                                      #train_frac=train_frac,\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                      lead_time=lead_time,lead_freq=lead_freq)\n",
    "    try:\n",
    "        annmodel = annutils.load_model(model_save_path,\n",
    "                                       custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "        model = annmodel.model\n",
    "    except:\n",
    "        print('Unable to load saved model, rebuilding from pre-trained model...')\n",
    "        if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "            try:\n",
    "                loaded_model = annutils.load_model(os.path.join(local_root_path,'models', pretrain_model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "                pretrained_model = loaded_model.model\n",
    "                print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "            except:\n",
    "                # only model weights are saved, build model then load\n",
    "                if model_type.lower() in ['lstm','gru']:\n",
    "                    pretrained_model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                        num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                        output_shape=yallc.shape[1],\n",
    "                                                        act_func='sigmoid',\n",
    "                                                        layer_type=model_type.lower())\n",
    "                elif model_type.lower() == 'mlp':\n",
    "                    pretrained_model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                   num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                   output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "                elif model_type.lower() =='resnet':\n",
    "                    pretrained_model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations), \n",
    "                                                          num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                          output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "            \n",
    "            # remove last layer and attach new\n",
    "            o = keras.layers.Dense(yallc.shape[1], activation='linear',name='transfer_out')(pretrained_model.layers[-2].output)\n",
    "            model = keras.Model(inputs=pretrained_model.input, outputs=[o])\n",
    "\n",
    "        model.load_weights(model_save_path+'.h5')\n",
    "        print('Loaded model weights from %s.h5' % model_save_path)\n",
    "        xscaler,yscaler=joblib.load('%s_xyscaler.dump'%model_save_path)\n",
    "\n",
    "\n",
    "    train_pred = pd.DataFrame(np.clip(model.predict(xallc),0,1),yallc.index,columns=yallc.columns)\n",
    "    test_pred = pd.DataFrame(np.clip(model.predict(xallv),0,1),yallv.index,columns=yallv.columns)\n",
    "\n",
    "    for ii, location in enumerate(selected_output_variables):\n",
    "        \n",
    "        # compute training results\n",
    "        train_results = evaluate_sequences(yallc.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                           train_pred.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                           eval_metrics)\n",
    "        train_results['R^2'] = r2_score(train_pred.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                        yallc.loc[:,location].to_numpy().reshape(-1,1))\n",
    "        full_results['%s_train' %location] = train_results\n",
    "\n",
    "        # compute evaluation results\n",
    "        eval_results = evaluate_sequences(yallv.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                          test_pred.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                          eval_metrics)\n",
    "        eval_results['R^2'] = r2_score(test_pred.loc[:,location].to_numpy().reshape(-1,1),\n",
    "                                       yallv.loc[:,location].to_numpy().reshape(-1,1))\n",
    "\n",
    "        full_results['%s_test' %location] = eval_results\n",
    "\n",
    "        all_target = np.concatenate((yallc[location].to_numpy().reshape(-1,1),\n",
    "                                    yallv[location].to_numpy().reshape(-1,1)),axis=0)\n",
    "        all_pred = np.concatenate((train_pred[location].to_numpy().reshape(-1,1),\n",
    "                                    test_pred[location].to_numpy().reshape(-1,1)),axis=0)\n",
    "\n",
    "        # compute results at different EC ranges\n",
    "        for (lower_quantile, upper_quantile) in zip(percentiles,percentiles[1:]+[1,]):\n",
    "            lower_threshold = np.quantile(all_target, lower_quantile)\n",
    "            upper_threshold = np.quantile(all_target, upper_quantile)\n",
    "            eval_results = evaluate_sequences(all_target[(all_target > lower_threshold) & (all_target <= upper_threshold)],\n",
    "                                              all_pred[(all_target > lower_threshold) & (all_target <= upper_threshold)],\n",
    "                                              eval_metrics)\n",
    "            range_results[location][lower_quantile*100] = eval_results\n",
    "\n",
    "def add_subplot_axes(ax,rect,facecolor='w'): # matplotlib 2.0+\n",
    "    fig = plt.gcf()\n",
    "    box = ax.get_position()\n",
    "    width = box.width\n",
    "    height = box.height\n",
    "    inax_position  = ax.transAxes.transform(rect[0:2])\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    infig_position = transFigure.transform(inax_position)    \n",
    "    x = infig_position[0]\n",
    "    y = infig_position[1]\n",
    "    width *= rect[2]\n",
    "    height *= rect[3]\n",
    "    subax = fig.add_axes([x,y,width,height],facecolor=facecolor)  # matplotlib 2.0+\n",
    "    x_labelsize = subax.get_xticklabels()[0].get_size()\n",
    "    y_labelsize = subax.get_yticklabels()[0].get_size()\n",
    "    x_labelsize *= rect[2]**0.5\n",
    "    y_labelsize *= rect[3]**0.5\n",
    "    subax.xaxis.set_tick_params(labelsize=x_labelsize)\n",
    "    subax.yaxis.set_tick_params(labelsize=y_labelsize)\n",
    "    return subax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dyqvpvChW2-2",
   "metadata": {
    "id": "dyqvpvChW2-2"
   },
   "source": [
    "## Generate Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ayp3L_7F91my",
   "metadata": {
    "id": "Ayp3L_7F91my"
   },
   "source": [
    "### Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sQQ3AzvvSjjE",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:30:13.990136Z",
     "start_time": "2023-01-26T00:30:08.043347Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 11441,
     "status": "ok",
     "timestamp": 1670876381016,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "sQQ3AzvvSjjE",
    "outputId": "2b95677a-b076-4e11-aee2-14a4f433a3f9"
   },
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "key_station_mappings = {'CCWD Rock': 'RSL',\n",
    "                        'Middle River Intake':'MUP',\n",
    "                        'Old River Hwy 4': 'OH4',\n",
    "                        'OLD MID':'OLD_MID'}\n",
    "\n",
    "ncols=2\n",
    "fig_combined_timeseries, ax_combined_timeseries = plt.subplots(nrows=len(key_stations)//ncols,\n",
    "                                                               ncols=ncols,\n",
    "                                                               figsize=(6*ncols,3*len(key_stations)//ncols))\n",
    "fig_combined_timeseries.tight_layout(h_pad=3.5, w_pad=2)\n",
    "ii = 0\n",
    "for group_name, stations in final_groups[group_stations].items():\n",
    "    # prepare dataset\n",
    "    selected_output_variables = []\n",
    "    for station in stations:\n",
    "        for output in output_locations:\n",
    "            if station in output:\n",
    "                selected_output_variables.append(output)\n",
    "    print('Testing MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "\n",
    "    print([station.replace('target/','').replace('target','') for station in selected_output_variables])\n",
    "    \n",
    "    model_save_path = os.path.join(local_root_path, 'models', (('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name)\n",
    "\n",
    "    try:\n",
    "        annmodel = annutils.load_model(model_save_path,\n",
    "                                       custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "        model = annmodel.model\n",
    "    except:\n",
    "        if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "            try:\n",
    "                loaded_model = annutils.load_model(os.path.join(local_root_path,'models', pretrain_model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "                pretrained_model = loaded_model.model\n",
    "                xscaler=loaded_model.xscaler\n",
    "                yscaler=loaded_model.yscaler\n",
    "                print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "            except:\n",
    "                # only model weights are saved, build model then load\n",
    "                if model_type.lower() in ['lstm','gru']:\n",
    "                    pretrained_model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                        num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                        output_shape=yallc.shape[1],\n",
    "                                                        act_func='sigmoid',\n",
    "                                                        layer_type=model_type.lower())\n",
    "                elif model_type.lower() == 'mlp':\n",
    "                    pretrained_model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                   num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                   output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "                elif model_type.lower() =='resnet':\n",
    "                    pretrained_model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                          num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                          output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "            \n",
    "            # remove last layer and attach new\n",
    "            o = keras.layers.Dense(yallc.shape[1], activation='linear',name='transfer_out')(pretrained_model.layers[-2].output)\n",
    "            model = keras.Model(inputs=pretrained_model.input, outputs=[o])\n",
    "\n",
    "        model.load_weights(model_save_path+'.h5')\n",
    "        print('Loaded model weights from %s.h5' % model_save_path)\n",
    "        xscaler,yscaler=joblib.load('%s_xyscaler.dump'%model_save_path)\n",
    "    model.summary()\n",
    "    (xallc, yallc), (_, _), _, _ = \\\n",
    "    annutils.create_training_sets([dfinps],\n",
    "                                  [dfouts[selected_output_variables]],\n",
    "                                  train_frac=1.0,\n",
    "                                  ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                  lead_time=lead_time,lead_freq=lead_freq)\n",
    "\n",
    "    dfp = yscaler.inverse_transform(pd.DataFrame(np.clip(model.predict(xallc),0,1),yallc.index,columns=yallc.columns))\n",
    "\n",
    "    for location in selected_output_variables:\n",
    "        if any([k.lower() in location.lower() for k in key_stations]):\n",
    "            simplified_station_name = location.split('-')[0].replace('_',' ').replace('-',' ')\n",
    "            simplified_station_name = key_station_mappings.get(simplified_station_name) or simplified_station_name\n",
    "            y = dfouts.loc[:,location].copy()\n",
    "            y[y<0] = float('nan')\n",
    "\n",
    "            # Time Series Plots on Ground Truths vs. Model Predictions\n",
    "            ax_combined_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(y.iloc[(ndays+nwindows*window_size-1):],'-',color='C0',alpha=0.8)\n",
    "            ax_combined_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(dfp.loc[:,location],'-',color='C3',alpha=0.8)\n",
    "            if ii == ncols:\n",
    "                custom_lines = [Line2D([0], [0], color='C0'),\n",
    "                                Line2D([0], [0], color='C3')]\n",
    "                \n",
    "                ax_combined_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].legend(custom_lines,\n",
    "                                                                                   ['Observed','ANN Predicted'],\n",
    "                                                                                   bbox_to_anchor=(1.28, 2.76),\n",
    "                                                                                   fontsize=14)\n",
    "            \n",
    "\n",
    "            ax_combined_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].set_title(simplified_station_name)\n",
    "            ax_combined_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].set_xlim([df_inpout.index[0], df_inpout.index[-1]])\n",
    "\n",
    "            t = ax_combined_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].text(df_inpout.index[int(df_inpout.shape[0]*0.4)],\n",
    "                        ax_combined_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].get_ylim()[1]*0.58,\n",
    "                        '               Train       Test\\nR           %.3f       %.3f\\nNSE       %.3f       %.3f\\n%% Bias  %.3f%%   %.3f%%' % (full_results['%s_train' %location]['R'],\n",
    "                                                                 full_results['%s_test' %location]['R'],\n",
    "                                                                 full_results['%s_train' %location]['NSE'],\n",
    "                                                                 full_results['%s_test' %location]['NSE'],\n",
    "                                                                 full_results['%s_train' %location]['Bias'],\n",
    "                                                                 full_results['%s_test' %location]['Bias']),\n",
    "                         fontsize=14)\n",
    "            t.set_bbox(dict(facecolor='white', alpha=0.5, edgecolor='white'))\n",
    "            if ii >= len(key_stations) - ncols:\n",
    "                ax_combined_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].set_xlabel('Time')\n",
    "            ii += 1\n",
    "fig_savepath = os.path.join(local_root_path,\"images/%s_combined_time_series.png\"% ((('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name))\n",
    "fig_combined_timeseries.savefig(fig_savepath,bbox_inches='tight',dpi=300)\n",
    "print('Figure saved as %s' % fig_savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmIQJiLO956V",
   "metadata": {
    "id": "mmIQJiLO956V"
   },
   "source": [
    "### Exceedance Prob Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DC0e0D4q9y17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:30:19.481006Z",
     "start_time": "2023-01-26T00:30:13.993123Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6814,
     "status": "ok",
     "timestamp": 1670876387825,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "DC0e0D4q9y17",
    "outputId": "c9bfab5f-b402-4359-e9e9-f38d8e367498"
   },
   "outputs": [],
   "source": [
    "ncols=2\n",
    "fig_combined_exceedance, ax_combined_exceedance = plt.subplots(nrows=len(key_stations)//ncols,\n",
    "                                                               ncols=ncols,\n",
    "                                                               figsize=(6*ncols,3*len(key_stations)//ncols))\n",
    "fig_combined_exceedance.tight_layout(h_pad=3.5, w_pad=2)\n",
    "ii = 0\n",
    "for group_name, stations in final_groups[group_stations].items():\n",
    "    # prepare dataset\n",
    "    selected_output_variables = []\n",
    "    for station in stations:\n",
    "        for output in output_locations:\n",
    "            if station in output:\n",
    "                selected_output_variables.append(output)\n",
    "    print('Testing MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "\n",
    "    print([station.replace('target/','').replace('target','') for station in selected_output_variables])\n",
    "    \n",
    "    model_save_path = os.path.join(local_root_path, 'models', (('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name)\n",
    "\n",
    "    try:\n",
    "        annmodel = annutils.load_model(model_save_path,\n",
    "                                       custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "        model = annmodel.model\n",
    "    except:\n",
    "        if pretrain_model_path_prefix is not None and len(pretrain_model_path_prefix) > 0:\n",
    "            try:\n",
    "                loaded_model = annutils.load_model(os.path.join(local_root_path,'models', pretrain_model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "                pretrained_model = loaded_model.model\n",
    "                xscaler=loaded_model.xscaler\n",
    "                yscaler=loaded_model.yscaler\n",
    "                print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', pretrain_model_path_prefix))\n",
    "            except:\n",
    "                # only model weights are saved, build model then load\n",
    "                if model_type.lower() in ['lstm','gru']:\n",
    "                    pretrained_model = build_lstm_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                        num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                        output_shape=yallc.shape[1],\n",
    "                                                        act_func='sigmoid',\n",
    "                                                        layer_type=model_type.lower())\n",
    "                elif model_type.lower() == 'mlp':\n",
    "                    pretrained_model = build_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                   num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                   output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "                elif model_type.lower() =='resnet':\n",
    "                    pretrained_model = build_resnet_model(num_neurons_multiplier[0]*len(output_locations),\n",
    "                                                          num_neurons_multiplier[1]*len(output_locations),\n",
    "                                                          output_shape=yallc.shape[1], act_func='sigmoid')\n",
    "            \n",
    "            # remove last layer and attach new\n",
    "            o = keras.layers.Dense(yallc.shape[1], activation='linear',name='transfer_out')(pretrained_model.layers[-2].output)\n",
    "            model = keras.Model(inputs=pretrained_model.input, outputs=[o])\n",
    "\n",
    "        model.load_weights(model_save_path+'.h5')\n",
    "        print('Loaded model weights from %s.h5' % model_save_path)\n",
    "        xscaler,yscaler=joblib.load('%s_xyscaler.dump'%model_save_path)\n",
    "    model.summary()\n",
    "    (xallc, yallc), (_, _), xscaler, yscaler = \\\n",
    "    annutils.create_training_sets([dfinps],\n",
    "                                  [dfouts[selected_output_variables]],\n",
    "                                  train_frac=1.0,\n",
    "                                  ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                  lead_time=lead_time,lead_freq=lead_freq)\n",
    "\n",
    "    dfp = yscaler.inverse_transform(pd.DataFrame(np.clip(model.predict(xallc),0,1),yallc.index,columns=yallc.columns))\n",
    "\n",
    "    print('Generating combined exceedance plots...')\n",
    "    for location in selected_output_variables:\n",
    "        if any([k.lower() in location.lower() for k in key_stations]):\n",
    "            simplified_station_name = location.split('-')[0].replace('_',' ').replace('-',' ')\n",
    "            simplified_station_name = key_station_mappings.get(simplified_station_name) or simplified_station_name\n",
    "            y = dfouts.loc[:,location].copy()\n",
    "            y[y<0] = float('nan')\n",
    "\n",
    "            # Exceedance probability plots\n",
    "            test_y_sorted = np.sort(y.iloc[(ndays+nwindows*window_size-1):].dropna().to_numpy().reshape(-1,1),axis=0)\n",
    "            test_pred_sorted = np.sort(dfp.loc[:,location].clip(0,dfouts.loc[:,location].max()).to_numpy().reshape(-1,1),axis=0)\n",
    "\n",
    "            # Calculate the proportional values of samples\n",
    "            p = 1. * np.arange(len(test_y_sorted)-1,-1,-1) / (len(test_y_sorted) - 1)\n",
    "            p_pred = 1. * np.arange(len(test_pred_sorted)-1,-1,-1) / (len(test_pred_sorted) - 1)\n",
    "\n",
    "            # Plot the sorted data:\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(p, test_y_sorted,'-',color='C0')\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(p_pred, test_pred_sorted,'-',color='C3')\n",
    "            if ii == ncols:\n",
    "                ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].legend(['Target','Model Predictions'],fontsize=12,bbox_to_anchor=(1.28, 2.76))\n",
    "\n",
    "            plt.xticks(fontsize=12)\n",
    "            plt.yticks(fontsize=12)\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].ticklabel_format(axis='y',style='sci',scilimits=(0,0))\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].set_ylabel('EC ' + r\"$(\\mu S/cm)$\")\n",
    "            ylims = ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].get_ylim()\n",
    "            if ii >= len(key_stations) - ncols:\n",
    "                ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].set_xlabel('Exceedance Probability ' + r\"$(\\%)$\")\n",
    "                text_yloc = ylims[0]-(ylims[1]-ylims[0])*0.3\n",
    "            else:\n",
    "                text_yloc = ylims[0]-(ylims[1]-ylims[0])*0.18\n",
    "\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].text(0.48,\n",
    "                                                                             text_yloc,\n",
    "                                                                             '(%s)'%chr(97+ii),weight='bold')\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].set_title(simplified_station_name)\n",
    "            \n",
    "            subax1 = add_subplot_axes(ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)],[0.45,0.58,0.5,0.3]) # xloc, yloc, xwidth, ywidth\n",
    "            subax1.plot(y.iloc[(ndays+nwindows*window_size-1):],'-',color='C0',alpha=0.8)\n",
    "            subax1.plot(dfp.loc[:,location].clip(0,dfouts.loc[:,location].max(),axis=0),'-',color='C3',alpha=0.8)\n",
    "\n",
    "            plt.yticks(fontsize=12)\n",
    "            plt.xticks(fontsize=12)\n",
    "            subax1.ticklabel_format(axis='y',style='sci',scilimits=(0,0))\n",
    "            subax1.yaxis.set_major_locator(ticker.MaxNLocator(4))\n",
    "            subax1.xaxis.set_major_locator(ticker.MaxNLocator(6))\n",
    "            subax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            ii += 1\n",
    "fig_savepath = os.path.join(local_root_path,\"images/%s_combined_exceedance_prob.png\"% ((('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name))\n",
    "fig_combined_exceedance.savefig(fig_savepath,bbox_inches='tight',dpi=300)\n",
    "print('Figure saved as %s' % fig_savepath)\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RlOAfzjwW8IZ",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:30:19.496936Z",
     "start_time": "2023-01-26T00:30:19.483993Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1670876387828,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "RlOAfzjwW8IZ",
    "outputId": "5b754078-5944-4006-f45b-12c04e6a3151"
   },
   "outputs": [],
   "source": [
    "# create a pickle file on Google Drive and write results \n",
    "\n",
    "results_path = os.path.join(local_root_path,\"results\",('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix) if pretrain_model_path_prefix else '') + given_name+ '_full_results.pkl')\n",
    "\n",
    "f = open(results_path,\"wb\")\n",
    "pickle.dump(full_results,f)\n",
    "f.close()\n",
    "print('Numerical results saved to %s' % results_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zyWhVMJKVfZP",
   "metadata": {
    "id": "zyWhVMJKVfZP"
   },
   "source": [
    "### Station-wise Heatmap Plots of $r^2$, Bias, RSR and Bias at Different EC Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E-F7hULi-6TB",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T00:30:21.195863Z",
     "start_time": "2023-01-26T00:30:19.499923Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "executionInfo": {
     "elapsed": 2451,
     "status": "ok",
     "timestamp": 1670876390256,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "E-F7hULi-6TB",
    "outputId": "74fd0e14-301a-443e-b8b5-8ee84d845dbf"
   },
   "outputs": [],
   "source": [
    "# plot percentile results for key stations\n",
    "stations = []\n",
    "simplified_station_names = []\n",
    "for s in list(range_results.keys()):\n",
    "    new_s = s.split('-')[0].replace('_',' ').replace('-',' ').split(':')[0]\n",
    "    if new_s in key_station_mappings:\n",
    "        new_s = key_station_mappings[new_s]\n",
    "    stations.append(s)\n",
    "    simplified_station_names.append(new_s)\n",
    "\n",
    "eval_metrics=['r^2','Bias','RSR','NSE']\n",
    "fig, ax = plt.subplots(nrows=1,ncols=len(eval_metrics),figsize=(15,8))\n",
    "fig_savepath = os.path.join(local_root_path,\"images/%s_Range_Performance.png\"% ((('lr_mult_%s_%s_transfer_'% (str(lr_mult),pretrain_model_path_prefix)) if pretrain_model_path_prefix else '')+given_name))\n",
    "\n",
    "for ii, metric in enumerate(eval_metrics):\n",
    "    cmap='viridis'\n",
    "    plot_metric = metric\n",
    "    if metric=='r^2':\n",
    "        plot_metric = 'R'\n",
    "    elif metric=='RSR':\n",
    "        plot_metric = 'NSE'\n",
    "        cmap='viridis_r'\n",
    "    elif metric =='Bias':\n",
    "        cmap = 'RdYlBu_r'\n",
    "\n",
    "    to_plot=np.zeros([len(stations),len(percentiles)])\n",
    "    for x_loc, station in enumerate(stations):\n",
    "        for y_loc, percentile_range in enumerate(percentiles):\n",
    "            if metric=='r^2':\n",
    "                to_plot[x_loc,y_loc] = range_results[station][int(percentile_range*100)][plot_metric]**2\n",
    "            elif metric=='RSR':\n",
    "                to_plot[x_loc,y_loc] = np.sqrt(1-range_results[station][int(percentile_range*100)][plot_metric])\n",
    "            else:\n",
    "                to_plot[x_loc,y_loc] = range_results[station][int(percentile_range*100)][plot_metric]\n",
    "\n",
    "\n",
    "    current_plot = ax[ii].pcolor(to_plot,cmap=cmap)\n",
    "    current_plot.set_clim(current_plot.get_clim()[0],max(current_plot.get_clim()[1],1))\n",
    "    cbar = plt.colorbar(current_plot,ax=ax[ii],aspect=30,fraction=0.1, pad=0.04)\n",
    "\n",
    "    if metric =='Bias':\n",
    "        ax[ii].set_title('Percent Bias (%)')\n",
    "    else:\n",
    "        ax[ii].set_title(r'${}$'.format(metric))\n",
    "    if ii == 0:\n",
    "        ax[ii].set_ylabel('Station')\n",
    "        ax[ii].set_yticks(np.arange(len(stations))+0.5)\n",
    "        ax[ii].set_yticklabels(simplified_station_names)\n",
    "        # ax.tick_params(axis='y', labelrotation = 45, size=0)        \n",
    "    else:\n",
    "        ax[ii].set_yticks([])\n",
    "\n",
    "    ax[ii].set_xlabel('Range')\n",
    "    ax[ii].set_xticks(np.arange(len(percentiles))+0.5)\n",
    "    ax[ii].xaxis.set_tick_params(size=0)\n",
    "    ax[ii].set_xticklabels(['0-%d%%'%(percentiles[1]*100),'%d-%d%%'%(percentiles[1]*100,percentiles[2]*100),'%d-100%%'%(percentiles[2]*100)],fontsize=10)\n",
    "\n",
    "    ax[ii].text(1.4, ax[ii].get_ylim()[0]-3,\n",
    "                '(%s)'%chr(97+ii),weight='bold')\n",
    "plt.savefig(fig_savepath,bbox_inches='tight',dpi=300)\n",
    "print('Figure saved as %s' % fig_savepath)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mmDR0VennE5I"
   ],
   "provenance": [
    {
     "file_id": "1aptnja4pL9YZOYV3I15qPxsmEWzOSDfX",
     "timestamp": 1664304366259
    },
    {
     "file_id": "1OZ9foQyQP5P9hfWyEjEg0lTJW3s8sXTs",
     "timestamp": 1652819509687
    },
    {
     "file_id": "1xVpgVH4RbKZ3-vjcRePmqVU_4kjv8zZs",
     "timestamp": 1646788842444
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:Salinity_DWR]",
   "language": "python",
   "name": "conda-env-Salinity_DWR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "3d3652577a7c35b42d007c0caebd8ec483b346ab09486faf41fb90f43ee21a27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
