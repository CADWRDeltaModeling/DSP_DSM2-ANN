{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-27T20:38:17.821349800Z",
     "start_time": "2023-06-27T20:38:17.798348600Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "local_root_path = \".\"\n",
    "\n",
    "sys.path.append(local_root_path)\n",
    "import annutils\n",
    "\n",
    "from datetime import datetime\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T20:38:17.851138900Z",
     "start_time": "2023-06-27T20:38:17.809348700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# experiments = [\"6years\", \"6yearsAugmented\", \"colab\"]\n",
    "experiments = [  \"colab\"]\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"ResNet\": [8,2],\n",
    "    \"Res-LSTM\":[8,2],\n",
    "    \"LSTM\":[8],\n",
    "    \"GRU\": [8],\n",
    "    \"Res-GRU\":[8,2],\n",
    "    # \"Transformer\":[8,2]\n",
    "}\n",
    "\n",
    "initial_lr = 0.001\n",
    "\n",
    "'''\n",
    "Dropout ratio at (before) the input layer\n",
    "'''\n",
    "input_dropout = 0.\n",
    "\n",
    "'''\n",
    "Dropout ratio at intermediate layers\n",
    "'''\n",
    "intermediate_dropout = 0\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "num_sheets = 9\n",
    "\n",
    "compression_opts = dict(method='zip', archive_name='out.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T20:38:17.858137300Z",
     "start_time": "2023-06-27T20:38:17.825349800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def build_model_string(model_type, num_neurons_multiplier, input_dropout=0, intermediate_dropout=0):\n",
    "    model_type = model_type.lower()\n",
    "    model_str_def = None\n",
    "    if model_type == 'mlp':\n",
    "        ## 1. MLP Network\n",
    "        model_str_def = '%sd%d_%sd%d_o1' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                            num_neurons_multiplier[0],\n",
    "                                            ('dr%.2f_' % intermediate_dropout if intermediate_dropout > 0 else ''),\n",
    "                                            num_neurons_multiplier[1])\n",
    "\n",
    "    elif model_type == 'lstm':\n",
    "        # 2. LSTM Network\n",
    "        model_str_def = '%slstm%d_%sf_o1' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                             num_neurons_multiplier[0],\n",
    "                                             ('dr%.2f_' % intermediate_dropout if intermediate_dropout > 0 else ''),)\n",
    "\n",
    "    elif model_type == 'gru':\n",
    "        # 3. GRU Network\n",
    "        model_str_def = '%sg%d_%sf_o1' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                          num_neurons_multiplier[0],\n",
    "                                          ('dr%.2f_' % intermediate_dropout if intermediate_dropout > 0 else ''),)\n",
    "\n",
    "    elif model_type == 'resnet':\n",
    "        # 4. ResNet\n",
    "        if intermediate_dropout > 0:\n",
    "            num_neurons_multiplier.insert(1, 'dr%.2f' % intermediate_dropout)\n",
    "        model_str_def = '%sresnet%s' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                        '_' + '_'.join([str(ii) for ii in num_neurons_multiplier]))\n",
    "        num_res_blocks = 1\n",
    "\n",
    "    elif model_type == 'res-lstm':\n",
    "        # 5. Res-LSTM\n",
    "        if intermediate_dropout > 0:\n",
    "            num_neurons_multiplier.insert(1, 'dr%.2f' % intermediate_dropout)\n",
    "        model_str_def = '%sresidual_lstm%s' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                               '_' + '_'.join([str(ii) for ii in num_neurons_multiplier]))\n",
    "\n",
    "    elif model_type == 'res-gru':\n",
    "        # 6. Res-GRU\n",
    "        if intermediate_dropout > 0:\n",
    "            num_neurons_multiplier.insert(1, 'dr%.2f' % intermediate_dropout)\n",
    "        model_str_def = '%sresidual_gru%s' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                              '_' + '_'.join([str(ii) for ii in num_neurons_multiplier]))\n",
    "\n",
    "    elif model_type == 'transformer':        # 7. Transformer\n",
    "        model_str_def = '%stransformer' % ('dr%.2f_' % input_dropout if input_dropout > 0 else '')\n",
    "\n",
    "    return model_str_def\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T20:38:17.858137300Z",
     "start_time": "2023-06-27T20:38:17.851138900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "###############\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./tf_training_logs/ --port=6006\n",
    "now = datetime.now()\n",
    "root_logdir = os.path.join(os.curdir, \"tf_training_logs\", now.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(root_logdir)  ## Tensorflow Board Setup\n",
    "###############"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T20:38:17.869138100Z",
     "start_time": "2023-06-27T20:38:17.855140400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next cell is a bunch of ann specific defs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"kernel_initializer\": \"he_normal\"\n",
    "}\n",
    "\n",
    "def basic_1d(\n",
    "        filters,\n",
    "        stage=0,\n",
    "        block=0,\n",
    "        kernel_size=3,\n",
    "        numerical_name=False,\n",
    "        stride=None,\n",
    "        force_identity_shortcut=False\n",
    "):\n",
    "    \"\"\"\n",
    "    A one-dimensional basic block.\n",
    "    :param filters: the output’s feature space\n",
    "    :param stage: int representing the stage of this block (starting from 0)\n",
    "    :param block: int representing this block (starting from 0)\n",
    "    :param kernel_size: size of the kernel\n",
    "    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n",
    "    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        if block != 0 or stage == 0:\n",
    "            stride = 1\n",
    "        else:\n",
    "            stride = 2\n",
    "\n",
    "    if block > 0 and numerical_name:\n",
    "        block_char = \"b{}\".format(block)\n",
    "    else:\n",
    "        block_char = chr(ord('a') + block)\n",
    "\n",
    "    stage_char = str(stage + 2)\n",
    "\n",
    "    def f(x):\n",
    "        y = keras.layers.ZeroPadding1D(padding=1, name=\"padding{}{}_branch2a\".format(stage_char, block_char))(x)\n",
    "        y = keras.layers.Conv1D(filters, kernel_size, strides=stride, use_bias=False,\n",
    "                                name=\"res{}{}_branch2a\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "        y = keras.layers.Activation(\"relu\", name=\"res{}{}_branch2a_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        y = keras.layers.ZeroPadding1D(padding=1, name=\"padding{}{}_branch2b\".format(stage_char, block_char))(y)\n",
    "        y = keras.layers.Conv1D(filters, kernel_size, use_bias=False,\n",
    "                                name=\"res{}{}_branch2b\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "\n",
    "        if block != 0 or force_identity_shortcut:\n",
    "            shortcut = x\n",
    "        else:\n",
    "            shortcut = keras.layers.Conv1D(filters, 1, strides=stride, use_bias=False,\n",
    "                                           name=\"res{}{}_branch1\".format(stage_char, block_char),\n",
    "                                           **parameters)(x)\n",
    "            shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = keras.layers.Add(name=\"res{}{}\".format(stage_char, block_char))([y, shortcut])\n",
    "\n",
    "        y = keras.layers.Activation(\"relu\", name=\"res{}{}_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    return f\n",
    "\n",
    "def bottleneck_1d(\n",
    "        filters,\n",
    "        stage=0,\n",
    "        block=0,\n",
    "        kernel_size=3,\n",
    "        numerical_name=False,\n",
    "        stride=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    A one-dimensional bottleneck block.\n",
    "    :param filters: the output’s feature space\n",
    "    :param stage: int representing the stage of this block (starting from 0)\n",
    "    :param block: int representing this block (starting from 0)\n",
    "    :param kernel_size: size of the kernel\n",
    "    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n",
    "    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = 1 if block != 0 or stage == 0 else 2\n",
    "\n",
    "    # axis = -1 if keras.backend.image_data_format() == \"channels_last\" else 1\n",
    "\n",
    "    if block > 0 and numerical_name:\n",
    "        block_char = \"b{}\".format(block)\n",
    "    else:\n",
    "        block_char = chr(ord('a') + block)\n",
    "\n",
    "    stage_char = str(stage + 2)\n",
    "\n",
    "    def f(x):\n",
    "        y = keras.layers.Conv1D(filters, 1, strides=stride, use_bias=False,\n",
    "                                name=\"res{}{}_branch2a\".format(stage_char, block_char),\n",
    "                                **parameters)(x)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "        y = keras.layers.Activation(\"relu\", name=\"res{}{}_branch2a_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        y = keras.layers.ZeroPadding1D(padding=1, name=\"padding{}{}_branch2b\".format(stage_char, block_char))(y)\n",
    "        y = keras.layers.Conv1D(filters, kernel_size, use_bias=False,\n",
    "                                name=\"res{}{}_branch2b\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "        y = keras.layers.Activation(\"relu\", name=\"res{}{}_branch2b_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        y = keras.layers.Conv1D(filters * 4, 1, use_bias=False,\n",
    "                                name=\"res{}{}_branch2c\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "\n",
    "        if block == 0:\n",
    "            shortcut = keras.layers.Conv1D(filters * 4, 1, strides=stride, use_bias=False,\n",
    "                                           name=\"res{}{}_branch1\".format(stage_char, block_char),\n",
    "                                           **parameters)(x)\n",
    "            shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "        else:\n",
    "            shortcut = x\n",
    "\n",
    "        y = keras.layers.Add(name=\"res{}{}\".format(stage_char, block_char))([y, shortcut])\n",
    "        y = keras.layers.Activation(\"relu\", name=\"res{}{}_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    return f\n",
    "\n",
    "###############\n",
    "\"\"\"# Custom loss function\"\"\"\n",
    "\n",
    "def mse_loss_masked(y_true, y_pred):\n",
    "    squared_diff = tf.reduce_sum(tf.math.squared_difference(y_pred[y_true > 0], y_true[y_true > 0]))\n",
    "    return squared_diff / (tf.reduce_sum(tf.cast(y_true > 0, tf.float32)) + 0.01)\n",
    "\n",
    "# Define Sequential model\n",
    "NFEATURES =  8 #dfinps.shape[1]  # * (ndays + nwindows)\n",
    "\n",
    "def build_layer_from_string_def(s='i120', width_multiplier=1,\n",
    "                                block=0,\n",
    "                                force_identity_shortcut=False,\n",
    "                                return_sequences_rnn=True):\n",
    "    if s[0:4] == 'lstm':\n",
    "        return layers.LSTM(units=int(s[4:]) * width_multiplier, return_sequences=return_sequences_rnn,\n",
    "                           activation='sigmoid')\n",
    "    elif s[0:3] == 'res':\n",
    "        fields = s[3:].split('x')\n",
    "        return basic_1d(filters=int(fields[0]),\n",
    "                        stage=int(fields[3]),\n",
    "                        block=block,\n",
    "                        kernel_size=int(fields[1]),\n",
    "                        stride=int(fields[2]),\n",
    "                        force_identity_shortcut=force_identity_shortcut)\n",
    "    elif s[0:3] == 'c1d':\n",
    "        fields = s[3:].split('x')\n",
    "        return keras.layers.Conv1D(filters=int(fields[0]), kernel_size=int(fields[1]), strides=int(fields[2]),\n",
    "                                   padding='causal', activation='linear')\n",
    "    elif s[0:2] == 'td':\n",
    "        return keras.layers.TimeDistributed(keras.layers.Dense(int(s[2:]), activation='elu'))\n",
    "    elif s[0:2] == 'dr':\n",
    "        return keras.layers.Dropout(float(s[2:]))\n",
    "    # elif s[0] == 'i':\n",
    "    #     return keras.layers.InputLayer(input_shape=[int(s[1:]), NFEATURES])\n",
    "    elif s[0] == 'f':\n",
    "        return keras.layers.Flatten()\n",
    "    elif s[0] == 'g':\n",
    "        return keras.layers.GRU(int(s[1:]) * width_multiplier, return_sequences=True, activation='relu')\n",
    "    elif s[0] == 'd':\n",
    "        return keras.layers.Dense(int(s[1:]) * width_multiplier, activation='elu')\n",
    "    elif s[0] == 'o':\n",
    "        return keras.layers.Dense(int(s[1:]) * width_multiplier, activation='linear')\n",
    "    else:\n",
    "        raise Exception('Unknown layer def: %s' % s)\n",
    "\n",
    "###############\n",
    "\n",
    "def build_model_from_string_def(strdef='i120_f_d4_d2_d1', width_multiplier=1):\n",
    "    layer_strings = strdef.split('_')\n",
    "    print ('layer_strings:%s' % layer_strings)\n",
    "    inputs = keras.layers.Input(shape=[int(layer_strings[0][1:]) * NFEATURES])\n",
    "    x = None\n",
    "    prev_conv_output_num_of_channels = None\n",
    "    return_sequences_rnn = None\n",
    "    for block, f in enumerate(layer_strings[1:-1]):\n",
    "        if x is None:\n",
    "            if ('lstm' in strdef) or ('g' in strdef):\n",
    "                # these layers require 2D inputs and permutation\n",
    "                x = layers.Reshape((ndays + nwindows, NFEATURES))(inputs)\n",
    "                prev_conv_output_num_of_channels = NFEATURES\n",
    "                x = layers.Permute((2, 1))(x)\n",
    "                return_sequences_rnn = layer_strings[block + 2].startswith(('lstm', 'g', 'res', 'c1d'))\n",
    "            elif ('res' in strdef) or ('cld' in strdef):\n",
    "                # these layers require 2D inputs\n",
    "                x = layers.Reshape((ndays + nwindows, NFEATURES))(inputs)\n",
    "                prev_conv_output_num_of_channels = NFEATURES\n",
    "            else:\n",
    "                x = inputs\n",
    "\n",
    "        x = build_layer_from_string_def(f, width_multiplier, block,\n",
    "                                        force_identity_shortcut=(\n",
    "                                                f.startswith('res') and prev_conv_output_num_of_channels == int(\n",
    "                                            f[3:].split('x')[0])),\n",
    "                                        return_sequences_rnn=return_sequences_rnn)(x)\n",
    "        if f.startswith('lstm'):\n",
    "            prev_conv_output_num_of_channels = int(f[4:])\n",
    "        elif f.startswith('res') or f.startswith('c1d'):\n",
    "            prev_conv_output_num_of_channels = int(f[3:].split('x')[0])\n",
    "\n",
    "    outputs = keras.layers.Dense(int(layer_strings[-1][1:]) * width_multiplier, activation='linear')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_resnet_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid',\n",
    "                       filters=num_sheets - 1, kernel_size=3, stride=1,\n",
    "                       num_res_blocks=1, input_dropout=0.):\n",
    "    inputs = layers.Input(shape=NFEATURES * (ndays + nwindows))\n",
    "    x = layers.Reshape((ndays + nwindows, NFEATURES))(inputs)\n",
    "    x = layers.Dropout(input_dropout)(x)\n",
    "    for ii in range(num_res_blocks - 1):\n",
    "        # TODO: think about conv filter numbers and kernel sizes\n",
    "        intermediate_features = layers.ZeroPadding1D(padding=1, name=\"padding%d_branch2a\" % ii)(x)\n",
    "        intermediate_features = layers.Conv1D(filters=NFEATURES, kernel_size=2, strides=1, use_bias=False,\n",
    "                                              name=\"res%d_branch2a\" % ii)(intermediate_features)\n",
    "        intermediate_features = layers.BatchNormalization()(intermediate_features)\n",
    "        intermediate_features = layers.Activation(\"relu\", name=\"res%d_branch2a_relu\" % ii)(intermediate_features)\n",
    "\n",
    "        intermediate_features = layers.Conv1D(filters=NFEATURES, kernel_size=2, strides=1, use_bias=False,\n",
    "                                              name=\"res%d_branch2b\" % ii)(intermediate_features)\n",
    "        intermediate_features = layers.BatchNormalization()(intermediate_features)\n",
    "        intermediate_features = layers.Activation(\"relu\", name=\"res%d_branch2b_relu\" % ii)(intermediate_features)\n",
    "\n",
    "        shortcut = x\n",
    "        x = layers.Add(name=\"res%d_add\" % ii)([intermediate_features, shortcut])\n",
    "\n",
    "    y = layers.ZeroPadding1D(padding=1, name=\"padding%d_branch2a\" % num_res_blocks)(x)\n",
    "    y = layers.Conv1D(filters, kernel_size, strides=stride, use_bias=False,\n",
    "                      name=\"res%d_branch2a\" % num_res_blocks)(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation(\"relu\", name=\"res%d_branch2a_relu\" % num_res_blocks)(y)\n",
    "\n",
    "    y = layers.ZeroPadding1D(padding=1, name=\"padding%d_branch2b\" % num_res_blocks)(y)\n",
    "    y = layers.Conv1D(filters, kernel_size, use_bias=False,\n",
    "                      name=\"res%d_branch2b\" % num_res_blocks)(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(nhidden1, activation=act_func)(y)\n",
    "\n",
    "    shortcut = inputs\n",
    "    shortcut = layers.Dense(nhidden1, activation=act_func)(shortcut)\n",
    "\n",
    "    y = layers.Add(name=\"res%d_add\" % num_res_blocks)([y, shortcut])\n",
    "    y = layers.Dropout(intermediate_dropout)(y)\n",
    "\n",
    "    y = layers.Activation(\"relu\", name=\"res_relu\")(y)\n",
    "\n",
    "    y = layers.Dense(nhidden2, activation=act_func)(y)\n",
    "    outputs = layers.Dense(output_shape, activation=keras.activations.linear, name='output')(y)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "def build_residual_lstm_model(nhidden1=8, nhidden2=2, output_shape=1,\n",
    "                              act_func='sigmoid', layer_type='lstm',\n",
    "                              conv_init=None,\n",
    "                              input_dropout=0.):\n",
    "    rnn_layer = layers.LSTM if layer_type == 'lstm' else layers.GRU\n",
    "    input_shape = NFEATURES * (ndays + nwindows)\n",
    "    print(\"input shape: \", input_shape)\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Reshape((ndays + nwindows, NFEATURES))(inputs)\n",
    "    x = layers.Dropout(input_dropout)(x)\n",
    "    x = layers.Permute((2, 1))(x)\n",
    "\n",
    "    y = tf.keras.layers.Conv1D(ndays + nwindows, 1, activation='relu',\n",
    "                               kernel_initializer=conv_init,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),\n",
    "                               trainable=False)(x)\n",
    "\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(nhidden1, activation=act_func)(y)\n",
    "    y = layers.Dropout(intermediate_dropout)(y)\n",
    "    y = layers.Dense(nhidden2, activation=act_func)(y)\n",
    "    y = layers.Dense(output_shape, activation=keras.activations.linear, name='mlp_output')(y)\n",
    "\n",
    "    shortcut = x\n",
    "    shortcut = layers.Dense(nhidden1, activation=act_func)(shortcut)\n",
    "    shortcut = rnn_layer(units=output_shape * 2, activation=act_func, return_sequences=True)(shortcut)\n",
    "    shortcut = layers.Flatten()(shortcut)\n",
    "    shortcut = layers.Dense(output_shape, activation=keras.activations.linear, name='lstm_output')(shortcut)\n",
    "\n",
    "    outputs = layers.Add(name=\"res_add\")([y, shortcut])\n",
    "    # outputs = layers.Activation(\"relu\",name=\"res_relu\")(outputs)\n",
    "    outputs = layers.LeakyReLU(alpha=0.3, name=\"res_relu\")(outputs)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_transformer(head_size,\n",
    "                      num_heads,\n",
    "                      ff_dim,\n",
    "                      num_transformer_blocks,\n",
    "                      mlp_units,\n",
    "                      output_shape,\n",
    "                      dropout=0,\n",
    "                      mlp_dropout=0,\n",
    "                      input_dropout=0):\n",
    "    inputs = keras.Input(shape=NFEATURES * (ndays + nwindows))\n",
    "    x = layers.Reshape((ndays + nwindows, NFEATURES))(inputs)\n",
    "    x = layers.Dropout(input_dropout)(x)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(output_shape)(x)\n",
    "    outputs = layers.LeakyReLU(alpha=0.3, name=\"res_relu\")(outputs)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T20:38:17.932138900Z",
     "start_time": "2023-06-27T20:38:17.871139500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def build_or_load_model(model_path, model_str_def, num_neurons_multiplier, output_shape  ):\n",
    "    xscaler = None\n",
    "    yscaler = None\n",
    "    if os.path.exists(model_path + '.h5'):\n",
    "        loaded_model = annutils.load_model(model_path,\n",
    "                                           custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "        model = loaded_model.model\n",
    "        xscaler = loaded_model.xscaler\n",
    "        yscaler = loaded_model.yscaler\n",
    "        print('Ignored defined model arc and loaded pre-trained model from %s.h5' % model_path)\n",
    "\n",
    "    len_stations = output_shape[1]\n",
    "    print(\"len_stations: \", len_stations)\n",
    "    if 'resnet' in model_str_def.lower():\n",
    "        num_res_blocks =1\n",
    "        model = build_resnet_model(nhidden1=num_neurons_multiplier[0] * len_stations,\n",
    "                                   nhidden2=num_neurons_multiplier[-1] * len_stations, output_shape=len_stations,\n",
    "                                   num_res_blocks=num_res_blocks,\n",
    "                                   input_dropout=input_dropout)\n",
    "    elif ('residual_lstm' in model_str_def.lower()) or ('residual_gru' in model_str_def.lower()):\n",
    "        print(\"model is lstm or gru\")\n",
    "        print(\"ndays: \", ndays)\n",
    "        print(\"nwindows: \", nwindows)\n",
    "        print(\"window_size: \", window_size)\n",
    "        print(\"output_shape: \", output_shape)\n",
    "        conv_init = tf.constant_initializer(annutils.conv_filter_generator(ndays=ndays,\n",
    "                                                                           window_size=window_size,\n",
    "                                                                           nwindows=nwindows))\n",
    "\n",
    "        layer_type = model_str_def.lower().split('_')[2]\n",
    "        print(\"layer_type: \", layer_type)\n",
    "        model = build_residual_lstm_model(num_neurons_multiplier[0] * len_stations,\n",
    "                                          num_neurons_multiplier[-1] * len_stations,\n",
    "                                          output_shape=len_stations,\n",
    "                                          act_func='sigmoid',\n",
    "                                          layer_type=layer_type,\n",
    "                                          conv_init=conv_init,\n",
    "                                          input_dropout=input_dropout)\n",
    "    elif 'transformer' in model_str_def.lower():\n",
    "        model = build_transformer(head_size=256,\n",
    "                                  num_heads=4,\n",
    "                                  ff_dim=4,\n",
    "                                  num_transformer_blocks=4,\n",
    "                                  mlp_units=[128],\n",
    "                                  output_shape=len_stations,\n",
    "                                  mlp_dropout=0.4,\n",
    "                                  dropout=0.25,\n",
    "                                  input_dropout=input_dropout)\n",
    "    else:\n",
    "        model = build_model_from_string_def(model_str_def, width_multiplier=len_stations)\n",
    "\n",
    "    return model, xscaler, yscaler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T20:38:17.947419500Z",
     "start_time": "2023-06-27T20:38:17.939418400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we should be ready to make some models and do some training\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "experiments = [  \"colab\"]\n",
    "models = {\n",
    "    \"ResNet\": [8,2],\n",
    "    \"Res-LSTM\":[8,2],\n",
    "    \"LSTM\":[8],\n",
    "    \"GRU\": [8],\n",
    "    \"Res-GRU\":[8,2],\n",
    "    # \"Transformer\":[8,2]  # this seems like its taking something like 27h to train!!! 2000s per epoch\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T20:38:59.230159700Z",
     "start_time": "2023-06-27T20:38:59.220160400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "\tplt.plot(history.history['loss'])\n",
    "\tplt.plot(history.history['val_loss'])\n",
    "\tplt.title('model loss')\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\tplt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T20:38:17.980421700Z",
     "start_time": "2023-06-27T20:38:17.966418900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment:  colab\n",
      "model_str_def: resnet_8_2\n",
      "len_stations:  23\n",
      "Model summary:\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 944)]        0           []                               \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)           (None, 118, 8)       0           ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 118, 8)       0           ['reshape_10[0][0]']             \n",
      "                                                                                                  \n",
      " padding1_branch2a (ZeroPadding  (None, 120, 8)      0           ['dropout_6[0][0]']              \n",
      " 1D)                                                                                              \n",
      "                                                                                                  \n",
      " res1_branch2a (Conv1D)         (None, 118, 8)       192         ['padding1_branch2a[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 118, 8)      32          ['res1_branch2a[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " res1_branch2a_relu (Activation  (None, 118, 8)      0           ['batch_normalization_2[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " padding1_branch2b (ZeroPadding  (None, 120, 8)      0           ['res1_branch2a_relu[0][0]']     \n",
      " 1D)                                                                                              \n",
      "                                                                                                  \n",
      " res1_branch2b (Conv1D)         (None, 118, 8)       192         ['padding1_branch2b[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 118, 8)      32          ['res1_branch2b[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_12 (Flatten)           (None, 944)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 184)          173880      ['flatten_12[0][0]']             \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 184)          173880      ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " res1_add (Add)                 (None, 184)          0           ['dense_16[0][0]',               \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 184)          0           ['res1_add[0][0]']               \n",
      "                                                                                                  \n",
      " res_relu (Activation)          (None, 184)          0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 46)           8510        ['res_relu[0][0]']               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 23)           1081        ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 357,799\n",
      "Trainable params: 357,767\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Creating new scalers\n",
      "Xscaler Min[0]: 3519.98323631286\n",
      "Xscaler Max[0]: 561182.0\n",
      "Epoch 1/50\n",
      "715/715 - 8s - loss: 0.0095 - val_loss: 0.0254 - 8s/epoch - 11ms/step\n",
      "Epoch 2/50\n",
      "715/715 - 7s - loss: 0.0030 - val_loss: 0.0048 - 7s/epoch - 9ms/step\n",
      "Epoch 3/50\n",
      "715/715 - 7s - loss: 0.0022 - val_loss: 0.0043 - 7s/epoch - 9ms/step\n",
      "Epoch 4/50\n",
      "715/715 - 7s - loss: 0.0019 - val_loss: 0.0050 - 7s/epoch - 9ms/step\n",
      "Epoch 5/50\n",
      "715/715 - 6s - loss: 0.0016 - val_loss: 0.0065 - 6s/epoch - 9ms/step\n",
      "Epoch 6/50\n",
      "715/715 - 7s - loss: 0.0014 - val_loss: 0.0043 - 7s/epoch - 9ms/step\n",
      "Epoch 7/50\n",
      "715/715 - 7s - loss: 0.0014 - val_loss: 0.0079 - 7s/epoch - 9ms/step\n",
      "Epoch 8/50\n",
      "715/715 - 7s - loss: 0.0013 - val_loss: 0.0043 - 7s/epoch - 9ms/step\n",
      "Epoch 9/50\n",
      "715/715 - 7s - loss: 0.0012 - val_loss: 0.0043 - 7s/epoch - 10ms/step\n",
      "Epoch 10/50\n",
      "715/715 - 7s - loss: 0.0012 - val_loss: 0.0037 - 7s/epoch - 9ms/step\n",
      "Epoch 11/50\n",
      "715/715 - 7s - loss: 0.0011 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 12/50\n",
      "715/715 - 7s - loss: 0.0010 - val_loss: 0.0048 - 7s/epoch - 9ms/step\n",
      "Epoch 13/50\n",
      "715/715 - 7s - loss: 0.0010 - val_loss: 0.0042 - 7s/epoch - 10ms/step\n",
      "Epoch 14/50\n",
      "715/715 - 7s - loss: 9.8050e-04 - val_loss: 0.0045 - 7s/epoch - 10ms/step\n",
      "Epoch 15/50\n",
      "715/715 - 7s - loss: 9.2848e-04 - val_loss: 0.0045 - 7s/epoch - 10ms/step\n",
      "Epoch 16/50\n",
      "715/715 - 7s - loss: 8.9660e-04 - val_loss: 0.0045 - 7s/epoch - 9ms/step\n",
      "Epoch 17/50\n",
      "715/715 - 7s - loss: 9.1998e-04 - val_loss: 0.0037 - 7s/epoch - 9ms/step\n",
      "Epoch 18/50\n",
      "715/715 - 7s - loss: 8.7442e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Epoch 19/50\n",
      "715/715 - 7s - loss: 8.4516e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 20/50\n",
      "715/715 - 7s - loss: 8.3640e-04 - val_loss: 0.0040 - 7s/epoch - 9ms/step\n",
      "Epoch 21/50\n",
      "715/715 - 7s - loss: 7.9490e-04 - val_loss: 0.0036 - 7s/epoch - 9ms/step\n",
      "Epoch 22/50\n",
      "715/715 - 7s - loss: 7.9536e-04 - val_loss: 0.0045 - 7s/epoch - 9ms/step\n",
      "Epoch 23/50\n",
      "715/715 - 7s - loss: 7.9312e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 24/50\n",
      "715/715 - 7s - loss: 7.8168e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Epoch 25/50\n",
      "715/715 - 7s - loss: 7.5706e-04 - val_loss: 0.0037 - 7s/epoch - 9ms/step\n",
      "Epoch 26/50\n",
      "715/715 - 7s - loss: 7.5162e-04 - val_loss: 0.0036 - 7s/epoch - 10ms/step\n",
      "Epoch 27/50\n",
      "715/715 - 7s - loss: 7.4083e-04 - val_loss: 0.0038 - 7s/epoch - 9ms/step\n",
      "Epoch 28/50\n",
      "715/715 - 7s - loss: 7.2772e-04 - val_loss: 0.0037 - 7s/epoch - 9ms/step\n",
      "Epoch 29/50\n",
      "715/715 - 7s - loss: 7.2071e-04 - val_loss: 0.0036 - 7s/epoch - 9ms/step\n",
      "Epoch 30/50\n",
      "715/715 - 7s - loss: 7.0979e-04 - val_loss: 0.0034 - 7s/epoch - 9ms/step\n",
      "Epoch 31/50\n",
      "715/715 - 7s - loss: 7.2678e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 32/50\n",
      "715/715 - 7s - loss: 6.8596e-04 - val_loss: 0.0043 - 7s/epoch - 9ms/step\n",
      "Epoch 33/50\n",
      "715/715 - 7s - loss: 6.7089e-04 - val_loss: 0.0035 - 7s/epoch - 9ms/step\n",
      "Epoch 34/50\n",
      "715/715 - 7s - loss: 7.0356e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 35/50\n",
      "715/715 - 7s - loss: 6.7691e-04 - val_loss: 0.0038 - 7s/epoch - 9ms/step\n",
      "Epoch 36/50\n",
      "715/715 - 7s - loss: 6.7193e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Epoch 37/50\n",
      "715/715 - 7s - loss: 6.5015e-04 - val_loss: 0.0047 - 7s/epoch - 9ms/step\n",
      "Epoch 38/50\n",
      "715/715 - 6s - loss: 6.4731e-04 - val_loss: 0.0039 - 6s/epoch - 9ms/step\n",
      "Epoch 39/50\n",
      "715/715 - 6s - loss: 6.4552e-04 - val_loss: 0.0036 - 6s/epoch - 9ms/step\n",
      "Epoch 40/50\n",
      "715/715 - 7s - loss: 6.4799e-04 - val_loss: 0.0036 - 7s/epoch - 9ms/step\n",
      "Epoch 41/50\n",
      "715/715 - 7s - loss: 6.3686e-04 - val_loss: 0.0038 - 7s/epoch - 9ms/step\n",
      "Epoch 42/50\n",
      "715/715 - 7s - loss: 6.1348e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 43/50\n",
      "715/715 - 7s - loss: 6.2259e-04 - val_loss: 0.0033 - 7s/epoch - 9ms/step\n",
      "Epoch 44/50\n",
      "715/715 - 7s - loss: 6.3157e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Epoch 45/50\n",
      "715/715 - 7s - loss: 6.0871e-04 - val_loss: 0.0057 - 7s/epoch - 9ms/step\n",
      "Epoch 46/50\n",
      "715/715 - 7s - loss: 6.1133e-04 - val_loss: 0.0040 - 7s/epoch - 9ms/step\n",
      "Epoch 47/50\n",
      "715/715 - 7s - loss: 6.1804e-04 - val_loss: 0.0046 - 7s/epoch - 9ms/step\n",
      "Epoch 48/50\n",
      "715/715 - 7s - loss: 6.1201e-04 - val_loss: 0.0037 - 7s/epoch - 9ms/step\n",
      "Epoch 49/50\n",
      "715/715 - 7s - loss: 5.8974e-04 - val_loss: 0.0036 - 7s/epoch - 9ms/step\n",
      "Epoch 50/50\n",
      "715/715 - 7s - loss: 6.1120e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Model saved to .\\Experiments\\colab\\models\\mtl_i118_resnet_8_2\n",
      "Training time: 5 min\n",
      "model_str_def: residual_lstm_8_2\n",
      "len_stations:  23\n",
      "model is lstm or gru\n",
      "ndays:  118\n",
      "nwindows:  0\n",
      "window_size:  0\n",
      "output_shape:  (91421, 23)\n",
      "layer_type:  lstm\n",
      "input shape:  944\n",
      "Model summary:\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 944)]        0           []                               \n",
      "                                                                                                  \n",
      " reshape_11 (Reshape)           (None, 118, 8)       0           ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 118, 8)       0           ['reshape_11[0][0]']             \n",
      "                                                                                                  \n",
      " permute_9 (Permute)            (None, 8, 118)       0           ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 8, 118)       14042       ['permute_9[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)           (None, 944)          0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 184)          173880      ['flatten_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 8, 184)       21896       ['permute_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 184)          0           ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  (None, 8, 46)        42504       ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 46)           8510        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)           (None, 368)          0           ['lstm_6[0][0]']                 \n",
      "                                                                                                  \n",
      " mlp_output (Dense)             (None, 23)           1081        ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_output (Dense)            (None, 23)           8487        ['flatten_14[0][0]']             \n",
      "                                                                                                  \n",
      " res_add (Add)                  (None, 23)           0           ['mlp_output[0][0]',             \n",
      "                                                                  'lstm_output[0][0]']            \n",
      "                                                                                                  \n",
      " res_relu (LeakyReLU)           (None, 23)           0           ['res_add[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 270,400\n",
      "Trainable params: 256,358\n",
      "Non-trainable params: 14,042\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Creating new scalers\n",
      "Xscaler Min[0]: 3519.98323631286\n",
      "Xscaler Max[0]: 561182.0\n",
      "Epoch 1/50\n",
      "715/715 - 9s - loss: 0.0120 - val_loss: 0.0092 - 9s/epoch - 13ms/step\n",
      "Epoch 2/50\n",
      "715/715 - 7s - loss: 0.0062 - val_loss: 0.0071 - 7s/epoch - 10ms/step\n",
      "Epoch 3/50\n",
      "715/715 - 7s - loss: 0.0042 - val_loss: 0.0061 - 7s/epoch - 10ms/step\n",
      "Epoch 4/50\n",
      "715/715 - 7s - loss: 0.0031 - val_loss: 0.0064 - 7s/epoch - 9ms/step\n",
      "Epoch 5/50\n",
      "715/715 - 7s - loss: 0.0025 - val_loss: 0.0056 - 7s/epoch - 10ms/step\n",
      "Epoch 6/50\n",
      "715/715 - 7s - loss: 0.0021 - val_loss: 0.0055 - 7s/epoch - 10ms/step\n",
      "Epoch 7/50\n",
      "715/715 - 7s - loss: 0.0019 - val_loss: 0.0046 - 7s/epoch - 10ms/step\n",
      "Epoch 8/50\n",
      "715/715 - 7s - loss: 0.0016 - val_loss: 0.0044 - 7s/epoch - 10ms/step\n",
      "Epoch 9/50\n",
      "715/715 - 7s - loss: 0.0015 - val_loss: 0.0046 - 7s/epoch - 9ms/step\n",
      "Epoch 10/50\n",
      "715/715 - 7s - loss: 0.0014 - val_loss: 0.0051 - 7s/epoch - 9ms/step\n",
      "Epoch 11/50\n",
      "715/715 - 7s - loss: 0.0013 - val_loss: 0.0048 - 7s/epoch - 9ms/step\n",
      "Epoch 12/50\n",
      "715/715 - 7s - loss: 0.0013 - val_loss: 0.0048 - 7s/epoch - 9ms/step\n",
      "Epoch 13/50\n",
      "715/715 - 7s - loss: 0.0011 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 14/50\n",
      "715/715 - 7s - loss: 0.0011 - val_loss: 0.0047 - 7s/epoch - 9ms/step\n",
      "Epoch 15/50\n",
      "715/715 - 7s - loss: 0.0010 - val_loss: 0.0043 - 7s/epoch - 9ms/step\n",
      "Epoch 16/50\n",
      "715/715 - 7s - loss: 0.0010 - val_loss: 0.0040 - 7s/epoch - 9ms/step\n",
      "Epoch 17/50\n",
      "715/715 - 7s - loss: 9.9551e-04 - val_loss: 0.0042 - 7s/epoch - 9ms/step\n",
      "Epoch 18/50\n",
      "715/715 - 7s - loss: 9.5017e-04 - val_loss: 0.0037 - 7s/epoch - 9ms/step\n",
      "Epoch 19/50\n",
      "715/715 - 7s - loss: 9.2104e-04 - val_loss: 0.0040 - 7s/epoch - 9ms/step\n",
      "Epoch 20/50\n",
      "715/715 - 7s - loss: 9.0353e-04 - val_loss: 0.0041 - 7s/epoch - 10ms/step\n",
      "Epoch 21/50\n",
      "715/715 - 7s - loss: 8.5805e-04 - val_loss: 0.0042 - 7s/epoch - 9ms/step\n",
      "Epoch 22/50\n",
      "715/715 - 7s - loss: 8.5123e-04 - val_loss: 0.0035 - 7s/epoch - 10ms/step\n",
      "Epoch 23/50\n",
      "715/715 - 7s - loss: 7.9677e-04 - val_loss: 0.0051 - 7s/epoch - 10ms/step\n",
      "Epoch 24/50\n",
      "715/715 - 7s - loss: 7.8242e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 25/50\n",
      "715/715 - 7s - loss: 8.0488e-04 - val_loss: 0.0045 - 7s/epoch - 9ms/step\n",
      "Epoch 26/50\n",
      "715/715 - 7s - loss: 7.6866e-04 - val_loss: 0.0040 - 7s/epoch - 9ms/step\n",
      "Epoch 27/50\n",
      "715/715 - 7s - loss: 7.7935e-04 - val_loss: 0.0040 - 7s/epoch - 9ms/step\n",
      "Epoch 28/50\n",
      "715/715 - 7s - loss: 7.7825e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Epoch 29/50\n",
      "715/715 - 7s - loss: 7.6791e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 30/50\n",
      "715/715 - 7s - loss: 6.8395e-04 - val_loss: 0.0038 - 7s/epoch - 9ms/step\n",
      "Epoch 31/50\n",
      "715/715 - 7s - loss: 7.0325e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 32/50\n",
      "715/715 - 7s - loss: 6.8382e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Epoch 33/50\n",
      "715/715 - 7s - loss: 7.2930e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 34/50\n",
      "715/715 - 7s - loss: 6.7553e-04 - val_loss: 0.0042 - 7s/epoch - 10ms/step\n",
      "Epoch 35/50\n",
      "715/715 - 7s - loss: 6.5559e-04 - val_loss: 0.0040 - 7s/epoch - 10ms/step\n",
      "Epoch 36/50\n",
      "715/715 - 7s - loss: 6.4854e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Epoch 37/50\n",
      "715/715 - 7s - loss: 6.3965e-04 - val_loss: 0.0037 - 7s/epoch - 9ms/step\n",
      "Epoch 38/50\n",
      "715/715 - 7s - loss: 6.2541e-04 - val_loss: 0.0044 - 7s/epoch - 9ms/step\n",
      "Epoch 39/50\n",
      "715/715 - 7s - loss: 6.1408e-04 - val_loss: 0.0037 - 7s/epoch - 9ms/step\n",
      "Epoch 40/50\n",
      "715/715 - 7s - loss: 6.2064e-04 - val_loss: 0.0036 - 7s/epoch - 9ms/step\n",
      "Epoch 41/50\n",
      "715/715 - 7s - loss: 6.1064e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 42/50\n",
      "715/715 - 7s - loss: 5.8207e-04 - val_loss: 0.0053 - 7s/epoch - 9ms/step\n",
      "Epoch 43/50\n",
      "715/715 - 7s - loss: 5.9127e-04 - val_loss: 0.0042 - 7s/epoch - 9ms/step\n",
      "Epoch 44/50\n",
      "715/715 - 7s - loss: 5.6932e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Epoch 45/50\n",
      "715/715 - 7s - loss: 5.9622e-04 - val_loss: 0.0037 - 7s/epoch - 9ms/step\n",
      "Epoch 46/50\n",
      "715/715 - 7s - loss: 5.7317e-04 - val_loss: 0.0044 - 7s/epoch - 9ms/step\n",
      "Epoch 47/50\n",
      "715/715 - 7s - loss: 5.3840e-04 - val_loss: 0.0039 - 7s/epoch - 9ms/step\n",
      "Epoch 48/50\n",
      "715/715 - 7s - loss: 5.6675e-04 - val_loss: 0.0041 - 7s/epoch - 9ms/step\n",
      "Epoch 49/50\n",
      "715/715 - 7s - loss: 5.3384e-04 - val_loss: 0.0038 - 7s/epoch - 10ms/step\n",
      "Epoch 50/50\n",
      "715/715 - 7s - loss: 5.5951e-04 - val_loss: 0.0038 - 7s/epoch - 9ms/step\n",
      "Model saved to .\\Experiments\\colab\\models\\mtl_i118_residual_lstm_8_2\n",
      "Training time: 5 min\n",
      "model_str_def: lstm8_f_o1\n",
      "len_stations:  23\n",
      "layer_strings:['i118', 'lstm8', 'f', 'o1']\n",
      "Model summary:\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 944)]             0         \n",
      "                                                                 \n",
      " reshape_12 (Reshape)        (None, 118, 8)            0         \n",
      "                                                                 \n",
      " permute_10 (Permute)        (None, 8, 118)            0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 184)               223008    \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 184)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 23)                4255      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 227,263\n",
      "Trainable params: 227,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Creating new scalers\n",
      "Xscaler Min[0]: 3519.98323631286\n",
      "Xscaler Max[0]: 561182.0\n",
      "Epoch 1/50\n",
      "715/715 - 30s - loss: 0.0177 - val_loss: 0.0183 - 30s/epoch - 42ms/step\n",
      "Epoch 2/50\n",
      "715/715 - 24s - loss: 0.0115 - val_loss: 0.0156 - 24s/epoch - 33ms/step\n",
      "Epoch 3/50\n",
      "715/715 - 26s - loss: 0.0101 - val_loss: 0.0148 - 26s/epoch - 37ms/step\n",
      "Epoch 4/50\n",
      "715/715 - 24s - loss: 0.0095 - val_loss: 0.0149 - 24s/epoch - 34ms/step\n",
      "Epoch 5/50\n",
      "715/715 - 24s - loss: 0.0091 - val_loss: 0.0152 - 24s/epoch - 33ms/step\n",
      "Epoch 6/50\n",
      "715/715 - 24s - loss: 0.0087 - val_loss: 0.0132 - 24s/epoch - 33ms/step\n",
      "Epoch 7/50\n",
      "715/715 - 24s - loss: 0.0083 - val_loss: 0.0107 - 24s/epoch - 34ms/step\n",
      "Epoch 8/50\n",
      "715/715 - 25s - loss: 0.0078 - val_loss: 0.0096 - 25s/epoch - 35ms/step\n",
      "Epoch 9/50\n",
      "715/715 - 24s - loss: 0.0073 - val_loss: 0.0082 - 24s/epoch - 34ms/step\n",
      "Epoch 10/50\n",
      "715/715 - 24s - loss: 0.0066 - val_loss: 0.0061 - 24s/epoch - 34ms/step\n",
      "Epoch 11/50\n",
      "715/715 - 24s - loss: 0.0056 - val_loss: 0.0060 - 24s/epoch - 34ms/step\n",
      "Epoch 12/50\n",
      "715/715 - 24s - loss: 0.0043 - val_loss: 0.0059 - 24s/epoch - 34ms/step\n",
      "Epoch 13/50\n",
      "715/715 - 24s - loss: 0.0035 - val_loss: 0.0052 - 24s/epoch - 34ms/step\n",
      "Epoch 14/50\n",
      "715/715 - 24s - loss: 0.0031 - val_loss: 0.0045 - 24s/epoch - 33ms/step\n",
      "Epoch 15/50\n",
      "715/715 - 25s - loss: 0.0029 - val_loss: 0.0044 - 25s/epoch - 35ms/step\n",
      "Epoch 16/50\n",
      "715/715 - 25s - loss: 0.0029 - val_loss: 0.0058 - 25s/epoch - 35ms/step\n",
      "Epoch 17/50\n",
      "715/715 - 25s - loss: 0.0028 - val_loss: 0.0048 - 25s/epoch - 35ms/step\n",
      "Epoch 18/50\n",
      "715/715 - 24s - loss: 0.0027 - val_loss: 0.0043 - 24s/epoch - 34ms/step\n",
      "Epoch 19/50\n",
      "715/715 - 24s - loss: 0.0026 - val_loss: 0.0040 - 24s/epoch - 33ms/step\n",
      "Epoch 20/50\n",
      "715/715 - 24s - loss: 0.0026 - val_loss: 0.0041 - 24s/epoch - 33ms/step\n",
      "Epoch 21/50\n",
      "715/715 - 24s - loss: 0.0025 - val_loss: 0.0047 - 24s/epoch - 33ms/step\n",
      "Epoch 22/50\n",
      "715/715 - 24s - loss: 0.0025 - val_loss: 0.0045 - 24s/epoch - 33ms/step\n",
      "Epoch 23/50\n",
      "715/715 - 24s - loss: 0.0025 - val_loss: 0.0048 - 24s/epoch - 33ms/step\n",
      "Epoch 24/50\n",
      "715/715 - 24s - loss: 0.0024 - val_loss: 0.0039 - 24s/epoch - 33ms/step\n",
      "Epoch 25/50\n",
      "715/715 - 24s - loss: 0.0024 - val_loss: 0.0050 - 24s/epoch - 33ms/step\n",
      "Epoch 26/50\n",
      "715/715 - 24s - loss: 0.0023 - val_loss: 0.0044 - 24s/epoch - 33ms/step\n",
      "Epoch 27/50\n",
      "715/715 - 24s - loss: 0.0023 - val_loss: 0.0047 - 24s/epoch - 33ms/step\n",
      "Epoch 28/50\n",
      "715/715 - 24s - loss: 0.0023 - val_loss: 0.0037 - 24s/epoch - 33ms/step\n",
      "Epoch 29/50\n",
      "715/715 - 24s - loss: 0.0022 - val_loss: 0.0039 - 24s/epoch - 34ms/step\n",
      "Epoch 30/50\n",
      "715/715 - 24s - loss: 0.0022 - val_loss: 0.0040 - 24s/epoch - 33ms/step\n",
      "Epoch 31/50\n",
      "715/715 - 24s - loss: 0.0022 - val_loss: 0.0038 - 24s/epoch - 34ms/step\n",
      "Epoch 32/50\n",
      "715/715 - 24s - loss: 0.0022 - val_loss: 0.0036 - 24s/epoch - 33ms/step\n",
      "Epoch 33/50\n",
      "715/715 - 24s - loss: 0.0021 - val_loss: 0.0039 - 24s/epoch - 33ms/step\n",
      "Epoch 34/50\n",
      "715/715 - 25s - loss: 0.0021 - val_loss: 0.0038 - 25s/epoch - 34ms/step\n",
      "Epoch 35/50\n",
      "715/715 - 25s - loss: 0.0021 - val_loss: 0.0042 - 25s/epoch - 35ms/step\n",
      "Epoch 36/50\n",
      "715/715 - 24s - loss: 0.0020 - val_loss: 0.0040 - 24s/epoch - 33ms/step\n",
      "Epoch 37/50\n",
      "715/715 - 24s - loss: 0.0020 - val_loss: 0.0037 - 24s/epoch - 34ms/step\n",
      "Epoch 38/50\n",
      "715/715 - 24s - loss: 0.0020 - val_loss: 0.0039 - 24s/epoch - 34ms/step\n",
      "Epoch 39/50\n",
      "715/715 - 24s - loss: 0.0020 - val_loss: 0.0045 - 24s/epoch - 33ms/step\n",
      "Epoch 40/50\n",
      "715/715 - 24s - loss: 0.0020 - val_loss: 0.0037 - 24s/epoch - 33ms/step\n",
      "Epoch 41/50\n",
      "715/715 - 24s - loss: 0.0019 - val_loss: 0.0040 - 24s/epoch - 33ms/step\n",
      "Epoch 42/50\n",
      "715/715 - 24s - loss: 0.0019 - val_loss: 0.0043 - 24s/epoch - 33ms/step\n",
      "Epoch 43/50\n",
      "715/715 - 24s - loss: 0.0019 - val_loss: 0.0039 - 24s/epoch - 33ms/step\n",
      "Epoch 44/50\n",
      "715/715 - 24s - loss: 0.0019 - val_loss: 0.0041 - 24s/epoch - 34ms/step\n",
      "Epoch 45/50\n",
      "715/715 - 24s - loss: 0.0019 - val_loss: 0.0043 - 24s/epoch - 33ms/step\n",
      "Epoch 46/50\n",
      "715/715 - 24s - loss: 0.0019 - val_loss: 0.0043 - 24s/epoch - 33ms/step\n",
      "Epoch 47/50\n",
      "715/715 - 24s - loss: 0.0019 - val_loss: 0.0049 - 24s/epoch - 34ms/step\n",
      "Epoch 48/50\n",
      "715/715 - 24s - loss: 0.0018 - val_loss: 0.0043 - 24s/epoch - 33ms/step\n",
      "Epoch 49/50\n",
      "715/715 - 24s - loss: 0.0018 - val_loss: 0.0044 - 24s/epoch - 33ms/step\n",
      "Epoch 50/50\n",
      "715/715 - 24s - loss: 0.0018 - val_loss: 0.0037 - 24s/epoch - 34ms/step\n",
      "Model saved to .\\Experiments\\colab\\models\\mtl_i118_lstm8_f_o1\n",
      "Training time: 20 min\n",
      "model_str_def: g8_f_o1\n",
      "len_stations:  23\n",
      "layer_strings:['i118', 'g8', 'f', 'o1']\n",
      "Model summary:\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 944)]             0         \n",
      "                                                                 \n",
      " reshape_13 (Reshape)        (None, 118, 8)            0         \n",
      "                                                                 \n",
      " permute_11 (Permute)        (None, 8, 118)            0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 8, 184)            167808    \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 1472)              0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 23)                33879     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201,687\n",
      "Trainable params: 201,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Creating new scalers\n",
      "Xscaler Min[0]: 3519.98323631286\n",
      "Xscaler Max[0]: 561182.0\n",
      "Epoch 1/50\n",
      "715/715 - 18s - loss: 0.0074 - val_loss: 0.0060 - 18s/epoch - 25ms/step\n",
      "Epoch 2/50\n",
      "715/715 - 16s - loss: 0.0033 - val_loss: 0.0068 - 16s/epoch - 23ms/step\n",
      "Epoch 3/50\n",
      "715/715 - 15s - loss: 0.0027 - val_loss: 0.0056 - 15s/epoch - 21ms/step\n",
      "Epoch 4/50\n",
      "715/715 - 15s - loss: 0.0023 - val_loss: 0.0067 - 15s/epoch - 21ms/step\n",
      "Epoch 5/50\n",
      "715/715 - 15s - loss: 0.0021 - val_loss: 0.0052 - 15s/epoch - 21ms/step\n",
      "Epoch 6/50\n",
      "715/715 - 15s - loss: 0.0020 - val_loss: 0.0052 - 15s/epoch - 21ms/step\n",
      "Epoch 7/50\n",
      "715/715 - 16s - loss: 0.0019 - val_loss: 0.0053 - 16s/epoch - 22ms/step\n",
      "Epoch 8/50\n",
      "715/715 - 15s - loss: 0.0018 - val_loss: 0.0055 - 15s/epoch - 21ms/step\n",
      "Epoch 9/50\n",
      "715/715 - 16s - loss: 0.0017 - val_loss: 0.0058 - 16s/epoch - 22ms/step\n",
      "Epoch 10/50\n",
      "715/715 - 15s - loss: 0.0017 - val_loss: 0.0046 - 15s/epoch - 21ms/step\n",
      "Epoch 11/50\n",
      "715/715 - 15s - loss: 0.0016 - val_loss: 0.0051 - 15s/epoch - 21ms/step\n",
      "Epoch 12/50\n",
      "715/715 - 15s - loss: 0.0016 - val_loss: 0.0056 - 15s/epoch - 22ms/step\n",
      "Epoch 13/50\n",
      "715/715 - 15s - loss: 0.0016 - val_loss: 0.0048 - 15s/epoch - 21ms/step\n",
      "Epoch 14/50\n",
      "715/715 - 15s - loss: 0.0015 - val_loss: 0.0044 - 15s/epoch - 21ms/step\n",
      "Epoch 15/50\n",
      "715/715 - 15s - loss: 0.0015 - val_loss: 0.0047 - 15s/epoch - 21ms/step\n",
      "Epoch 16/50\n",
      "715/715 - 15s - loss: 0.0015 - val_loss: 0.0051 - 15s/epoch - 21ms/step\n",
      "Epoch 17/50\n",
      "715/715 - 16s - loss: 0.0015 - val_loss: 0.0047 - 16s/epoch - 22ms/step\n",
      "Epoch 18/50\n",
      "715/715 - 15s - loss: 0.0014 - val_loss: 0.0063 - 15s/epoch - 21ms/step\n",
      "Epoch 19/50\n",
      "715/715 - 15s - loss: 0.0014 - val_loss: 0.0049 - 15s/epoch - 21ms/step\n",
      "Epoch 20/50\n",
      "715/715 - 15s - loss: 0.0014 - val_loss: 0.0048 - 15s/epoch - 21ms/step\n",
      "Epoch 21/50\n",
      "715/715 - 15s - loss: 0.0014 - val_loss: 0.0048 - 15s/epoch - 21ms/step\n",
      "Epoch 22/50\n",
      "715/715 - 15s - loss: 0.0014 - val_loss: 0.0047 - 15s/epoch - 21ms/step\n",
      "Epoch 23/50\n",
      "715/715 - 15s - loss: 0.0014 - val_loss: 0.0051 - 15s/epoch - 21ms/step\n",
      "Epoch 24/50\n",
      "715/715 - 15s - loss: 0.0014 - val_loss: 0.0055 - 15s/epoch - 21ms/step\n",
      "Epoch 25/50\n",
      "715/715 - 15s - loss: 0.0014 - val_loss: 0.0043 - 15s/epoch - 21ms/step\n",
      "Epoch 26/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0050 - 15s/epoch - 21ms/step\n",
      "Epoch 27/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0047 - 15s/epoch - 21ms/step\n",
      "Epoch 28/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0047 - 15s/epoch - 21ms/step\n",
      "Epoch 29/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0045 - 15s/epoch - 21ms/step\n",
      "Epoch 30/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0048 - 15s/epoch - 21ms/step\n",
      "Epoch 31/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0052 - 15s/epoch - 21ms/step\n",
      "Epoch 32/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0046 - 15s/epoch - 21ms/step\n",
      "Epoch 33/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0049 - 15s/epoch - 21ms/step\n",
      "Epoch 34/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0047 - 15s/epoch - 21ms/step\n",
      "Epoch 35/50\n",
      "715/715 - 15s - loss: 0.0013 - val_loss: 0.0051 - 15s/epoch - 21ms/step\n",
      "Epoch 36/50\n",
      "715/715 - 16s - loss: 0.0013 - val_loss: 0.0045 - 16s/epoch - 22ms/step\n",
      "Epoch 37/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0049 - 15s/epoch - 21ms/step\n",
      "Epoch 38/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0048 - 15s/epoch - 21ms/step\n",
      "Epoch 39/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0052 - 15s/epoch - 21ms/step\n",
      "Epoch 40/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0056 - 15s/epoch - 21ms/step\n",
      "Epoch 41/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0047 - 15s/epoch - 21ms/step\n",
      "Epoch 42/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0056 - 15s/epoch - 21ms/step\n",
      "Epoch 43/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0054 - 15s/epoch - 21ms/step\n",
      "Epoch 44/50\n",
      "715/715 - 16s - loss: 0.0012 - val_loss: 0.0049 - 16s/epoch - 22ms/step\n",
      "Epoch 45/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0045 - 15s/epoch - 21ms/step\n",
      "Epoch 46/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0047 - 15s/epoch - 21ms/step\n",
      "Epoch 47/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0044 - 15s/epoch - 21ms/step\n",
      "Epoch 48/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0051 - 15s/epoch - 21ms/step\n",
      "Epoch 49/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0050 - 15s/epoch - 21ms/step\n",
      "Epoch 50/50\n",
      "715/715 - 15s - loss: 0.0012 - val_loss: 0.0048 - 15s/epoch - 21ms/step\n",
      "Model saved to .\\Experiments\\colab\\models\\mtl_i118_g8_f_o1\n",
      "Training time: 12 min\n",
      "model_str_def: residual_gru_8_2\n",
      "len_stations:  23\n",
      "model is lstm or gru\n",
      "ndays:  118\n",
      "nwindows:  0\n",
      "window_size:  0\n",
      "output_shape:  (91421, 23)\n",
      "layer_type:  gru\n",
      "input shape:  944\n",
      "Model summary:\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 944)]        0           []                               \n",
      "                                                                                                  \n",
      " reshape_14 (Reshape)           (None, 118, 8)       0           ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 118, 8)       0           ['reshape_14[0][0]']             \n",
      "                                                                                                  \n",
      " permute_12 (Permute)           (None, 8, 118)       0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 8, 118)       14042       ['permute_12[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_17 (Flatten)           (None, 944)          0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 184)          173880      ['flatten_17[0][0]']             \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 8, 184)       21896       ['permute_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 184)          0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " gru_4 (GRU)                    (None, 8, 46)        32016       ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 46)           8510        ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_18 (Flatten)           (None, 368)          0           ['gru_4[0][0]']                  \n",
      "                                                                                                  \n",
      " mlp_output (Dense)             (None, 23)           1081        ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_output (Dense)            (None, 23)           8487        ['flatten_18[0][0]']             \n",
      "                                                                                                  \n",
      " res_add (Add)                  (None, 23)           0           ['mlp_output[0][0]',             \n",
      "                                                                  'lstm_output[0][0]']            \n",
      "                                                                                                  \n",
      " res_relu (LeakyReLU)           (None, 23)           0           ['res_add[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 259,912\n",
      "Trainable params: 245,870\n",
      "Non-trainable params: 14,042\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Creating new scalers\n",
      "Xscaler Min[0]: 3519.98323631286\n",
      "Xscaler Max[0]: 561182.0\n",
      "Epoch 1/50\n",
      "715/715 - 12s - loss: 0.0127 - val_loss: 0.0103 - 12s/epoch - 17ms/step\n",
      "Epoch 2/50\n",
      "715/715 - 9s - loss: 0.0065 - val_loss: 0.0064 - 9s/epoch - 13ms/step\n",
      "Epoch 3/50\n",
      "715/715 - 9s - loss: 0.0045 - val_loss: 0.0060 - 9s/epoch - 13ms/step\n",
      "Epoch 4/50\n",
      "715/715 - 9s - loss: 0.0031 - val_loss: 0.0056 - 9s/epoch - 13ms/step\n",
      "Epoch 5/50\n",
      "715/715 - 9s - loss: 0.0025 - val_loss: 0.0055 - 9s/epoch - 13ms/step\n",
      "Epoch 6/50\n",
      "715/715 - 9s - loss: 0.0022 - val_loss: 0.0055 - 9s/epoch - 13ms/step\n",
      "Epoch 7/50\n",
      "715/715 - 10s - loss: 0.0019 - val_loss: 0.0055 - 10s/epoch - 13ms/step\n",
      "Epoch 8/50\n",
      "715/715 - 9s - loss: 0.0017 - val_loss: 0.0051 - 9s/epoch - 13ms/step\n",
      "Epoch 9/50\n",
      "715/715 - 10s - loss: 0.0015 - val_loss: 0.0046 - 10s/epoch - 13ms/step\n",
      "Epoch 10/50\n",
      "715/715 - 9s - loss: 0.0014 - val_loss: 0.0061 - 9s/epoch - 13ms/step\n",
      "Epoch 11/50\n",
      "715/715 - 9s - loss: 0.0013 - val_loss: 0.0047 - 9s/epoch - 13ms/step\n",
      "Epoch 12/50\n",
      "715/715 - 9s - loss: 0.0012 - val_loss: 0.0045 - 9s/epoch - 13ms/step\n",
      "Epoch 13/50\n",
      "715/715 - 9s - loss: 0.0012 - val_loss: 0.0040 - 9s/epoch - 13ms/step\n",
      "Epoch 14/50\n",
      "715/715 - 9s - loss: 0.0011 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Epoch 15/50\n",
      "715/715 - 9s - loss: 0.0011 - val_loss: 0.0044 - 9s/epoch - 13ms/step\n",
      "Epoch 16/50\n",
      "715/715 - 9s - loss: 0.0011 - val_loss: 0.0044 - 9s/epoch - 13ms/step\n",
      "Epoch 17/50\n",
      "715/715 - 9s - loss: 9.7781e-04 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Epoch 18/50\n",
      "715/715 - 9s - loss: 9.6079e-04 - val_loss: 0.0040 - 9s/epoch - 13ms/step\n",
      "Epoch 19/50\n",
      "715/715 - 9s - loss: 9.4678e-04 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Epoch 20/50\n",
      "715/715 - 9s - loss: 8.9760e-04 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Epoch 21/50\n",
      "715/715 - 9s - loss: 8.8529e-04 - val_loss: 0.0041 - 9s/epoch - 13ms/step\n",
      "Epoch 22/50\n",
      "715/715 - 10s - loss: 8.5747e-04 - val_loss: 0.0036 - 10s/epoch - 14ms/step\n",
      "Epoch 23/50\n",
      "715/715 - 10s - loss: 8.4646e-04 - val_loss: 0.0041 - 10s/epoch - 14ms/step\n",
      "Epoch 24/50\n",
      "715/715 - 10s - loss: 8.0196e-04 - val_loss: 0.0039 - 10s/epoch - 13ms/step\n",
      "Epoch 25/50\n",
      "715/715 - 9s - loss: 8.0955e-04 - val_loss: 0.0041 - 9s/epoch - 13ms/step\n",
      "Epoch 26/50\n",
      "715/715 - 9s - loss: 7.6329e-04 - val_loss: 0.0042 - 9s/epoch - 13ms/step\n",
      "Epoch 27/50\n",
      "715/715 - 9s - loss: 7.9525e-04 - val_loss: 0.0042 - 9s/epoch - 13ms/step\n",
      "Epoch 28/50\n",
      "715/715 - 10s - loss: 7.6125e-04 - val_loss: 0.0040 - 10s/epoch - 14ms/step\n",
      "Epoch 29/50\n",
      "715/715 - 11s - loss: 7.2104e-04 - val_loss: 0.0035 - 11s/epoch - 15ms/step\n",
      "Epoch 30/50\n",
      "715/715 - 10s - loss: 7.1730e-04 - val_loss: 0.0037 - 10s/epoch - 13ms/step\n",
      "Epoch 31/50\n",
      "715/715 - 10s - loss: 7.1839e-04 - val_loss: 0.0039 - 10s/epoch - 13ms/step\n",
      "Epoch 32/50\n",
      "715/715 - 9s - loss: 6.8792e-04 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Epoch 33/50\n",
      "715/715 - 10s - loss: 6.9318e-04 - val_loss: 0.0043 - 10s/epoch - 13ms/step\n",
      "Epoch 34/50\n",
      "715/715 - 10s - loss: 6.4407e-04 - val_loss: 0.0037 - 10s/epoch - 13ms/step\n",
      "Epoch 35/50\n",
      "715/715 - 9s - loss: 6.8144e-04 - val_loss: 0.0036 - 9s/epoch - 13ms/step\n",
      "Epoch 36/50\n",
      "715/715 - 9s - loss: 6.4555e-04 - val_loss: 0.0045 - 9s/epoch - 13ms/step\n",
      "Epoch 37/50\n",
      "715/715 - 10s - loss: 6.3356e-04 - val_loss: 0.0038 - 10s/epoch - 14ms/step\n",
      "Epoch 38/50\n",
      "715/715 - 9s - loss: 6.3725e-04 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Epoch 39/50\n",
      "715/715 - 10s - loss: 6.2743e-04 - val_loss: 0.0038 - 10s/epoch - 14ms/step\n",
      "Epoch 40/50\n",
      "715/715 - 9s - loss: 6.1423e-04 - val_loss: 0.0040 - 9s/epoch - 13ms/step\n",
      "Epoch 41/50\n",
      "715/715 - 9s - loss: 6.1096e-04 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Epoch 42/50\n",
      "715/715 - 9s - loss: 6.1758e-04 - val_loss: 0.0037 - 9s/epoch - 13ms/step\n",
      "Epoch 43/50\n",
      "715/715 - 9s - loss: 5.8667e-04 - val_loss: 0.0041 - 9s/epoch - 13ms/step\n",
      "Epoch 44/50\n",
      "715/715 - 9s - loss: 5.7609e-04 - val_loss: 0.0038 - 9s/epoch - 13ms/step\n",
      "Epoch 45/50\n",
      "715/715 - 9s - loss: 5.7531e-04 - val_loss: 0.0036 - 9s/epoch - 13ms/step\n",
      "Epoch 46/50\n",
      "715/715 - 9s - loss: 5.7821e-04 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Epoch 47/50\n",
      "715/715 - 9s - loss: 5.9246e-04 - val_loss: 0.0040 - 9s/epoch - 13ms/step\n",
      "Epoch 48/50\n",
      "715/715 - 9s - loss: 5.6324e-04 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Epoch 49/50\n",
      "715/715 - 9s - loss: 5.9920e-04 - val_loss: 0.0034 - 9s/epoch - 13ms/step\n",
      "Epoch 50/50\n",
      "715/715 - 9s - loss: 5.2983e-04 - val_loss: 0.0039 - 9s/epoch - 13ms/step\n",
      "Model saved to .\\Experiments\\colab\\models\\mtl_i118_residual_gru_8_2\n",
      "Training time: 7 min\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for experiment in experiments:\n",
    "    print(\"experiment: \", experiment)\n",
    "\n",
    "    # create folders to save results\n",
    "    result_folders = ['models', 'results', 'images']\n",
    "    for result_folder in result_folders:\n",
    "        folder_path = os.path.join(local_root_path, \"Experiments\", experiment, result_folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    train_X = pd.read_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), index_col=0, compression=compression_opts)\n",
    "    train_Y = pd.read_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), index_col=0, compression=compression_opts)\n",
    "    test_X = pd.read_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), index_col=0, compression=compression_opts)\n",
    "    test_Y = pd.read_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), index_col=0, compression=compression_opts)\n",
    "\n",
    "    for  model_type, num_neurons_multiplier in models.items():\n",
    "        start = time.time()\n",
    "        model_str_def = build_model_string(model_type, num_neurons_multiplier)\n",
    "\n",
    "        full_model_str_def = 'i%d_' % (ndays + nwindows) + model_str_def\n",
    "\n",
    "        model_path_prefix = \"mtl_%s\" % (full_model_str_def)\n",
    "\n",
    "        print(\"model_str_def: %s\" % model_str_def)\n",
    "        model, xscaler, yscaler = build_or_load_model(model_path_prefix, full_model_str_def, num_neurons_multiplier, output_shape=train_Y.shape)\n",
    "\n",
    "        epochs = 50\n",
    "\n",
    "        print(\"Model summary:\")\n",
    "        print(model.summary())\n",
    "\n",
    "        if(xscaler is None or yscaler is None):\n",
    "            print(\"Creating new scalers\")\n",
    "\n",
    "        xscaler, yscaler = annutils.create_or_update_xyscaler(xscaler, yscaler, train_X, train_Y)\n",
    "        print(\"Xscaler Min[0]: %s\" % xscaler.min_val[0])\n",
    "        print(\"Xscaler Max[0]: %s\" % xscaler.max_val[0])\n",
    "\n",
    "        scaled_X = xscaler.transform(train_X)\n",
    "        scaled_Y = yscaler.transform(train_Y)\n",
    "\n",
    "        scaled_test_X = xscaler.transform(test_X)\n",
    "        scaled_test_Y = yscaler.transform(test_Y)\n",
    "\n",
    "        history = model.fit(\n",
    "            scaled_X,\n",
    "            scaled_Y,\n",
    "            epochs=epochs,\n",
    "            batch_size=128,\n",
    "            validation_data=(scaled_test_X, scaled_test_Y),\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\", patience=50, mode=\"min\", restore_best_weights=True),\n",
    "                tensorboard_cb\n",
    "            ],\n",
    "            verbose=2,\n",
    "        )\n",
    "\n",
    "        plot_history(history)\n",
    "\n",
    "        model_savepath = os.path.join(local_root_path, \"Experiments\", experiment, 'models', model_path_prefix)\n",
    "        annutils.save_model(model_savepath, model, xscaler, yscaler)\n",
    "        print('Model saved to %s' % model_savepath)\n",
    "        print('Training time: %d min' % ((time.time() - start) / 60))\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T21:31:15.633145500Z",
     "start_time": "2023-06-27T20:39:02.957794Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
