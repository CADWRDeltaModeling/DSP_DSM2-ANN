{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33e7463",
   "metadata": {
    "id": "e33e7463"
   },
   "source": [
    "# Train ANN with DSM2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65c382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T22:59:02.955029Z",
     "start_time": "2023-01-25T22:59:02.943081Z"
    }
   },
   "outputs": [],
   "source": [
    "local_root_path= \".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vsYLsgWMx2T-",
   "metadata": {
    "id": "vsYLsgWMx2T-"
   },
   "source": [
    "## Define Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RZffFrVYeP6C",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T22:59:02.984898Z",
     "start_time": "2023-01-25T22:59:02.958016Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1670888385428,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "RZffFrVYeP6C",
    "outputId": "9045c6e2-c759-4a4e-8ca8-37107b7d1246"
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Select NN architecture from:\n",
    "MLP, LSTM, GRU, ResNet, Res-LSTM, Res-GRU, Transformer\n",
    "'''\n",
    "model_type ='Res-LSTM'\n",
    "\n",
    "''' \n",
    "Numbers of neurons in the main branch\n",
    " - Provide two numbers for MLP, ResNet, Res-LSTM, Res-GRU, \n",
    " - Provide one number for LSTM, GRU.\n",
    " '''\n",
    "num_neurons_multiplier=[8, 2]\n",
    "\n",
    "'''\n",
    "Number of training epochs (Note: training will stop when reaching this number \n",
    "or test loss doesn't decrease for 50 epochs)\n",
    "'''\n",
    "epochs = 500\n",
    "\n",
    "'''\n",
    "Dataset to be split for training and test\n",
    "'''\n",
    "data_file = \"observed_data_daily.xlsx\"\n",
    "\n",
    "'''\n",
    "Ratio to split the dataset for training\n",
    "'''\n",
    "train_ratio = 0.7\n",
    "'''\n",
    "Which part of dataset used for for testing\n",
    "Available options: {'first', 'last', 'first_n_last', 'manual'}\n",
    "'''\n",
    "which_part_for_training = 'last'\n",
    "\n",
    "'''\n",
    "Dictionary of extra test sets\n",
    "- keys: test scenario name\n",
    "- values: names of excel files\n",
    "'''\n",
    "extra_test_sets = {'dcc0':\"dsm2_ann_inputs_dcc0.xlsx\",\n",
    "                   'smscg1':\"dsm2_ann_inputs_smscg1.xlsx\",\n",
    "                   'dcc1':\"dsm2_ann_inputs_dcc1.xlsx\",\n",
    "                   'smscg0':\"dsm2_ann_inputs_smscg0.xlsx\",}\n",
    "                   \n",
    "'''\n",
    "Whether to use hourly Martinez Stage as additional input\n",
    "'''\n",
    "use_hourly_stage_input = False\n",
    "\n",
    "''' \n",
    "Whether to (True) train models from scratch or (False) evaluate pre-trained models\n",
    "'''\n",
    "train_models = True\n",
    "\n",
    "'''Whether to save evaluation results (metric values, figures) to Google Drive'''\n",
    "save_results = True\n",
    "\n",
    "############################################\n",
    "############  Siyu's Notes:  ###############\n",
    "############################################\n",
    "'''\n",
    "In the pickle file, numerical results will be saved with the following suffixes:\n",
    "-- Training results (chronologically-split observed data): '_train'\n",
    "-- Test results (chronologically-split observed data): '_test'\n",
    "-- Results on addtional augmented data (defined by extra_test_sets): '_extra_test'\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################\n",
    "########## End of hyper-param definitions ##########\n",
    "####################################################\n",
    "\n",
    "initial_lr=0.001\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "'''\n",
    "Define the model architecture: (input layer will be build automatically)\n",
    "\n",
    "Supported abbreviation-layer pairs (detailed definitions can be found in \"Layer builders\" section)\n",
    "- 'lstm': keras.layers.LSTM\n",
    "- 'res': resnet block (basic_1d)\n",
    "- 'c1d': keras.layers.Conv1D\n",
    "- 'td': keras.layers.TimeDistributed\n",
    "- 'dr': keras.layers.Dropout\n",
    "- 'f': keras.layers.Flatten\n",
    "- 'g': keras.layers.GRU\n",
    "- 'd': keras.layers.Dense \n",
    "- 'o': keras.layers.Dense\n",
    "\n",
    "Usage of resnet blocks: res(num_of_filters)x(kernel_size)x(stride)x(stages)\n",
    "stages: # of resnet units in the block\n",
    "example: model_str_def = 'res10x3x1x1_f_d8_d2_o1'\n",
    "\n",
    "Usage of 1D conv layers: c1d(num_of_filters)x(kernel_size)x(stride)\n",
    "example: model_str_def = 'c1d10x3x1_c1d10x3x1_f_d8_d2_o1'\n",
    "\n",
    "'''\n",
    "model_type = model_type.lower()\n",
    "\n",
    "if model_type =='mlp':\n",
    "    ## 1. MLP Network\n",
    "    model_str_def = 'd8_d2_o1'\n",
    "\n",
    "elif model_type =='lstm':\n",
    "    # 2. LSTM Network\n",
    "    model_str_def = 'lstm%d_f_o1' % (num_neurons_multiplier[0])\n",
    "\n",
    "elif model_type =='gru':\n",
    "    # 3. GRU Network\n",
    "    model_str_def = 'g%d_f_o1' % (num_neurons_multiplier[0])\n",
    "\n",
    "elif model_type =='resnet':\n",
    "    # 4. ResNet\n",
    "    model_str_def = 'resnet%s' % ('_' + '_'.join([str(ii) for ii in num_neurons_multiplier]))\n",
    "    num_res_blocks=1\n",
    "\n",
    "elif model_type =='res-lstm':\n",
    "    # 5. Res-LSTM\n",
    "    model_str_def = 'residual_lstm%s' % ('_' + '_'.join([str(ii) for ii in num_neurons_multiplier]))\n",
    "\n",
    "elif model_type =='res-gru':\n",
    "    # 6. Res-GRU\n",
    "    model_str_def = 'residual_gru%s' % ('_' + '_'.join([str(ii) for ii in num_neurons_multiplier]))\n",
    "\n",
    "elif model_type =='transformer':\n",
    "    # 7. Transformer\n",
    "    model_str_def = 'transformer'\n",
    "else:\n",
    "    raise \"Model %s is not supported\" % model_type\n",
    "\n",
    "\n",
    "if model_type =='mlp':\n",
    "    # pre-processing option 1: apply pre-defined average windowing:\n",
    "    ndays=8\n",
    "    window_size=11\n",
    "    nwindows=10\n",
    "else:\n",
    "    # pre-processing option 2: directly use daily measurements as inputs\n",
    "    ndays=118\n",
    "    window_size=0\n",
    "    nwindows=0\n",
    "\n",
    "if 'daily' in data_file:\n",
    "    interval = '1D'\n",
    "elif 'hourly' in data_file:\n",
    "    interval = '1h'\n",
    "else:\n",
    "    raise \"Data file %s is not supported in this script\" % data_file\n",
    "\n",
    "print('Dataset: %s' % data_file)\n",
    "\n",
    "# percentile thresholds for ranged results\n",
    "percentiles = [0,0.75,0.95]  \n",
    "\n",
    "picked_training_years = [('1990-10-1','1991-9-30'),\n",
    "                         ('1992-10-1','1995-9-30'),\n",
    "                         ('1996-10-1','1998-9-30'),\n",
    "                         ('1999-10-1','2003-9-30'),\n",
    "                         ('2004-10-1','2006-9-30'),\n",
    "                         ('2007-10-1','2010-9-30'),\n",
    "                         ('2011-10-1','2013-9-30'),\n",
    "                         ('2014-10-1','2016-9-30'),\n",
    "                         ('2017-10-1','2019-9-30'),]\n",
    "resume_training = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Uue48h1hnUe",
   "metadata": {
    "id": "5Uue48h1hnUe"
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l9U0JyrTcWj6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T22:59:12.803861Z",
     "start_time": "2023-01-25T22:59:02.986889Z"
    },
    "executionInfo": {
     "elapsed": 6667,
     "status": "ok",
     "timestamp": 1670888392262,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "l9U0JyrTcWj6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install -q -U keras-tuner\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras import layers\n",
    "#import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GvLrxElFxryN",
   "metadata": {
    "id": "GvLrxElFxryN"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034249e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T22:59:12.833730Z",
     "start_time": "2023-01-25T22:59:12.807844Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "num_sheets = 9\n",
    "\n",
    "num_dataset = {\"observed_data_daily.xlsx\":'daily',\n",
    "               \"observed_data_hourly.xlsx\":'hourly'}\n",
    "sys.path.append(local_root_path)\n",
    "import annutils\n",
    "data_path = os.path.join(local_root_path,data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lWskSM_sjwMb",
   "metadata": {
    "id": "lWskSM_sjwMb"
   },
   "source": [
    "### Read Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eDPf7Gzf0GQb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T22:59:16.087321Z",
     "start_time": "2023-01-25T22:59:12.835721Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4984,
     "status": "ok",
     "timestamp": 1670888399860,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "eDPf7Gzf0GQb",
    "outputId": "2c7f7a3f-a52c-4497-80cf-436b56d06672"
   },
   "outputs": [],
   "source": [
    "if data_file != \"observed_data_daily.xlsx\":\n",
    "    # sorted by EC median, small to large\n",
    "    observed_stations_ordered_by_median = ['RSMKL008', 'RSAN032', 'RSAN037', 'RSAC092', 'SLTRM004', 'ROLD024',\n",
    "                                           'CHVCT000', 'RSAN018', 'CHSWP003', 'CHDMC006', 'SLDUT007', 'RSAN072',\n",
    "                                           'OLD_MID', 'RSAN058', 'ROLD059', 'RSAN007', 'RSAC081', 'SLMZU025',\n",
    "                                           'RSAC075', 'SLMZU011', 'SLSUS012', 'SLCBN002', 'RSAC064']\n",
    "else:\n",
    "    dfouts=pd.read_excel(data_path,num_sheets-1,index_col=0,parse_dates=True)\n",
    "\n",
    "    output_stations_w_duplicates_for_time = dfouts.columns[~dfouts.columns.str.contains('_dup')]\n",
    "    observed_stations_ordered_by_median = list(set(output_stations_w_duplicates_for_time))\n",
    "    print('Found %d stations:' % len(observed_stations_ordered_by_median))\n",
    "    print(observed_stations_ordered_by_median)\n",
    "\n",
    "station_orders = {s:ii for ii, s in enumerate(observed_stations_ordered_by_median)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72szFV0iPb6",
   "metadata": {
    "id": "f72szFV0iPb6"
   },
   "source": [
    "### (Optional) Read Hourly Martinez Stage as Additional Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ciu7v1_2CH",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T22:59:16.102255Z",
     "start_time": "2023-01-25T22:59:16.089312Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1670888399861,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "09ciu7v1_2CH"
   },
   "outputs": [],
   "source": [
    "if use_hourly_stage_input:\n",
    "    assert len(extra_test_sets) == 0, 'Cannot include extra test sets when incorporating Martinez hourly stage in inputs'\n",
    "    martinez = pd.read_csv(os.path.join(local_root_path, \"martinez_stage1hr.csv\"),comment='#', parse_dates=['Time'], index_col='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fAj2onMjiy6d",
   "metadata": {
    "id": "fAj2onMjiy6d"
   },
   "source": [
    "### Read Test Sets\n",
    "\n",
    "\n",
    "*   Test Split from Base Data (Defined by *data_file*, split according to )\n",
    "*   Augmented Data (Defined in *extra_test_sets*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hWhNLHvLx_Cw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:12.993054Z",
     "start_time": "2023-01-25T22:59:16.104247Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129851,
     "status": "ok",
     "timestamp": 1670888529698,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "hWhNLHvLx_Cw",
    "outputId": "75c99ba6-4bf8-42fa-83fa-13fa74eb9a3e"
   },
   "outputs": [],
   "source": [
    "output_stations = None\n",
    "\n",
    "\n",
    "xscaler = None\n",
    "yscaler = None\n",
    "\n",
    "for file in list(extra_test_sets.values()) + [data_file]:\n",
    "    data_path = os.path.join(local_root_path,file)\n",
    "\n",
    "    dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "    dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    if output_stations is None:\n",
    "        if len(extra_test_sets) > 0:\n",
    "            # read station names\n",
    "            output_stations = list(dfouts.columns)\n",
    "            name_mapping = {}\n",
    "            for s in output_stations:\n",
    "                for ss in observed_stations_ordered_by_median:\n",
    "                    if ss in s:\n",
    "                        name_mapping[s] = ss\n",
    "            output_stations = list(name_mapping.values())\n",
    "        else:\n",
    "            output_stations = observed_stations_ordered_by_median\n",
    "    \n",
    "    output_stations = [x for x in sorted(output_stations, key=lambda s: station_orders[s.split('-')[0]],reverse=False)]\n",
    "\n",
    "    if file in extra_test_sets.values():\n",
    "        dfouts = dfouts.rename(columns=name_mapping)[output_stations]\n",
    "    else:\n",
    "        dfouts = dfouts[output_stations]\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    if xscaler is None:\n",
    "        xscaler,yscaler = annutils.create_xyscaler([dfinps],[dfouts])\n",
    "    else:\n",
    "        temp_xscaler,temp_yscaler = annutils.create_xyscaler([dfinps],[dfouts])\n",
    "        if np.any(np.isnan(temp_xscaler.max_val)) or np.any(np.isnan(temp_yscaler.max_val)):\n",
    "            break\n",
    "        xscaler.update(temp_xscaler)\n",
    "        yscaler.update(temp_yscaler)\n",
    "\n",
    "x_extrat = None\n",
    "y_extrat = None\n",
    "\n",
    "for file in extra_test_sets.values():\n",
    "    data_path = os.path.join(local_root_path,file)\n",
    "\n",
    "    dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "    dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = dfouts.rename(columns=name_mapping)[output_stations]\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    if x_extrat is None:\n",
    "        (x_extrat, y_extrat), (_, _), _, _ = \\\n",
    "            annutils.create_training_sets([dfinps],\n",
    "                                          [dfouts],\n",
    "                                          train_frac=1,\n",
    "                                          ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                          xscaler=xscaler,yscaler=yscaler)\n",
    "    else:\n",
    "        (xt, yt), (_, _), _, _ = \\\n",
    "            annutils.create_training_sets([dfinps],\n",
    "                                          [dfouts],\n",
    "                                          train_frac=1,\n",
    "                                          ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                          xscaler=xscaler,yscaler=yscaler)\n",
    "        x_extrat = pd.concat([x_extrat,xt],axis=0)\n",
    "        y_extrat = pd.concat([y_extrat,yt],axis=0)\n",
    "        del xt, yt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44pRCVhhiLcP",
   "metadata": {
    "id": "44pRCVhhiLcP"
   },
   "source": [
    "### Load Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maYSshoEBNZe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:22.171679Z",
     "start_time": "2023-01-25T23:01:12.995042Z"
    },
    "executionInfo": {
     "elapsed": 8224,
     "status": "ok",
     "timestamp": 1670888537904,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "maYSshoEBNZe"
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(local_root_path,data_file)\n",
    "dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "if data_file == \"observed_data_1h.xlsx\":\n",
    "    index = pd.date_range(start=dfouts.index[0].date(), end=dfouts.index[-2].date(), freq='D')\n",
    "    df2 = pd.DataFrame(dfouts.loc[:,observed_stations_ordered_by_median][index[0]:(index[-1] + pd.Timedelta('23 h'))].values.reshape([len(index),24*len(output_stations)]),\n",
    "                       columns=['%s-%d' % (station, ii) for ii in range(24) for station in observed_stations_ordered_by_median],\n",
    "                       index=index)\n",
    "    output_col_list = df2.columns\n",
    "    df_inpout = pd.concat(dflist[0:(num_sheets-1)]+[df2],axis=1).dropna(axis=0)\n",
    "    dfinps = df_inpout.loc[:,~df_inpout.columns.isin(output_col_list)]\n",
    "    dfouts = df_inpout.loc[:,df_inpout.columns.isin(output_col_list)]\n",
    "    del df2\n",
    "else:\n",
    "    df_inpout = pd.concat(dflist[0:(num_sheets)],axis=1).dropna(axis=0)\n",
    "    dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = dfouts[output_stations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YLpmSrt8yNrL",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:22.187607Z",
     "start_time": "2023-01-25T23:01:22.173669Z"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1670888537905,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "YLpmSrt8yNrL"
   },
   "outputs": [],
   "source": [
    "if use_hourly_stage_input:\n",
    "    index = pd.date_range(start=dfouts.index[0].date(), end=dfouts.index[-1].date(), freq='D')\n",
    "    martinez_input = pd.DataFrame(martinez[index[0]:(index[-1] + pd.Timedelta('23 h'))].values.reshape([-1,24]),\n",
    "                        columns=['Martinez_Stage-%d' % ii for ii in range(24)],\n",
    "                        index=index)\n",
    "    stage_scaler = annutils.myscaler()\n",
    "    _ = stage_scaler.fit_transform(martinez_input)\n",
    "    normalized_martinez_input = stage_scaler.transform(martinez_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CSZh3_--OuJ5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:22.218474Z",
     "start_time": "2023-01-25T23:01:22.191592Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1670888537906,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "CSZh3_--OuJ5",
    "outputId": "a81430bb-1efb-4c06-de2a-1523497a5bb8"
   },
   "outputs": [],
   "source": [
    "dfinps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b598dae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:22.250330Z",
     "start_time": "2023-01-25T23:01:22.220466Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1670888537907,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "3b598dae",
    "outputId": "0f9de835-2c33-47fc-ea3d-fd541509245b"
   },
   "outputs": [],
   "source": [
    "dfouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209223b",
   "metadata": {
    "id": "a209223b"
   },
   "source": [
    "## Setup Training and Test Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15b9e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:22.266265Z",
     "start_time": "2023-01-25T23:01:22.252322Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1670888538169,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "1b15b9e9",
    "outputId": "e2e5d318-f94b-4824-9f2c-d1c52eb0d66e"
   },
   "outputs": [],
   "source": [
    "start_year = max(dfinps.index[0].year, dfouts.index[0].year)\n",
    "end_year = min(dfinps.index[-1].year, dfouts.index[-1].year)\n",
    "if data_file == \"observed_data_daily.xlsx\":\n",
    "    calib_slice = slice(str(start_year), str(int(start_year+train_ratio*(end_year-start_year))))\n",
    "    valid_slice = slice(str(int(start_year+train_ratio*(end_year-start_year))+1), str(end_year))\n",
    "else:\n",
    "    if which_part_for_training =='first':\n",
    "        calib_slice = slice(str(start_year), str(int(start_year+train_ratio*(end_year-start_year))))\n",
    "        valid_slice = slice(str(int(start_year+train_ratio*(end_year-start_year))+1), str(end_year))\n",
    "    elif which_part_for_training =='last':\n",
    "        calib_slice = slice(str(int(start_year+(1-train_ratio)*(end_year-start_year))+1), str(end_year))\n",
    "        valid_slice = slice(str(start_year), str(int(start_year+(1-train_ratio)*(end_year-start_year))))\n",
    "    elif which_part_for_training =='first_n_last':\n",
    "        calib_slice = [slice(str(start_year),\n",
    "                             str(int(start_year+train_ratio/2*(end_year-start_year)))),\n",
    "                       slice(str(int(start_year+(1-train_ratio/2)*(end_year-start_year))+1),\n",
    "                             str(end_year))]\n",
    "        valid_slice = slice(str(int(start_year+train_ratio/2*(end_year-start_year))+1),\n",
    "                            str(int(start_year+(1-train_ratio/2)*(end_year-start_year))))\n",
    "    elif which_part_for_training =='manual' and picked_training_years is not None:\n",
    "        calib_slice = [slice(str(start_year), str(end_year)) for (start_year,end_year) in picked_training_years]\n",
    "        valid_slice = [slice(start_year, end_year) for ((_,start_year),(end_year,_)) in zip([(None,'1989-10-1'),]+picked_training_years,picked_training_years+[('2020-9-30',None),])]\n",
    "    else:\n",
    "        raise Exception('Unknown data splitting method')\n",
    "print('Training set:', calib_slice)\n",
    "print('Test set:', valid_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pVb2f40mh0ll",
   "metadata": {
    "id": "pVb2f40mh0ll"
   },
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fLtxSBKTuF1b",
   "metadata": {
    "id": "fLtxSBKTuF1b"
   },
   "source": [
    "### ResNet Block Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VmzBaCvGuFNk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:22.297131Z",
     "start_time": "2023-01-25T23:01:22.268258Z"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1670888538169,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "VmzBaCvGuFNk"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"kernel_initializer\": \"he_normal\"\n",
    "}\n",
    "\n",
    "def basic_1d(\n",
    "    filters,\n",
    "    stage=0,\n",
    "    block=0,\n",
    "    kernel_size=3,\n",
    "    numerical_name=False,\n",
    "    stride=None,\n",
    "    force_identity_shortcut=False\n",
    "):\n",
    "    \"\"\"\n",
    "    A one-dimensional basic block.\n",
    "    :param filters: the output’s feature space\n",
    "    :param stage: int representing the stage of this block (starting from 0)\n",
    "    :param block: int representing this block (starting from 0)\n",
    "    :param kernel_size: size of the kernel\n",
    "    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n",
    "    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        if block != 0 or stage == 0:\n",
    "            stride = 1\n",
    "        else:\n",
    "            stride = 2\n",
    "\n",
    "    # axis = -1 if keras.backend.image_data_format() == \"channels_last\" else 1\n",
    "\n",
    "\n",
    "    if block > 0 and numerical_name:\n",
    "        block_char = \"b{}\".format(block)\n",
    "    else:\n",
    "        block_char = chr(ord('a') + block)\n",
    "\n",
    "    stage_char = str(stage + 2)\n",
    "\n",
    "    def f(x):\n",
    "        y = keras.layers.ZeroPadding1D(padding=1,name=\"padding{}{}_branch2a\".format(stage_char, block_char))(x)\n",
    "        y = keras.layers.Conv1D(filters,kernel_size,strides=stride,use_bias=False,\n",
    "                                name=\"res{}{}_branch2a\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "        y = keras.layers.Activation(\"relu\", name=\"res{}{}_branch2a_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        y = keras.layers.ZeroPadding1D(padding=1,name=\"padding{}{}_branch2b\".format(stage_char, block_char))(y)\n",
    "        y = keras.layers.Conv1D(filters,kernel_size,use_bias=False,\n",
    "                                name=\"res{}{}_branch2b\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "\n",
    "        if block != 0 or force_identity_shortcut:\n",
    "            shortcut = x\n",
    "        else:\n",
    "            shortcut = keras.layers.Conv1D(filters,1,strides=stride,use_bias=False,\n",
    "                                           name=\"res{}{}_branch1\".format(stage_char, block_char),\n",
    "                                           **parameters)(x)\n",
    "            shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = keras.layers.Add(name=\"res{}{}\".format(stage_char, block_char))([y, shortcut])\n",
    "        \n",
    "        y = keras.layers.Activation(\"relu\",name=\"res{}{}_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def bottleneck_1d(\n",
    "    filters,\n",
    "    stage=0,\n",
    "    block=0,\n",
    "    kernel_size=3,\n",
    "    numerical_name=False,\n",
    "    stride=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    A one-dimensional bottleneck block.\n",
    "    :param filters: the output’s feature space\n",
    "    :param stage: int representing the stage of this block (starting from 0)\n",
    "    :param block: int representing this block (starting from 0)\n",
    "    :param kernel_size: size of the kernel\n",
    "    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n",
    "    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = 1 if block != 0 or stage == 0 else 2\n",
    "\n",
    "    # axis = -1 if keras.backend.image_data_format() == \"channels_last\" else 1\n",
    "\n",
    "\n",
    "    if block > 0 and numerical_name:\n",
    "        block_char = \"b{}\".format(block)\n",
    "    else:\n",
    "        block_char = chr(ord('a') + block)\n",
    "\n",
    "    stage_char = str(stage + 2)\n",
    "\n",
    "    def f(x):\n",
    "        y = keras.layers.Conv1D(filters,1,strides=stride,use_bias=False,\n",
    "                                name=\"res{}{}_branch2a\".format(stage_char, block_char),\n",
    "                                **parameters)(x)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "        y = keras.layers.Activation(\"relu\",name=\"res{}{}_branch2a_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        y = keras.layers.ZeroPadding1D(padding=1,name=\"padding{}{}_branch2b\".format(stage_char, block_char))(y)\n",
    "        y = keras.layers.Conv1D(filters,kernel_size,use_bias=False,\n",
    "                                name=\"res{}{}_branch2b\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "        y = keras.layers.Activation(\"relu\",name=\"res{}{}_branch2b_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        y = keras.layers.Conv1D(filters * 4, 1, use_bias=False,\n",
    "                                name=\"res{}{}_branch2c\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "\n",
    "        if block == 0:\n",
    "            shortcut = keras.layers.Conv1D(filters * 4, 1, strides=stride, use_bias=False,\n",
    "                                           name=\"res{}{}_branch1\".format(stage_char, block_char),\n",
    "                                           **parameters)(x)\n",
    "            shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "        else:\n",
    "            shortcut = x\n",
    "\n",
    "        y = keras.layers.Add(name=\"res{}{}\".format(stage_char, block_char))([y, shortcut])\n",
    "        y = keras.layers.Activation(\"relu\",name=\"res{}{}_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1I1z9y8qDJkv",
   "metadata": {
    "id": "1I1z9y8qDJkv"
   },
   "source": [
    "### Layer Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803eca3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:22.326992Z",
     "start_time": "2023-01-25T23:01:22.299118Z"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1670888538170,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "803eca3b"
   },
   "outputs": [],
   "source": [
    "\"\"\"# Custom loss function\"\"\"\n",
    "def mse_loss_masked(y_true, y_pred):\n",
    "    squared_diff = tf.reduce_sum(tf.math.squared_difference(y_pred[y_true>0],y_true[y_true>0]))\n",
    "    return squared_diff/(tf.reduce_sum(tf.cast(y_true>0, tf.float32))+0.01)\n",
    "\n",
    "# Define Sequential model\n",
    "NFEATURES = (num_sheets-1) # * (ndays + nwindows) \n",
    "def build_layer_from_string_def(s='i120',width_multiplier=1,\n",
    "                                block=0,\n",
    "                                force_identity_shortcut=False,\n",
    "                                return_sequences_rnn=True):\n",
    "    if s[0:4] == 'lstm':\n",
    "        return layers.LSTM(units = int(s[4:])*width_multiplier, return_sequences=return_sequences_rnn, activation='sigmoid')\n",
    "    elif s[0:3] == 'res':\n",
    "        fields = s[3:].split('x')\n",
    "        return basic_1d(filters=int(fields[0]),\n",
    "                        stage=int(fields[3]),\n",
    "                        block=block,\n",
    "                        kernel_size=int(fields[1]),\n",
    "                        stride=int(fields[2]),\n",
    "                        force_identity_shortcut=force_identity_shortcut)\n",
    "    elif s[0:3] == 'c1d':\n",
    "        fields = s[3:].split('x')\n",
    "        return keras.layers.Conv1D(filters=int(fields[0]), kernel_size=int(fields[1]), strides=int(fields[2]),\n",
    "                                   padding='causal', activation='linear')\n",
    "    elif s[0:2] == 'td':\n",
    "        return keras.layers.TimeDistributed(keras.layers.Dense(int(s[2:]), activation='elu'))\n",
    "    elif s[0:2] == 'dr':\n",
    "        return keras.layers.Dropout(float(s[2:]))\n",
    "    # elif s[0] == 'i':\n",
    "    #     return keras.layers.InputLayer(input_shape=[int(s[1:]), NFEATURES])\n",
    "    elif s[0] == 'f':\n",
    "        return keras.layers.Flatten()\n",
    "    elif s[0] == 'g':\n",
    "        return keras.layers.GRU(int(s[1:])*width_multiplier, return_sequences=return_sequences_rnn, activation='relu')\n",
    "    elif s[0] == 'd':\n",
    "        return keras.layers.Dense(int(s[1:])*width_multiplier, activation='elu')\n",
    "    elif s[0] == 'o':\n",
    "        return keras.layers.Dense(int(s[1:])*width_multiplier, activation='linear')\n",
    "    else:\n",
    "        raise Exception('Unknown layer def: %s' % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cKOdJ4_aiB_c",
   "metadata": {
    "id": "cKOdJ4_aiB_c"
   },
   "source": [
    "### Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v-ai_17piaeS",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:22.374782Z",
     "start_time": "2023-01-25T23:01:22.328983Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1670888538171,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "v-ai_17piaeS"
   },
   "outputs": [],
   "source": [
    "def build_model_from_string_def(strdef='i120_f_d4_d2_d1',width_multiplier=1):\n",
    "    layer_strings = strdef.split('_')\n",
    "    inputs = keras.layers.Input(shape=[int(layer_strings[0][1:]) * NFEATURES + 24 * use_hourly_stage_input])\n",
    "    x = None\n",
    "    prev_conv_output_num_of_channels = None\n",
    "    return_sequences_rnn = None\n",
    "    for block,f in enumerate(layer_strings[1:-1]):\n",
    "        if x is None:\n",
    "            if f.startswith(('lstm','g')):         \n",
    "                # these layers require 2D inputs and permutation\n",
    "                x = layers.Reshape((ndays+nwindows,NFEATURES))(inputs)\n",
    "                prev_conv_output_num_of_channels = NFEATURES\n",
    "                x = layers.Permute((2,1))(x)\n",
    "                # return_sequences_rnn = layer_strings[block+2].startswith(('lstm','g','res','c1d'))\n",
    "                return_sequences_rnn = True\n",
    "            elif f.startswith(('res','c1d')):\n",
    "                # these layers require 2D inputs\n",
    "                x = layers.Reshape((ndays+nwindows,NFEATURES))(inputs)\n",
    "                prev_conv_output_num_of_channels = NFEATURES\n",
    "            else:\n",
    "                x = inputs\n",
    "\n",
    "\n",
    "        x = build_layer_from_string_def(f,width_multiplier,block,\n",
    "                                        force_identity_shortcut=(f.startswith('res') and prev_conv_output_num_of_channels==int(f[3:].split('x')[0])),\n",
    "                                        return_sequences_rnn=return_sequences_rnn)(x)\n",
    "        if f.startswith('lstm'):         \n",
    "            prev_conv_output_num_of_channels=int(f[4:])\n",
    "        elif f.startswith('res') or f.startswith('c1d'):\n",
    "            prev_conv_output_num_of_channels=int(f[3:].split('x')[0])\n",
    "\n",
    "\n",
    "    outputs = keras.layers.Dense(int(layer_strings[-1][1:])*width_multiplier, activation='linear')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "def build_resnet_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid',\n",
    "                       filters=num_sheets-1, kernel_size=3, stride=1,\n",
    "                       num_res_blocks=1):\n",
    "    inputs = layers.Input(shape=NFEATURES* (ndays + nwindows) + 24 * use_hourly_stage_input)\n",
    "    x = layers.Reshape((ndays+nwindows,NFEATURES))(inputs)\n",
    "    for ii in range(num_res_blocks - 1):\n",
    "        # TODO: think about conv filter numbers and kernel sizes\n",
    "        intermediate_features = layers.ZeroPadding1D(padding=1,name=\"padding%d_branch2a\" %ii)(x)\n",
    "        intermediate_features = layers.Conv1D(filters=NFEATURES,kernel_size=2,strides=1,use_bias=False,\n",
    "                          name=\"res%d_branch2a\" %ii)(intermediate_features)\n",
    "        intermediate_features = layers.BatchNormalization()(intermediate_features)\n",
    "        intermediate_features = layers.Activation(\"relu\", name=\"res%d_branch2a_relu\" % ii)(intermediate_features)\n",
    "\n",
    "        intermediate_features = layers.Conv1D(filters=NFEATURES,kernel_size=2,strides=1,use_bias=False,\n",
    "                          name=\"res%d_branch2b\" %ii)(intermediate_features)\n",
    "        intermediate_features = layers.BatchNormalization()(intermediate_features)\n",
    "        intermediate_features = layers.Activation(\"relu\", name=\"res%d_branch2b_relu\" % ii)(intermediate_features)\n",
    "\n",
    "        shortcut = x\n",
    "        x = layers.Add(name=\"res%d_add\" % ii)([intermediate_features, shortcut])\n",
    "\n",
    "    y = layers.ZeroPadding1D(padding=1,name=\"padding%d_branch2a\" % num_res_blocks)(x)\n",
    "    y = layers.Conv1D(filters,kernel_size,strides=stride,use_bias=False,\n",
    "                                name=\"res%d_branch2a\" % num_res_blocks)(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation(\"relu\", name=\"res%d_branch2a_relu\" % num_res_blocks)(y)\n",
    "\n",
    "    y = layers.ZeroPadding1D(padding=1,name=\"padding%d_branch2b\" % num_res_blocks)(y)\n",
    "    y = layers.Conv1D(filters,kernel_size,use_bias=False,\n",
    "                            name=\"res%d_branch2b\" % num_res_blocks)(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(nhidden1, activation=act_func)(y)\n",
    "\n",
    "    shortcut = inputs\n",
    "    shortcut = layers.Dense(nhidden1, activation=act_func)(shortcut)\n",
    "\n",
    "    y = layers.Add(name=\"res%d_add\" % num_res_blocks)([y, shortcut])\n",
    "    \n",
    "    y = layers.Activation(\"relu\",name=\"res_relu\")(y)\n",
    "\n",
    "\n",
    "    y = layers.Dense(nhidden2, activation=act_func)(y)\n",
    "    outputs= layers.Dense(output_shape, activation=keras.activations.linear,name='output')(y)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "def build_residual_lstm_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid',layer_type='lstm',conv_init=None):\n",
    "    rnn_layer = layers.LSTM if layer_type == 'lstm' else layers.GRU\n",
    "    if use_hourly_stage_input:\n",
    "        inputs = layers.Input(shape=NFEATURES*(ndays+nwindows) + 24)\n",
    "        intermediate = layers.Reshape((NFEATURES*(ndays+nwindows) + 24, 1))(inputs)\n",
    "        first_half = tf.keras.layers.Cropping1D(cropping=(0,24))(intermediate)\n",
    "        x = layers.Reshape((ndays+nwindows,num_sheets-1))(first_half)\n",
    "\n",
    "        second_half = tf.keras.layers.Cropping1D(cropping=(NFEATURES*(ndays+nwindows),0))(intermediate)\n",
    "    else:\n",
    "        inputs = layers.Input(shape=NFEATURES*(ndays+nwindows))\n",
    "        x = layers.Reshape((ndays+nwindows,num_sheets-1))(inputs)\n",
    "    x = layers.Permute((2,1))(x)\n",
    "\n",
    "    y = tf.keras.layers.Conv1D(8+10,1, activation='relu',\n",
    "                            kernel_initializer=conv_init,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),\n",
    "                            trainable=False)(x)\n",
    "\n",
    "    y = layers.Flatten()(y)\n",
    "    if use_hourly_stage_input:\n",
    "        y = tf.keras.layers.concatenate([y,tf.keras.layers.Reshape((24,))(second_half)])\n",
    "    y = layers.Dense(nhidden1, activation=act_func)(y)\n",
    "    y = layers.Dense(nhidden2, activation=act_func)(y)\n",
    "    y = layers.Dense(output_shape, activation=keras.activations.linear,name='mlp_output')(y)\n",
    "\n",
    "\n",
    "    shortcut = layers.Dense(nhidden1, activation=act_func)(x)\n",
    "    shortcut = rnn_layer(units = output_shape*2, activation=act_func,return_sequences=True)(shortcut)\n",
    "    shortcut = layers.Flatten()(shortcut)\n",
    "    shortcut = layers.Dense(output_shape, activation=keras.activations.linear,name='lstm_output')(shortcut)\n",
    "    if use_hourly_stage_input:\n",
    "        stage_shortcut = layers.Reshape((1,24))(second_half)\n",
    "        stage_shortcut = rnn_layer(units = output_shape*2, activation=act_func,return_sequences=True)(stage_shortcut)\n",
    "        stage_shortcut = layers.Flatten()(stage_shortcut)\n",
    "        stage_shortcut = layers.Dense(output_shape, activation=keras.activations.linear,name='lstm_stage_output')(stage_shortcut)\n",
    "        shortcut = layers.Add(name=\"stage_add\")([shortcut,stage_shortcut])\n",
    "\n",
    "    outputs = layers.Add(name=\"res_add\")([y, shortcut])\n",
    "    # outputs = layers.Activation(\"relu\",name=\"res_relu\")(outputs)\n",
    "    outputs = layers.LeakyReLU(alpha=0.3,name=\"res_relu\")(outputs)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_transformer(head_size,\n",
    "                      num_heads,\n",
    "                      ff_dim,\n",
    "                      num_transformer_blocks,\n",
    "                      mlp_units,\n",
    "                      output_shape,\n",
    "                      dropout=0,\n",
    "                      mlp_dropout=0):\n",
    "    inputs = keras.Input(shape=NFEATURES*(ndays+nwindows) + 24 * use_hourly_stage_input)\n",
    "    x = layers.Reshape((ndays+nwindows,NFEATURES))(inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(output_shape)(x)\n",
    "    outputs = layers.LeakyReLU(alpha=0.3,name=\"res_relu\")(outputs)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1K-4mn8QRLP1",
   "metadata": {
    "id": "1K-4mn8QRLP1"
   },
   "source": [
    "Re-order stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AuQjo5zt3Q12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:01:22.390712Z",
     "start_time": "2023-01-25T23:01:22.376774Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1670888538171,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "AuQjo5zt3Q12"
   },
   "outputs": [],
   "source": [
    "group_stations = False\n",
    "station_without_groups = {'all':output_stations}\n",
    "station_with_groups = {'G1':['SSS','RSAC101','RSMKL008'],\n",
    "                  'G2':['Old_River_Hwy_4','Middle_River_Intake','CCWD_Rock','SLTRM004','RSAN032','RSAN037','SLDUT007','ROLD024','RSAN058','RSAN072','OLD_MID','ROLD059','CHDMC006','CHSWP003','CHVCT000'],\n",
    "                  'G3':['SLCBN002','SLSUS012','SLMZU011','SLMZU025','RSAC064','RSAC075','RSAC081','RSAN007','RSAC092','RSAN018','Martinez']}\n",
    "final_groups = {False: station_without_groups, \n",
    "                True: station_with_groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1n-PdzKDESd",
   "metadata": {
    "id": "o1n-PdzKDESd"
   },
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2383d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:04:26.778342Z",
     "start_time": "2023-01-25T23:01:22.392703Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "executionInfo": {
     "elapsed": 4925,
     "status": "ok",
     "timestamp": 1670888543081,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "2be2383d",
    "outputId": "25f41feb-293f-4747-af5f-5b87522c91e9"
   },
   "outputs": [],
   "source": [
    "full_model_str_def = 'i%d_'%(ndays + nwindows) +model_str_def\n",
    "if train_models:\n",
    "    for group_name, stations in final_groups[group_stations].items():\n",
    "        # prepare dataset\n",
    "        selected_output_variables = []\n",
    "        for station in stations:\n",
    "            for output in output_stations:\n",
    "                if station in output:\n",
    "                    if interval == '1h':\n",
    "                        selected_output_variables.extend(['%s-%d' % (output, hh) for hh in range(24)])\n",
    "                    elif interval == '1D':\n",
    "                        selected_output_variables.append(output)\n",
    "        \n",
    "        selected_output_variables = [x for x in sorted(selected_output_variables, key=lambda s: station_orders[s.split('-')[0]],reverse=False)]\n",
    "\n",
    "        model_path_prefix = \"mtl_%s_%s_%s_%s\" % (group_name, ('stage_n_' if use_hourly_stage_input else '') + full_model_str_def, num_dataset[data_file],which_part_for_training+'_chronological')\n",
    "\n",
    "        print('Training MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "\n",
    "        print([station.replace('target/','').replace('target','') for station in selected_output_variables])\n",
    "        \n",
    "\n",
    "        (xallc, yallc), (xallv, yallv), _, _ = \\\n",
    "        annutils.create_training_sets([dfinps],\n",
    "                                    [dfouts[output_stations]],\n",
    "                                    calib_slice=calib_slice,\n",
    "                                    valid_slice=valid_slice,\n",
    "                                    ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                    xscaler=xscaler, yscaler=yscaler)\n",
    "        if use_hourly_stage_input:\n",
    "            xallc = pd.merge(xallc,normalized_martinez_input, left_index=True, right_index=True)\n",
    "            xallv = pd.merge(xallv,normalized_martinez_input, left_index=True, right_index=True)\n",
    "\n",
    "        if model_str_def.startswith('resnet'):\n",
    "            model = build_resnet_model(nhidden1=num_neurons_multiplier[0]*yallc.shape[1], nhidden2=num_neurons_multiplier[1]*yallc.shape[1], output_shape=yallc.shape[1],\n",
    "                                        num_res_blocks=num_res_blocks)\n",
    "        elif model_str_def.startswith('residual_lstm') or model_str_def.startswith('residual_gru'):\n",
    "            conv_init = tf.constant_initializer(annutils.conv_filter_generator(ndays=8,window_size=11,nwindows=10))\n",
    "\n",
    "            model = build_residual_lstm_model(num_neurons_multiplier[0]*len(output_stations),\n",
    "                                    num_neurons_multiplier[1]*len(output_stations),\n",
    "                                    output_shape=yallc.shape[1],\n",
    "                                    act_func='sigmoid',\n",
    "                                    layer_type=model_str_def.lower().split('_')[1],\n",
    "                                    conv_init=conv_init)\n",
    "        elif model_str_def == 'transformer':\n",
    "            model = build_transformer(head_size=256,\n",
    "                                    num_heads=4,\n",
    "                                    ff_dim=4,\n",
    "                                    num_transformer_blocks=4,\n",
    "                                    mlp_units=[128],\n",
    "                                    output_shape=yallc.shape[1],\n",
    "                                    mlp_dropout=0.4,\n",
    "                                    dropout=0.25)\n",
    "        else:\n",
    "            model = build_model_from_string_def(full_model_str_def,width_multiplier=yallc.shape[1])\n",
    "\n",
    "\n",
    "        display(model.summary())\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            xallc,\n",
    "            yallc,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(xallv, yallv),\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\", patience=100, mode=\"min\", restore_best_weights=True),\n",
    "                keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=50, min_lr=1e-6, verbose=1),\n",
    "            ],\n",
    "            verbose=2\n",
    "        )\n",
    "        print('Finished training %d epochs in %.2f seconds' % (history.epoch[-1], time.time() - start_time))\n",
    "        if save_results:\n",
    "            model_savepath = os.path.join(local_root_path, 'models', (model_path_prefix))\n",
    "            annutils.save_model(model_savepath, model, xscaler, yscaler)\n",
    "            print('Model saved to %s' % model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8yCcJAMh2RFH",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:04:26.793277Z",
     "start_time": "2023-01-25T23:04:26.780337Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1670888543081,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "8yCcJAMh2RFH"
   },
   "outputs": [],
   "source": [
    "# with tf.GradientTape() as tape:\n",
    "#   # Forward pass\n",
    "#   y = model(xallc.values)\n",
    "#   loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# # Calculate gradients with respect to every trainable variable\n",
    "# grad = tape.gradient(loss, model.trainable_variables)\n",
    "# print(loss)\n",
    "# # print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UhhYKM9y5RHv",
   "metadata": {
    "id": "UhhYKM9y5RHv"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SgVI7cpU5RH4",
   "metadata": {
    "id": "SgVI7cpU5RH4"
   },
   "source": [
    "## Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AldPuyGH5RH4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:04:26.808214Z",
     "start_time": "2023-01-25T23:04:26.796315Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1670888543082,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "AldPuyGH5RH4"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "eval_metrics = ['MSE', 'Bias', 'R', 'RMSD', 'NSE']\n",
    "\n",
    "def evaluate_sequences(target, pred, metrics):\n",
    "    assert len(target) == len(pred), 'Target and predicted sequence length must equal.'\n",
    "    valid_entries = target>0\n",
    "    sequence_length = np.sum(valid_entries)\n",
    "    if np.any(sequence_length == 0):\n",
    "        return {k: 0 for k in metrics}\n",
    "    target=target[valid_entries]\n",
    "    pred = pred[valid_entries]\n",
    "    SD_pred = np.sqrt( np.sum((pred-np.mean(pred)) ** 2) /(sequence_length-1))\n",
    "    SD_target = np.sqrt( np.sum((target-np.mean(target)) ** 2) /(sequence_length-1))\n",
    "\n",
    "    eval_results = defaultdict(float)\n",
    "    \n",
    "    for m in metrics:\n",
    "        if m =='MSE':\n",
    "            eval_results[m] = ((target - pred)**2).mean()\n",
    "        elif m =='Bias':\n",
    "            eval_results[m] = np.sum(pred - target)/np.sum(target) * 100\n",
    "        elif m == 'R':\n",
    "            eval_results[m] = np.sum(np.abs((pred-np.mean(pred)) * (target - np.mean(target)))) / (sequence_length * SD_pred * SD_target)\n",
    "        elif m == 'RMSD':\n",
    "            eval_results[m] = np.sqrt(np.sum( ( ( pred-np.mean(pred) ) * ( target - np.mean(target) ) ) ** 2 ) / sequence_length)\n",
    "        elif m == 'NSE':\n",
    "            eval_results[m] = 1 - np.sum( ( target - pred ) ** 2 ) / np.sum( (target - np.mean(target) ) ** 2 )\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RN9HCL9YX7w0",
   "metadata": {
    "id": "RN9HCL9YX7w0"
   },
   "source": [
    "## Compute Numerical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SbDcKUO25RH5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:04:33.310868Z",
     "start_time": "2023-01-25T23:04:26.811205Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13172,
     "status": "ok",
     "timestamp": 1670888556244,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "SbDcKUO25RH5",
    "outputId": "cc4f135b-3c30-4722-b445-c67405d311c5"
   },
   "outputs": [],
   "source": [
    "# for daily resolution\n",
    "if interval =='1D':\n",
    "    full_results={}\n",
    "    range_results=defaultdict(defaultdict)\n",
    "\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:(num_sheets)],axis=1).dropna(axis=0)\n",
    "    for group_name, stations in final_groups[group_stations].items():\n",
    "        print('Testing MTL ANN for %d stations: ' % len(output_stations))\n",
    "\n",
    "\n",
    "        model_path_prefix = \"mtl_%s_%s_%s_%s\" % (group_name, full_model_str_def, num_dataset[data_file],which_part_for_training+'_chronological')\n",
    "\n",
    "        # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "        (xallc, yallc), (xallv, yallv), _, _ = \\\n",
    "            annutils.create_training_sets([dfinps],\n",
    "                                        [dfouts[output_stations]],\n",
    "                                        calib_slice=calib_slice,\n",
    "                                        valid_slice=valid_slice,\n",
    "                                        ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                        xscaler=xscaler, yscaler=yscaler)\n",
    "        \n",
    "        annmodel = annutils.load_model(os.path.join(local_root_path,'models', (model_path_prefix)),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "\n",
    "        train_pred = annmodel.model.predict(xallc,verbose=0)\n",
    "        test_pred = annmodel.model.predict(xallv,verbose=0)\n",
    "        extra_test_pred = annmodel.model.predict(x_extrat,verbose=0)\n",
    "        \n",
    "        all_target = np.concatenate((yallc,yallv),axis=0)\n",
    "        all_pred = np.concatenate((train_pred,test_pred),axis=0)\n",
    "        extra_test_target = y_extrat.to_numpy()\n",
    "\n",
    "        for ii, location in enumerate(selected_output_variables):\n",
    "            # compute training results\n",
    "            train_results = evaluate_sequences(train_pred[:,ii], yallc.iloc[:,ii], eval_metrics)\n",
    "            train_results['R^2'] = r2_score(train_pred[:,ii], yallc.iloc[:,ii])\n",
    "            full_results['%s_train' %location] = train_results\n",
    "\n",
    "            # compute test results\n",
    "            eval_results = evaluate_sequences(test_pred[:,ii], yallv.iloc[:,ii], eval_metrics)\n",
    "            eval_results['R^2'] = r2_score(test_pred[:,ii], yallv.iloc[:,ii])\n",
    "            full_results['%s_test' %location] = eval_results\n",
    "\n",
    "            # compute results on extra test data (augmented)\n",
    "            extra_test_results = evaluate_sequences(extra_test_pred[:,ii], y_extrat.iloc[:,ii], eval_metrics)\n",
    "            extra_test_results['R^2'] = r2_score(extra_test_pred[:,ii], y_extrat.iloc[:,ii])\n",
    "            full_results['%s_extra_test' %location] = extra_test_results\n",
    "\n",
    "            # compute results at different EC ranges on the complete base dataset (defined by data_file)\n",
    "            for (lower_quantile, upper_quantile) in zip(percentiles,percentiles[1:]+[1,]):\n",
    "                lower_threshold = np.quantile(all_target[:,ii], lower_quantile)\n",
    "                upper_threshold = np.quantile(all_target[:,ii], upper_quantile)\n",
    "                eval_results = evaluate_sequences(all_target[(all_target[:,ii] > lower_threshold) & (all_target[:,ii] <= upper_threshold),ii],\n",
    "                                                    all_pred[(all_target[:,ii] > lower_threshold) & (all_target[:,ii] <= upper_threshold),ii],\n",
    "                                                    eval_metrics)\n",
    "                range_results[location][lower_quantile*100] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sT1fU_4tQovv",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:04:33.341728Z",
     "start_time": "2023-01-25T23:04:33.312858Z"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1670888556245,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "sT1fU_4tQovv"
   },
   "outputs": [],
   "source": [
    "# for hourly resolution\n",
    "if interval =='1h':\n",
    "    full_results={}\n",
    "    range_results=defaultdict(defaultdict)\n",
    "\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:(num_sheets)],axis=1).dropna(axis=0)\n",
    "    for group_name, stations in final_groups[group_stations].items():\n",
    "        # prepare dataset\n",
    "        selected_output_variables = []\n",
    "        for station in stations:\n",
    "            for output in output_stations:\n",
    "                if station in output:\n",
    "                    if station in output:\n",
    "                        selected_output_variables.append(output)\n",
    "\n",
    "        print('Testing MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "\n",
    "        model_path_prefix = \"mtl_%s_%s_%s_%s\" % (group_name, ('stage_n_' if use_hourly_stage_input else '') + full_model_str_def, num_dataset[data_file],which_part_for_training+'_chronological')\n",
    "\n",
    "        # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "        (xallc, yallc), (xallv, yallv), xscaler, yscaler = \\\n",
    "            annutils.create_training_sets([dfinps],\n",
    "                                        [dfouts[['%s-%d' % (output, hh) for hh in range(24) for output in selected_output_variables]]],\n",
    "                                        calib_slice=calib_slice,\n",
    "                                        valid_slice=valid_slice,\n",
    "                                        ndays=ndays,window_size=window_size,nwindows=nwindows,)\n",
    "        \n",
    "        if use_hourly_stage_input:\n",
    "            xallc = pd.merge(xallc,normalized_martinez_input, left_index=True, right_index=True)\n",
    "            xallv = pd.merge(xallv,normalized_martinez_input, left_index=True, right_index=True)\n",
    "\n",
    "        annmodel = annutils.load_model(os.path.join(local_root_path,'models', (model_path_prefix)),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "\n",
    "        train_pred = annmodel.model.predict(xallc,verbose=0)\n",
    "        test_pred = annmodel.model.predict(xallv,verbose=0)\n",
    "        \n",
    "        all_target = np.concatenate((yallc,yallv),axis=0)\n",
    "        all_pred = np.concatenate((train_pred,test_pred),axis=0)\n",
    "\n",
    "        for ii, location in enumerate(selected_output_variables):\n",
    "            train_results = None\n",
    "            eval_results = None\n",
    "            \n",
    "            for hh in range(24):\n",
    "                location_with_hour_name = '%s-%d' % (output, hh) \n",
    "        \n",
    "            # compute training results\n",
    "            partial_train_results = evaluate_sequences(train_pred[:,ii*24+hh],\n",
    "                                                yallc.loc[:,location_with_hour_name],\n",
    "                                                eval_metrics)\n",
    "            partial_train_results['R^2'] = r2_score(train_pred[:,ii*24+hh], yallc.loc[:,location_with_hour_name])\n",
    "\n",
    "\n",
    "            # compute training results\n",
    "            partial_eval_results = evaluate_sequences(test_pred[:,ii*24+hh], yallv.loc[:,location_with_hour_name], eval_metrics)\n",
    "            partial_eval_results['R^2'] = r2_score(test_pred[:,ii*24+hh], yallv.loc[:,location_with_hour_name])\n",
    "\n",
    "            if train_results is None:\n",
    "                train_results = partial_train_results\n",
    "                eval_results = partial_eval_results\n",
    "            else:\n",
    "                for k in train_results.keys():\n",
    "                    train_results[k] += partial_train_results[k]/24\n",
    "                    eval_results[k] += partial_eval_results[k]/24\n",
    "\n",
    "            full_results['%s_train' %location] = train_results\n",
    "            full_results['%s_test' %location] = eval_results\n",
    "\n",
    "            # compute results at different EC ranges on the complete base dataset (defined by data_file)\n",
    "            for (lower_quantile, upper_quantile) in zip(percentiles,percentiles[1:]+[1,]):\n",
    "                lower_threshold = np.quantile(all_target[:,ii*24:(ii*24+23)].reshape(-1,), lower_quantile)\n",
    "                upper_threshold = np.quantile(all_target[:,ii*24:(ii*24+23)].reshape(-1,), upper_quantile)\n",
    "                current_target = all_target[:,ii*24:(ii*24+23)]\n",
    "                current_pred = all_pred[:,ii*24:(ii*24+23)]\n",
    "                eval_results = evaluate_sequences(current_target[(current_target > lower_threshold) & (current_target <= upper_threshold)].reshape(-1,),\n",
    "                                                    current_pred[(current_target > lower_threshold) & (current_target <= upper_threshold)].reshape(-1,),\n",
    "                                                    eval_metrics)\n",
    "                range_results[location][lower_quantile*100] = eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eDzCRYN2YC8Q",
   "metadata": {
    "id": "eDzCRYN2YC8Q"
   },
   "source": [
    "## Save Numerical Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WzXN7qKx5RH6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:04:33.357658Z",
     "start_time": "2023-01-25T23:04:33.343720Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1670888556245,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "WzXN7qKx5RH6",
    "outputId": "15cc7aa4-e50e-43b3-85f5-10923bbc98c5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if save_results:\n",
    "    # create a pickle file on Google Drive and write results \n",
    "    result_path_prefix = \"mtl_%s_%s_%s_%s\" % (group_name, ('stage_n_' if use_hourly_stage_input else '') + full_model_str_def, num_dataset[data_file],which_part_for_training+'_chronological')\n",
    "    results_path = os.path.join(local_root_path,\"results/%s_full_results.pkl\" % (result_path_prefix))\n",
    "\n",
    "    f = open(results_path,\"wb\")\n",
    "    pickle.dump(full_results,f)\n",
    "    f.close()\n",
    "    print('Numerical results saved to %s' % results_path)\n",
    "\n",
    "    ### Uncomment below if you want to save results at different ranges ###\n",
    "\n",
    "    # range_results_path = os.path.join(local_root_path,\"results/%s_ranged_results.pkl\" % (result_path_prefix))\n",
    "\n",
    "    # f = open(range_results_path,\"wb\")\n",
    "    # pickle.dump(range_results,f)\n",
    "    # f.close()\n",
    "    # print('Ranged numerical results saved to %s' % range_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dyqvpvChW2-2",
   "metadata": {
    "id": "dyqvpvChW2-2"
   },
   "source": [
    "## Generate Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5D4Xgu-oYTsf",
   "metadata": {
    "id": "5D4Xgu-oYTsf"
   },
   "source": [
    "### Exceedance Probability Plots and Time Series Plots for Key Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S9D5zkmsT2Wk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:04:33.373587Z",
     "start_time": "2023-01-25T23:04:33.359649Z"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1670888556246,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "S9D5zkmsT2Wk"
   },
   "outputs": [],
   "source": [
    "def add_subplot_axes(ax,rect,facecolor='w'): # matplotlib 2.0+\n",
    "    fig = plt.gcf()\n",
    "    box = ax.get_position()\n",
    "    width = box.width\n",
    "    height = box.height\n",
    "    inax_position  = ax.transAxes.transform(rect[0:2])\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    infig_position = transFigure.transform(inax_position)    \n",
    "    x = infig_position[0]\n",
    "    y = infig_position[1]\n",
    "    width *= rect[2]\n",
    "    height *= rect[3]\n",
    "    subax = fig.add_axes([x,y,width,height],facecolor=facecolor)  # matplotlib 2.0+\n",
    "    x_labelsize = subax.get_xticklabels()[0].get_size()\n",
    "    y_labelsize = subax.get_yticklabels()[0].get_size()\n",
    "    x_labelsize *= rect[2]**0.5\n",
    "    y_labelsize *= rect[3]**0.5\n",
    "    subax.xaxis.set_tick_params(labelsize=x_labelsize)\n",
    "    subax.yaxis.set_tick_params(labelsize=y_labelsize)\n",
    "    return subax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x9fBvIbrKvvg",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:04:38.992967Z",
     "start_time": "2023-01-25T23:04:33.375579Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9717,
     "status": "ok",
     "timestamp": 1670888565941,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "x9fBvIbrKvvg",
    "outputId": "275f840e-abbb-46e5-c622-361c10171449"
   },
   "outputs": [],
   "source": [
    "if (interval.lower() == '1d') and (data_file == \"observed_data_daily.xlsx\"):\n",
    "    key_stations = ['RSAC064','CCWD_Rock','CHDMC006', 'CHSWP003','RSAC092','RSAN018']\n",
    "else:\n",
    "    key_stations = ['RSAC064','CHDMC006', 'CHSWP003','RSAC092','RSAN018','ROLD024']\n",
    "\n",
    "\n",
    "key_station_mappings = {'JerseyPoint':'Jersey Point',\n",
    "                        'Emmaton':'Emmaton',\n",
    "                        'CCFBIntake':'CCFB Intake',\n",
    "                        'CVPIntake':'CVP Intake',\n",
    "                        'SLMZU025':'SLMZU025',\n",
    "                        'ROLD059':'ROLD059'}\n",
    "\n",
    "ncols=2\n",
    "fig_combined_exceedance, ax_combined_exceedance = plt.subplots(nrows=len(key_stations)//ncols,\n",
    "                                                               ncols=ncols,\n",
    "                                                               figsize=(6*ncols,3*len(key_stations)//ncols))\n",
    "fig_combined_exceedance.tight_layout(h_pad=3.5, w_pad=2)\n",
    "ii = 0\n",
    "for group_name, stations in final_groups[group_stations].items():\n",
    "    # prepare dataset\n",
    "    selected_output_variables = []\n",
    "    for station in stations:\n",
    "        for output in output_stations:\n",
    "            if station in output:\n",
    "                selected_output_variables.append(output)\n",
    "\n",
    "    selected_output_variables = [x for x in sorted(selected_output_variables, key=lambda s: station_orders[s.split('-')[0]],reverse=False)]\n",
    "\n",
    "    print('Testing MTL ANN for %d stations: ' % len(selected_output_variables))\n",
    "\n",
    "    print([station.replace('target/','').replace('target','') for station in selected_output_variables])\n",
    "    \n",
    "    model_path_prefix = \"mtl_%s_%s_%s_%s\" % (group_name, ('stage_n_' if use_hourly_stage_input else '') + full_model_str_def, num_dataset[data_file],which_part_for_training+'_chronological')\n",
    "\n",
    "    annmodel = annutils.load_model(os.path.join(local_root_path,'models', (model_path_prefix)),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "    model = annmodel.model\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    if interval =='1h':\n",
    "        (xallc, yallc), (_, _), xscaler, yscaler = \\\n",
    "        annutils.create_training_sets([dfinps],\n",
    "                                    [dfouts[['%s-%d' % (s, hh) for hh in range(24) for s in selected_output_variables]]],\n",
    "                                    train_frac=1.0,\n",
    "                                    ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    else:\n",
    "        (xallc, yallc), (_, _), _,_ = \\\n",
    "        annutils.create_training_sets([dfinps],\n",
    "                                    [dfouts],\n",
    "                                    train_frac=1.0,\n",
    "                                    ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                    xscaler=xscaler, yscaler=yscaler)\n",
    "    if use_hourly_stage_input:\n",
    "        xallc = pd.merge(xallc,normalized_martinez_input, left_index=True, right_index=True)\n",
    "\n",
    "    dfp = yscaler.inverse_transform(pd.DataFrame(np.clip(model.predict(xallc,verbose=0),0,1),yallc.index,columns=yallc.columns))\n",
    "\n",
    "    print('Generating combined exceedance plots...')\n",
    "    for location in selected_output_variables:\n",
    "        if any([k.lower() in location.lower() for k in key_stations]):\n",
    "            simplified_station_name = location.split('-')[0].replace('_',' ').replace('-',' ')\n",
    "            simplified_station_name = key_station_mappings.get(simplified_station_name) or simplified_station_name\n",
    "            \n",
    "            if interval.lower() == '1h':\n",
    "                location_cols = ['%s-%d' % (location, hh) for hh in range(24)]\n",
    "            else:\n",
    "                location_cols = location\n",
    "            y = dfouts.loc[:,location_cols].copy()\n",
    "            y[y<0] = float('nan')\n",
    "\n",
    "\n",
    "            # Combined exceedance probability plots\n",
    "            if interval.lower() != '1d':\n",
    "                valid_rows = (dfouts.loc[:,location_cols] > 0).all(axis=1)\n",
    "                test_y_sorted = np.sort(y[valid_rows].iloc[(ndays+nwindows*window_size-1):].dropna().to_numpy().reshape(-1,1),axis=0)\n",
    "                test_pred_sorted = np.sort(dfp[valid_rows.loc[dfp.index[0]:dfp.index[-1]]].loc[:,location_cols].clip(0,dfouts.loc[:,location_cols].max(),axis=1).to_numpy().reshape(-1,1),axis=0)\n",
    "            else:\n",
    "                valid_rows = dfouts.loc[:,location] > 0\n",
    "                test_y_sorted = np.sort(y[valid_rows].iloc[(ndays+nwindows*window_size-1):].dropna().to_numpy().reshape(-1,1),axis=0)\n",
    "                test_pred_sorted = np.sort(dfp[valid_rows.loc[dfp.index[0]:dfp.index[-1]]].loc[:,location].clip(0,dfouts.loc[:,location].max()).to_numpy().reshape(-1,1),axis=0)\n",
    "\n",
    "            # calculate the proportional values of samples\n",
    "            p = 1. * np.arange(len(test_y_sorted)-1,-1,-1) / (len(test_y_sorted) - 1)\n",
    "            p_pred = 1. * np.arange(len(test_pred_sorted)-1,-1,-1) / (len(test_pred_sorted) - 1)\n",
    "\n",
    "            # plot the sorted data:\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(p, test_y_sorted,'-',color='C0')\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(p_pred, test_pred_sorted,'-',color='C3')\n",
    "            if ii == ncols:\n",
    "                ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].legend(['Target','Model Predictions'],fontsize=12,bbox_to_anchor=(1.28, 2.76))\n",
    "\n",
    "            plt.xticks(fontsize=12)\n",
    "            plt.yticks(fontsize=12)\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].ticklabel_format(axis='y',style='sci',scilimits=(0,0))\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].set_ylabel('EC ' + r\"$(\\mu S/cm)$\")\n",
    "            ylims = ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].get_ylim()\n",
    "            if ii >= len(key_stations) - ncols:\n",
    "                ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].set_xlabel('Exceedance Probability ' + r\"$(\\%)$\")\n",
    "                text_yloc = ylims[0]-(ylims[1]-ylims[0])*0.3\n",
    "            else:\n",
    "                text_yloc = ylims[0]-(ylims[1]-ylims[0])*0.18\n",
    "\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].text(0.48,\n",
    "                                                                             text_yloc,\n",
    "                                                                             '(%s)'%chr(97+ii),weight='bold')\n",
    "            ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].set_title(simplified_station_name)\n",
    "            \n",
    "            subax1 = add_subplot_axes(ax_combined_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)],[0.45,0.58,0.5,0.3]) # xloc, yloc, xwidth, ywidth\n",
    "            subax1.plot(y.iloc[(ndays+nwindows*window_size-1):],'-',color='C0',alpha=0.8)\n",
    "            if interval.lower() != '1d':\n",
    "                assert interval.lower() == '1h', 'Only supports daily or hourly estimation, but got resolution = %s' % interval\n",
    "                date_labels = pd.date_range(start=y.index[ndays+nwindows*window_size-1],\n",
    "                                            end=y.index[-1] + pd.Timedelta('23 h'), freq='h')\n",
    "                subax1.plot(date_labels,\n",
    "                            dfp.loc[:,location_cols].clip(0,dfouts.loc[:,location_cols].max(),axis=1).to_numpy().ravel(order=\"C\"),'-',color='C3',alpha=0.8)\n",
    "            else:\n",
    "                subax1.plot(dfp.loc[:,location].clip(0,dfouts.loc[:,location].max(),axis=0),'-',color='C3',alpha=0.8)\n",
    "\n",
    "            plt.yticks(fontsize=12)\n",
    "            plt.xticks(fontsize=12)\n",
    "            subax1.ticklabel_format(axis='y',style='sci',scilimits=(0,0))\n",
    "            subax1.yaxis.set_major_locator(ticker.MaxNLocator(4))\n",
    "            subax1.xaxis.set_major_locator(ticker.MaxNLocator(6))\n",
    "            subax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            ii += 1\n",
    "\n",
    "if save_results:\n",
    "    fig_savepath = os.path.join(local_root_path,\"images/%s_combined_exceedance_prob.png\"% (model_path_prefix))\n",
    "    fig_combined_exceedance.savefig(fig_savepath,bbox_inches='tight',dpi=300)\n",
    "    print('Figure saved as %s' % fig_savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z30TDstuYpO5",
   "metadata": {
    "id": "z30TDstuYpO5"
   },
   "source": [
    "### Station-wise Heatmap Plots of $r^2$, Bias, RSR and Bias at Different EC Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E-F7hULi-6TB",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T23:04:40.863366Z",
     "start_time": "2023-01-25T23:04:38.995954Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "executionInfo": {
     "elapsed": 4966,
     "status": "ok",
     "timestamp": 1670888735495,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "E-F7hULi-6TB",
    "outputId": "2df2ce78-a4be-4c7e-daab-7fac79bad3fa"
   },
   "outputs": [],
   "source": [
    "# plot percentile results for key stations\n",
    "key_stations_only=False\n",
    "# key_stations = ['RSAN018', 'RSAC092', 'CHSWP003', 'CHDMC006', 'SLMZU025', 'ROLD059', 'CHVT000','CVP_Intake', 'CCFB_Intake','Emmaton','Jersey_Point']\n",
    "# key_stations = ['CVPIntake', 'CCFBIntake','Emmaton','JerseyPoint']\n",
    "\n",
    "station_orders = {s:ii for ii, s in enumerate(observed_stations_ordered_by_median)}\n",
    "\n",
    "key_station_mappings = {'JerseyPoint':'Jersey Point',#'RSAN018',\n",
    "                        'Emmaton':'Emmaton',#'RSAC092', \n",
    "                        'CCFBIntake':'CCFB Intake',#'CHSWP003',\n",
    "                        'CVPIntake':'CVP Intake',#'CHDMC006',\n",
    "                        'SLMZU025':'SLMZU025',\n",
    "                        'ROLD059':'ROLD059'}\n",
    "# train and test results plot in different figures\n",
    "\n",
    "stations = []\n",
    "simplified_station_names = []\n",
    "if key_stations_only:\n",
    "    for s in list(range_results.keys()):\n",
    "        for k in key_stations:\n",
    "            new_s = s.replace('_','').replace('-','').replace(' ','')\n",
    "            if k.lower() in new_s.lower():\n",
    "                stations.append(s)\n",
    "                simplified_station_names.append(key_station_mappings[k])\n",
    "else:\n",
    "    for s in list(range_results.keys()):\n",
    "        new_s = s.split('-')[0].replace('_',' ').replace('-',' ')\n",
    "        stations.append(s)\n",
    "        simplified_station_names.append(new_s)\n",
    "simplified_station_names = [x for _, x in sorted(zip(stations, simplified_station_names), key=lambda pair: station_orders[pair[0]],reverse=True)]\n",
    "stations = [x for x in sorted(stations, key=lambda s: station_orders[s],reverse=True)]\n",
    "\n",
    "# legends = ['{0:>3} ~ {1:<3}%'.format(s1, s2) for _ in datasets_to_plot for (s1, s2) in zip(percentiles[1:],percentiles[:-1])]\n",
    "\n",
    "# fig_name_prefix = os.path.join(result_path,'20220225_models/images/MTL_Range_Performance')\n",
    "plot_metrics=['r^2','Bias','RSR','NSE']\n",
    "fig, ax = plt.subplots(nrows=1,ncols=len(plot_metrics),figsize=(15,8))\n",
    "# plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "for ii, metric in enumerate(plot_metrics):\n",
    "    cmap='viridis'\n",
    "    plot_metric = metric\n",
    "    if metric=='r^2':\n",
    "        plot_metric = 'R'\n",
    "    elif metric=='RSR':\n",
    "        plot_metric = 'NSE'\n",
    "        cmap='viridis_r'\n",
    "    elif metric =='Bias':\n",
    "        # cmap = Pbias_map\n",
    "        cmap = 'RdYlBu_r'\n",
    "\n",
    "    to_plot=np.zeros([len(stations),len(percentiles)])\n",
    "    for x_loc, station in enumerate(stations):\n",
    "        for y_loc, percentile_range in enumerate(percentiles):\n",
    "            if metric=='r^2':\n",
    "                to_plot[x_loc,y_loc] = range_results[station][int(percentile_range*100)][plot_metric]**2\n",
    "            elif metric=='RSR':\n",
    "                to_plot[x_loc,y_loc] = np.sqrt(1-range_results[station][int(percentile_range*100)][plot_metric])\n",
    "            else:\n",
    "                to_plot[x_loc,y_loc] = range_results[station][int(percentile_range*100)][plot_metric]\n",
    "\n",
    "    # if metric=='r^2':\n",
    "    #     current_plot = ax[ii].pcolor(to_plot,vmin=0, vmax=1)\n",
    "    # else:\n",
    "    current_plot = ax[ii].pcolor(to_plot,cmap=cmap)\n",
    "    plt.colorbar(current_plot,ax=ax[ii],aspect=30,fraction=0.1, pad=0.04)\n",
    "    if metric =='Bias':\n",
    "        ax[ii].set_title('Percent Bias (%)')\n",
    "    else:\n",
    "        ax[ii].set_title(r'${}$'.format(metric))\n",
    "    if ii == 0:\n",
    "        ax[ii].set_ylabel('Station')\n",
    "        ax[ii].set_yticks(np.arange(len(stations))+0.5)\n",
    "        ax[ii].set_yticklabels(simplified_station_names)\n",
    "        # ax.tick_params(axis='y', labelrotation = 45, size=0)        \n",
    "    else:\n",
    "        ax[ii].set_yticks([])\n",
    "\n",
    "    ax[ii].set_xlabel('Range')\n",
    "    ax[ii].set_xticks(np.arange(len(percentiles))+0.5)\n",
    "    ax[ii].xaxis.set_tick_params(size=0)\n",
    "    ax[ii].set_xticklabels(['0-%d%%'%(percentiles[1]*100),'%d-%d%%'%(percentiles[1]*100,percentiles[2]*100),'%d-100%%'%(percentiles[2]*100)],fontsize=10)\n",
    "\n",
    "    ax[ii].text(1.4, ax[ii].get_ylim()[0]-3,\n",
    "                '(%s)'%chr(97+ii),weight='bold')\n",
    "if save_results:\n",
    "    fig_savepath = os.path.join(local_root_path,\"images/%s_Range_Performance.png\"% (model_path_prefix))\n",
    "    plt.savefig(fig_savepath,bbox_inches='tight',dpi=300)\n",
    "    print('Figure saved as %s' % fig_savepath)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "13K3UqvGrs-4QH6zWSerNEQxx5rqpesIX",
     "timestamp": 1649114480202
    },
    {
     "file_id": "1xVpgVH4RbKZ3-vjcRePmqVU_4kjv8zZs",
     "timestamp": 1645650753647
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:Salinity_DWR]",
   "language": "python",
   "name": "conda-env-Salinity_DWR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "3d3652577a7c35b42d007c0caebd8ec483b346ab09486faf41fb90f43ee21a27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
