{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33e7463",
   "metadata": {
    "id": "e33e7463"
   },
   "source": [
    "# Train ANN with Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc9d60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:20:21.503236Z",
     "start_time": "2023-01-20T00:20:21.496267Z"
    }
   },
   "outputs": [],
   "source": [
    "local_root_path= \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3z3YjNaVvZ0",
   "metadata": {
    "id": "w3z3YjNaVvZ0"
   },
   "source": [
    "## Define Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RZffFrVYeP6C",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:20:21.533615Z",
     "start_time": "2023-01-20T00:20:21.506223Z"
    },
    "id": "RZffFrVYeP6C"
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "############## Hyper-param definitions #############\n",
    "####################################################\n",
    "\n",
    "''' \n",
    "Select NN architecture from:\n",
    "MLP, LSTM, GRU, ResNet, Res-LSTM, Res-GRU, Transformer\n",
    "'''\n",
    "model_type ='Res-LSTM'\n",
    "\n",
    "''' \n",
    "Dropout ratio at (before) the input layer\n",
    "'''\n",
    "input_dropout = 0.\n",
    "\n",
    "''' \n",
    "Dropout ratio at intermediate layers\n",
    "'''\n",
    "intermediate_dropout = 0\n",
    "\n",
    "''' \n",
    "Numbers of neurons in the main branch\n",
    " - Provide two numbers for MLP, ResNet, Res-LSTM, Res-GRU, \n",
    " - Provide one number for LSTM, GRU.\n",
    " '''\n",
    "num_neurons_multiplier=[8, 2]\n",
    "\n",
    "'''\n",
    "Number of training epochs (Note: training will stop when reaching this number \n",
    "or test loss doesn't decrease for 50 epochs)\n",
    "'''\n",
    "epochs = 50\n",
    "\n",
    "'''Give a name to current train/test combination'''\n",
    "scenario_name = 'Tune_RSAC_RSAN'\n",
    "\n",
    "'''\n",
    "Lists of training and validation sets\n",
    "'''\n",
    "train_data = [\"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus20pct.xlsx\",\n",
    "              ]\n",
    "val_data = \"dsm2_ann_inputs_base.xlsx\"\n",
    "\n",
    "'''\n",
    "Dictionary of test sets\n",
    "- keys: test scenario name\n",
    "- values: names of excel files\n",
    "'''\n",
    "test_data = {'dcc0':\"dsm2_ann_inputs_dcc0.xlsx\",\n",
    "             'smscg1':\"dsm2_ann_inputs_smscg1.xlsx\",\n",
    "             'dcc1':\"dsm2_ann_inputs_dcc1.xlsx\",\n",
    "             'smscg0':\"dsm2_ann_inputs_smscg0.xlsx\"}\n",
    "\n",
    "'''\n",
    "Dictionary of extra test set(s)\n",
    "- keys: test scenario name\n",
    "- values: names of excel files\n",
    "'''\n",
    "extra_data = {'observed': \"observed_data_daily.xlsx\"}\n",
    "\n",
    "'''\n",
    "Ratio to split the extra dataset for test\n",
    "'''\n",
    "extra_data_test_ratio = 0.3\n",
    "'''\n",
    "Which part of extra dataset used for for test\n",
    "Available options: {'first', 'last', 'middle', 'manual'}\n",
    "'''\n",
    "which_part_for_test = 'last'\n",
    "\n",
    "'''Whether to print detailed metrics (std, MSE, ...) for key stations'''\n",
    "print_detailed_key_station_metrics = False\n",
    "\n",
    "''' \n",
    "Whether to (True) train models from scratch or (False) evaluate pre-trained models\n",
    "'''\n",
    "train_models = True\n",
    "\n",
    "'''Whether to save evaluation results (metric values, figures) to Google Drive'''\n",
    "save_results = True\n",
    "\n",
    "############################################\n",
    "############  Siyu's Notes:  ###############\n",
    "############################################\n",
    "'''\n",
    "In the pickle file, numerical results will be saved with the following suffixes:\n",
    "-- Training results (listed in train_data): '_train'\n",
    "-- Test results (listed in test_data): '_test'\n",
    "-- Validation results (defined by val_data): '_val'\n",
    "-- Results on chronologically-split observed data (defined by extra_data, sliced by valid_slice): '_observed'\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################\n",
    "########## End of hyper-param definitions ##########\n",
    "####################################################\n",
    "initial_lr=0.001\n",
    "\n",
    "'''\n",
    "Define the model architecture: (input layer will be build automatically)\n",
    "\n",
    "Supported abbreviation-layer pairs (detailed definitions can be found in \"Layer builders\" section)\n",
    "- 'lstm': keras.layers.LSTM\n",
    "- 'res': resnet block (basic_1d)\n",
    "- 'c1d': keras.layers.Conv1D\n",
    "- 'td': keras.layers.TimeDistributed\n",
    "- 'dr': keras.layers.Dropout\n",
    "- 'f': keras.layers.Flatten\n",
    "- 'g': keras.layers.GRU\n",
    "- 'd': keras.layers.Dense \n",
    "- 'o': keras.layers.Dense\n",
    "\n",
    "Usage of resnet blocks: res(num_of_filters)x(kernel_size)x(stride)x(stages)\n",
    "stages: # of resnet units in the block\n",
    "example: model_str_def = 'res10x3x1x1_f_d8_d2_o1'\n",
    "\n",
    "Usage of 1D conv layers: c1d(num_of_filters)x(kernel_size)x(stride)\n",
    "example: model_str_def = 'c1d10x3x1_c1d10x3x1_f_d8_d2_o1'\n",
    "\n",
    "'''\n",
    "# Note: numbers following 'd' or 'o' will be multiplied by [size of output] before being used as numbers of neurons in dense layer\n",
    "model_type = model_type.lower()\n",
    "\n",
    "if model_type =='mlp':\n",
    "    ## 1. MLP Network\n",
    "    model_str_def = '%sd%d_%sd%d_o1' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                        num_neurons_multiplier[0],\n",
    "                                        ('dr%.2f_' % intermediate_dropout if intermediate_dropout > 0 else ''),\n",
    "                                        num_neurons_multiplier[1])\n",
    "\n",
    "elif model_type =='lstm':\n",
    "    # 2. LSTM Network\n",
    "    model_str_def = '%slstm%d_%sf_o1' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                         num_neurons_multiplier[0],\n",
    "                                         ('dr%.2f_' % intermediate_dropout if intermediate_dropout > 0 else ''),)\n",
    "\n",
    "elif model_type =='gru':\n",
    "    # 3. GRU Network\n",
    "    model_str_def = '%sg%d_%sf_o1' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                    num_neurons_multiplier[0],\n",
    "                                    ('dr%.2f_' % intermediate_dropout if intermediate_dropout > 0 else ''),)\n",
    "\n",
    "elif model_type =='resnet':\n",
    "    # 4. ResNet\n",
    "    if intermediate_dropout > 0:\n",
    "        num_neurons_multiplier.insert(1,'dr%.2f' % intermediate_dropout)\n",
    "    model_str_def = '%sresnet%s' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                    '_' + '_'.join([str(ii) for ii in num_neurons_multiplier]))\n",
    "    num_res_blocks=1\n",
    "\n",
    "elif model_type =='res-lstm':\n",
    "    # 5. Res-LSTM\n",
    "    if intermediate_dropout > 0:\n",
    "        num_neurons_multiplier.insert(1,'dr%.2f' % intermediate_dropout)\n",
    "    model_str_def = '%sresidual_lstm%s' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                        '_' + '_'.join([str(ii) for ii in num_neurons_multiplier]))\n",
    "\n",
    "elif model_type =='res-gru':\n",
    "    # 6. Res-GRU\n",
    "    if intermediate_dropout > 0:\n",
    "        num_neurons_multiplier.insert(1,'dr%.2f' % intermediate_dropout)\n",
    "    model_str_def = '%sresidual_gru%s' % (('dr%.2f_' % input_dropout if input_dropout > 0 else ''),\n",
    "                                        '_' + '_'.join([str(ii) for ii in num_neurons_multiplier]))\n",
    "\n",
    "elif model_type =='transformer':\n",
    "    # 7. Transformer\n",
    "    model_str_def = '%stransformer' % ('dr%.2f_' % input_dropout if input_dropout > 0 else '')\n",
    "else:\n",
    "    raise \"Model %s is not supported\" % model_type\n",
    "\n",
    "'''\n",
    "Define parameters for input pre-processing\n",
    "- ndays: number of daily values in inputs\n",
    "- window_size: length of averaging windows\n",
    "- nwindows: number of moving averages\n",
    "'''\n",
    "if model_type =='mlp':\n",
    "    # apply pre-defined average windowing:\n",
    "    ndays=8\n",
    "    window_size=11\n",
    "    nwindows=10\n",
    "else:\n",
    "    # directly use daily measurements as inputs\n",
    "    ndays=118\n",
    "    window_size=0\n",
    "    nwindows=0\n",
    "\n",
    "\n",
    "# manually picked years for \"training\" in observed dataset\n",
    "# if which_part_for_test == 'manual', years not listed below will be used \n",
    "# for testing and results will be saved with '_observed' suffix in the \n",
    "# pickle file\n",
    "picked_training_years = [('1990-10-1','1991-9-30'),\n",
    "                         ('1992-10-1','1995-9-30'),\n",
    "                         ('1996-10-1','1998-9-30'),\n",
    "                         ('1999-10-1','2003-9-30'),\n",
    "                         ('2004-10-1','2006-9-30'),\n",
    "                         ('2007-10-1','2010-9-30'),\n",
    "                         ('2011-10-1','2013-9-30'),\n",
    "                         ('2014-10-1','2016-9-30'),\n",
    "                         ('2017-10-1','2019-9-30'),]\n",
    "\n",
    "\n",
    "# percentile thresholds for ranged results\n",
    "percentiles = [0,0.75,0.95]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97mRtvChV0Si",
   "metadata": {
    "id": "97mRtvChV0Si"
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l9U0JyrTcWj6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:20:21.549545Z",
     "start_time": "2023-01-20T00:20:21.535606Z"
    },
    "id": "l9U0JyrTcWj6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras import layers\n",
    "#import keras\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y4V3lgV4VgZj",
   "metadata": {
    "id": "Y4V3lgV4VgZj"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2WwdXA-qNPU",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:22:45.733554Z",
     "start_time": "2023-01-20T00:20:21.552531Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3408,
     "status": "ok",
     "timestamp": 1670875525619,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "v2WwdXA-qNPU",
    "outputId": "35549dae-fc37-467e-9fdb-380dcc8f9b16"
   },
   "outputs": [],
   "source": [
    "sys.path.append(local_root_path)\n",
    "import annutils\n",
    "\n",
    "observed_stations_ordered_by_median = ['RSMKL008', 'RSAN032', 'RSAN037', 'RSAC092', 'SLTRM004', 'ROLD024',\n",
    "                                           'CHVCT000', 'RSAN018', 'CHSWP003', 'CHDMC006', 'SLDUT007', 'RSAN072',\n",
    "                                           'OLD_MID', 'RSAN058', 'ROLD059', 'RSAN007', 'RSAC081', 'SLMZU025',\n",
    "                                           'RSAC075', 'SLMZU011', 'SLSUS012', 'SLCBN002', 'RSAC064']\n",
    "num_sheets = 9\n",
    "\n",
    "output_stations = None\n",
    "\n",
    "xscaler = None\n",
    "yscaler = None\n",
    "\n",
    "for data_file in train_data + [val_data,] + list(test_data.values()) + list(extra_data.values()):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "\n",
    "    dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "    dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    if output_stations is None:\n",
    "        # read station names\n",
    "        output_stations = list(dfouts.columns)\n",
    "        name_mapping = {}\n",
    "        for s in output_stations:\n",
    "            for ss in observed_stations_ordered_by_median:\n",
    "                if ss in s:\n",
    "                    name_mapping[s] = ss\n",
    "        output_stations = list(name_mapping.values())\n",
    "    \n",
    "    dfouts = dfouts.rename(columns=name_mapping)[output_stations]\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    if xscaler is None:\n",
    "        xscaler,yscaler = annutils.create_xyscaler([dfinps],[dfouts])\n",
    "    else:\n",
    "        temp_xscaler,temp_yscaler = annutils.create_xyscaler([dfinps],[dfouts])\n",
    "        if np.any(np.isnan(temp_xscaler.max_val)) or np.any(np.isnan(temp_yscaler.max_val)):\n",
    "            break\n",
    "        xscaler.update(temp_xscaler)\n",
    "        yscaler.update(temp_yscaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5g5YHRMT700K",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:25:21.803180Z",
     "start_time": "2023-01-20T00:22:45.735545Z"
    },
    "id": "5g5YHRMT700K"
   },
   "outputs": [],
   "source": [
    "observed_stations_ordered_by_median = ['RSMKL008', 'RSAN032', 'RSAN037', 'RSAC092', 'SLTRM004', 'ROLD024',\n",
    "                                           'CHVCT000', 'RSAN018', 'CHSWP003', 'CHDMC006', 'SLDUT007', 'RSAN072',\n",
    "                                           'OLD_MID', 'RSAN058', 'ROLD059', 'RSAN007', 'RSAC081', 'SLMZU025',\n",
    "                                           'RSAC075', 'SLMZU011', 'SLSUS012', 'SLCBN002', 'RSAC064']\n",
    "num_sheets = 9\n",
    "\n",
    "output_stations = None\n",
    "\n",
    "\n",
    "xscaler = None\n",
    "yscaler = None\n",
    "\n",
    "for data_file in train_data + [val_data,] + list(test_data.values()) + list(extra_data.values()):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "\n",
    "    dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "    dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    if output_stations is None:\n",
    "        # read station names\n",
    "        output_stations = list(dfouts.columns)\n",
    "        name_mapping = {}\n",
    "        for s in output_stations:\n",
    "            for ss in observed_stations_ordered_by_median:\n",
    "                if ss in s:\n",
    "                    name_mapping[s] = ss\n",
    "        output_stations = list(name_mapping.values())\n",
    "    \n",
    "    dfouts = dfouts.rename(columns=name_mapping)[output_stations]\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    if xscaler is None:\n",
    "        xscaler,yscaler = annutils.create_xyscaler([dfinps],[dfouts])\n",
    "    else:\n",
    "        temp_xscaler,temp_yscaler = annutils.create_xyscaler([dfinps],[dfouts])\n",
    "        if np.any(np.isnan(temp_xscaler.max_val)) or np.any(np.isnan(temp_yscaler.max_val)):\n",
    "            break\n",
    "        xscaler.update(temp_xscaler)\n",
    "        yscaler.update(temp_yscaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w_T9ydQBuRno",
   "metadata": {
    "id": "w_T9ydQBuRno"
   },
   "source": [
    "### Read Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a1ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:26:54.924841Z",
     "start_time": "2023-01-20T00:25:21.805173Z"
    }
   },
   "outputs": [],
   "source": [
    "xallc = None\n",
    "yallc = None\n",
    "\n",
    "for data_file in train_data:\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "\n",
    "    dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "    dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = dfouts.rename(columns=name_mapping)[output_stations]\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    if xallc is None:\n",
    "        (xallc, yallc), (_, _), _, _ = \\\n",
    "            annutils.create_training_sets([dfinps],\n",
    "                                          [dfouts],\n",
    "                                          train_frac=1,\n",
    "                                          ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                          xscaler=xscaler,yscaler=yscaler)\n",
    "    else:\n",
    "        (xc, yc), (_, _), _, _ = \\\n",
    "            annutils.create_training_sets([dfinps],\n",
    "                                          [dfouts],\n",
    "                                          train_frac=1,\n",
    "                                          ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                          xscaler=xscaler,yscaler=yscaler)\n",
    "        xallc = pd.concat([xallc,xc],axis=0)\n",
    "        yallc = pd.concat([yallc,yc],axis=0)\n",
    "        del xc, yc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DNKAUqrBuljr",
   "metadata": {
    "id": "DNKAUqrBuljr"
   },
   "source": [
    "### Read Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JJlpQ1JVuqco",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:27:06.719702Z",
     "start_time": "2023-01-20T00:26:54.926832Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12586,
     "status": "ok",
     "timestamp": 1670875904021,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "JJlpQ1JVuqco",
    "outputId": "f2edbd48-2220-4bd1-d9b1-f8b484b07140"
   },
   "outputs": [],
   "source": [
    "xallv = None\n",
    "yallv = None\n",
    "\n",
    "data_path = os.path.join(local_root_path,val_data)\n",
    "\n",
    "dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "dfouts = dfouts.rename(columns=name_mapping)[output_stations]\n",
    "# create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "(xallv, yallv), (_, _), _, _ = \\\n",
    "    annutils.create_training_sets([dfinps],\n",
    "                                  [dfouts],\n",
    "                                  train_frac=1,\n",
    "                                  ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                  xscaler=xscaler,yscaler=yscaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHDUNWTTuox7",
   "metadata": {
    "id": "pHDUNWTTuox7"
   },
   "source": [
    "### Read Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z7CMWGNIuH3h",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:27:54.974099Z",
     "start_time": "2023-01-20T00:27:06.721693Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51225,
     "status": "ok",
     "timestamp": 1670875955241,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "z7CMWGNIuH3h",
    "outputId": "4fc15df6-6c7b-45f8-bc5d-d7aa3d428b96"
   },
   "outputs": [],
   "source": [
    "xallt = None\n",
    "yallt = None\n",
    "for data_file in test_data.values():\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "\n",
    "    dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "    dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = dfouts.rename(columns=name_mapping)[output_stations]\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    if xallt is None:\n",
    "        (xallt, yallt), (_, _), _, _ = \\\n",
    "            annutils.create_training_sets([dfinps],\n",
    "                                          [dfouts],\n",
    "                                          train_frac=1,\n",
    "                                          ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                          xscaler=xscaler,yscaler=yscaler)\n",
    "    else:\n",
    "        (xt, yt), (_, _), _, _ = \\\n",
    "            annutils.create_training_sets([dfinps],\n",
    "                                          [dfouts],\n",
    "                                          train_frac=1,\n",
    "                                          ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                          xscaler=xscaler,yscaler=yscaler)\n",
    "        xallt = pd.concat([xallt,xt],axis=0)\n",
    "        yallt = pd.concat([yallt,yt],axis=0)\n",
    "        # del xt, yt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M1oG-SZmugef",
   "metadata": {
    "id": "M1oG-SZmugef"
   },
   "source": [
    "### Read Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_k4X3Wy629RG",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:28:04.111634Z",
     "start_time": "2023-01-20T00:27:54.976090Z"
    },
    "id": "_k4X3Wy629RG"
   },
   "outputs": [],
   "source": [
    "######### Read extra observed dataset ###############\n",
    "for data_file in extra_data.values():\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "\n",
    "    dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "    dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "    dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)][output_stations]\n",
    "\n",
    "    start_year = max(dfinps.index[0].year, dfouts.index[0].year)\n",
    "    end_year = min(dfinps.index[-1].year, dfouts.index[-1].year)\n",
    "    \n",
    "    if which_part_for_test =='last':\n",
    "        calib_slice = slice(str(start_year), str(int(start_year+(1-extra_data_test_ratio)*(end_year-start_year))))\n",
    "        valid_slice = slice(str(int(start_year+(1-extra_data_test_ratio)*(end_year-start_year))+1), str(end_year))\n",
    "    elif which_part_for_test =='first':\n",
    "        calib_slice = slice(str(int(start_year+(1-extra_data_test_ratio)*(end_year-start_year))+1), str(end_year))\n",
    "        valid_slice = slice(str(start_year), str(int(start_year+(1-extra_data_test_ratio)*(end_year-start_year))))\n",
    "    elif which_part_for_test =='middle':\n",
    "        calib_slice = [slice(str(start_year),\n",
    "                             str(int(start_year+(1-extra_data_test_ratio)/2*(end_year-start_year)))),\n",
    "                       slice(str(int(start_year+(1+extra_data_test_ratio)/2*(end_year-start_year)+1)),\n",
    "                             str(end_year))]\n",
    "        valid_slice = slice(str(int(start_year+(1-extra_data_test_ratio)/2*(end_year-start_year))+1),\n",
    "                            str(int(start_year+(1+extra_data_test_ratio)/2*(end_year-start_year))))\n",
    "    elif which_part_for_test =='manual' and picked_training_years is not None:\n",
    "        calib_slice = [slice(str(start_year), str(end_year)) for (start_year,end_year) in picked_training_years]\n",
    "        valid_slice = [slice(start_year, end_year) for ((_,start_year),(end_year,_)) in zip([(None,'1989-10-1'),]+picked_training_years,picked_training_years+[('2020-9-30',None),])]\n",
    "    else:\n",
    "        raise Exception('Unknown data splitting method')\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (x_extra_train, y_extra_train), (x_extra_test, y_extra_test), _, _ = \\\n",
    "            annutils.create_training_sets([dfinps],\n",
    "                                        [dfouts],\n",
    "                                        calib_slice=calib_slice,\n",
    "                                        valid_slice=valid_slice,\n",
    "                                        ndays=ndays,window_size=window_size,nwindows=nwindows,\n",
    "                                        xscaler=xscaler,yscaler=yscaler)\n",
    "    xallc = pd.concat([xallc,x_extra_train],axis=0)\n",
    "    yallc = pd.concat([yallc,y_extra_train],axis=0)\n",
    "    xallt = pd.concat([xallt,x_extra_test],axis=0)\n",
    "    yallt = pd.concat([yallt,y_extra_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqRixm2Gw74-",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:28:04.126569Z",
     "start_time": "2023-01-20T00:28:04.115617Z"
    },
    "id": "kqRixm2Gw74-"
   },
   "outputs": [],
   "source": [
    "del df_inpout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sj6XTEbbD6OS",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:28:04.141504Z",
     "start_time": "2023-01-20T00:28:04.128559Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1670875964484,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "sj6XTEbbD6OS",
    "outputId": "9308b6d8-4492-4390-b954-8a7d5eb05304"
   },
   "outputs": [],
   "source": [
    "print('Training set input shape: ', xallc.shape)\n",
    "print('Validation set input shape: ', xallv.shape)\n",
    "print('Test set input shape: ', xallt.shape)\n",
    "\n",
    "print('Output shape: ', yallc.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a39bc",
   "metadata": {
    "id": "c84a39bc"
   },
   "source": [
    "## Tensorboard Setup\n",
    "A log directory to keep the training logs\n",
    "\n",
    "Tensorboard starts a separate process and is best started from the command line. Open a command window and activate this environment (i.e. keras) and goto the current directory. Then type in\n",
    "```\n",
    "tensorboard --logdir=./tf_training_logs/ --port=6006\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd54a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:28:04.157434Z",
     "start_time": "2023-01-20T00:28:04.143493Z"
    },
    "id": "82fd54a3"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./tf_training_logs/ --port=6006\n",
    "root_logdir = os.path.join(os.curdir, \"tf_training_logs\")\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(root_logdir)## Tensorflow Board Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209223b",
   "metadata": {
    "id": "a209223b"
   },
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fLtxSBKTuF1b",
   "metadata": {
    "id": "fLtxSBKTuF1b"
   },
   "source": [
    "### ResNet Block Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VmzBaCvGuFNk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:28:04.188298Z",
     "start_time": "2023-01-20T00:28:04.159423Z"
    },
    "id": "VmzBaCvGuFNk"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"kernel_initializer\": \"he_normal\"\n",
    "}\n",
    "\n",
    "def basic_1d(\n",
    "    filters,\n",
    "    stage=0,\n",
    "    block=0,\n",
    "    kernel_size=3,\n",
    "    numerical_name=False,\n",
    "    stride=None,\n",
    "    force_identity_shortcut=False\n",
    "):\n",
    "    \"\"\"\n",
    "    A one-dimensional basic block.\n",
    "    :param filters: the output’s feature space\n",
    "    :param stage: int representing the stage of this block (starting from 0)\n",
    "    :param block: int representing this block (starting from 0)\n",
    "    :param kernel_size: size of the kernel\n",
    "    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n",
    "    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        if block != 0 or stage == 0:\n",
    "            stride = 1\n",
    "        else:\n",
    "            stride = 2\n",
    "\n",
    "\n",
    "    if block > 0 and numerical_name:\n",
    "        block_char = \"b{}\".format(block)\n",
    "    else:\n",
    "        block_char = chr(ord('a') + block)\n",
    "\n",
    "    stage_char = str(stage + 2)\n",
    "\n",
    "    def f(x):\n",
    "        y = keras.layers.ZeroPadding1D(padding=1,name=\"padding{}{}_branch2a\".format(stage_char, block_char))(x)\n",
    "        y = keras.layers.Conv1D(filters,kernel_size,strides=stride,use_bias=False,\n",
    "                                name=\"res{}{}_branch2a\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "        y = keras.layers.Activation(\"relu\", name=\"res{}{}_branch2a_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        y = keras.layers.ZeroPadding1D(padding=1,name=\"padding{}{}_branch2b\".format(stage_char, block_char))(y)\n",
    "        y = keras.layers.Conv1D(filters,kernel_size,use_bias=False,\n",
    "                                name=\"res{}{}_branch2b\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "\n",
    "        if block != 0 or force_identity_shortcut:\n",
    "            shortcut = x\n",
    "        else:\n",
    "            shortcut = keras.layers.Conv1D(filters,1,strides=stride,use_bias=False,\n",
    "                                           name=\"res{}{}_branch1\".format(stage_char, block_char),\n",
    "                                           **parameters)(x)\n",
    "            shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = keras.layers.Add(name=\"res{}{}\".format(stage_char, block_char))([y, shortcut])\n",
    "        \n",
    "        y = keras.layers.Activation(\"relu\",name=\"res{}{}_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def bottleneck_1d(\n",
    "    filters,\n",
    "    stage=0,\n",
    "    block=0,\n",
    "    kernel_size=3,\n",
    "    numerical_name=False,\n",
    "    stride=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    A one-dimensional bottleneck block.\n",
    "    :param filters: the output’s feature space\n",
    "    :param stage: int representing the stage of this block (starting from 0)\n",
    "    :param block: int representing this block (starting from 0)\n",
    "    :param kernel_size: size of the kernel\n",
    "    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n",
    "    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = 1 if block != 0 or stage == 0 else 2\n",
    "\n",
    "    # axis = -1 if keras.backend.image_data_format() == \"channels_last\" else 1\n",
    "\n",
    "\n",
    "    if block > 0 and numerical_name:\n",
    "        block_char = \"b{}\".format(block)\n",
    "    else:\n",
    "        block_char = chr(ord('a') + block)\n",
    "\n",
    "    stage_char = str(stage + 2)\n",
    "\n",
    "    def f(x):\n",
    "        y = keras.layers.Conv1D(filters,1,strides=stride,use_bias=False,\n",
    "                                name=\"res{}{}_branch2a\".format(stage_char, block_char),\n",
    "                                **parameters)(x)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "        y = keras.layers.Activation(\"relu\",name=\"res{}{}_branch2a_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        y = keras.layers.ZeroPadding1D(padding=1,name=\"padding{}{}_branch2b\".format(stage_char, block_char))(y)\n",
    "        y = keras.layers.Conv1D(filters,kernel_size,use_bias=False,\n",
    "                                name=\"res{}{}_branch2b\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "        y = keras.layers.Activation(\"relu\",name=\"res{}{}_branch2b_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        y = keras.layers.Conv1D(filters * 4, 1, use_bias=False,\n",
    "                                name=\"res{}{}_branch2c\".format(stage_char, block_char),\n",
    "                                **parameters)(y)\n",
    "        y = keras.layers.BatchNormalization()(y)\n",
    "\n",
    "        if block == 0:\n",
    "            shortcut = keras.layers.Conv1D(filters * 4, 1, strides=stride, use_bias=False,\n",
    "                                           name=\"res{}{}_branch1\".format(stage_char, block_char),\n",
    "                                           **parameters)(x)\n",
    "            shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "        else:\n",
    "            shortcut = x\n",
    "\n",
    "        y = keras.layers.Add(name=\"res{}{}\".format(stage_char, block_char))([y, shortcut])\n",
    "        y = keras.layers.Activation(\"relu\",name=\"res{}{}_relu\".format(stage_char, block_char))(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yKMql94RfaA4",
   "metadata": {
    "id": "yKMql94RfaA4"
   },
   "source": [
    "### Layer builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sOYi1S2zfaLF",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:28:04.204229Z",
     "start_time": "2023-01-20T00:28:04.190287Z"
    },
    "id": "sOYi1S2zfaLF"
   },
   "outputs": [],
   "source": [
    "\"\"\"# Custom loss function\"\"\"\n",
    "def mse_loss_masked(y_true, y_pred):\n",
    "    squared_diff = tf.reduce_sum(tf.math.squared_difference(y_pred[y_true>0],y_true[y_true>0]))\n",
    "    return squared_diff/(tf.reduce_sum(tf.cast(y_true>0, tf.float32))+0.01)\n",
    "\n",
    "# Define Sequential model\n",
    "NFEATURES = dfinps.shape[1] # * (ndays + nwindows) \n",
    "def build_layer_from_string_def(s='i120',width_multiplier=1,\n",
    "                                block=0,\n",
    "                                force_identity_shortcut=False,\n",
    "                                return_sequences_rnn=True):\n",
    "    if s[0:4] == 'lstm':\n",
    "        return layers.LSTM(units = int(s[4:])*width_multiplier, return_sequences=return_sequences_rnn, activation='sigmoid')\n",
    "    elif s[0:3] == 'res':\n",
    "        fields = s[3:].split('x')\n",
    "        return basic_1d(filters=int(fields[0]),\n",
    "                        stage=int(fields[3]),\n",
    "                        block=block,\n",
    "                        kernel_size=int(fields[1]),\n",
    "                        stride=int(fields[2]),\n",
    "                        force_identity_shortcut=force_identity_shortcut)\n",
    "    elif s[0:3] == 'c1d':\n",
    "        fields = s[3:].split('x')\n",
    "        return keras.layers.Conv1D(filters=int(fields[0]), kernel_size=int(fields[1]), strides=int(fields[2]),\n",
    "                                   padding='causal', activation='linear')\n",
    "    elif s[0:2] == 'td':\n",
    "        return keras.layers.TimeDistributed(keras.layers.Dense(int(s[2:]), activation='elu'))\n",
    "    elif s[0:2] == 'dr':\n",
    "        return keras.layers.Dropout(float(s[2:]))\n",
    "    # elif s[0] == 'i':\n",
    "    #     return keras.layers.InputLayer(input_shape=[int(s[1:]), NFEATURES])\n",
    "    elif s[0] == 'f':\n",
    "        return keras.layers.Flatten()\n",
    "    elif s[0] == 'g':\n",
    "        return keras.layers.GRU(int(s[1:])*width_multiplier, return_sequences=True, activation='relu')\n",
    "    elif s[0] == 'd':\n",
    "        return keras.layers.Dense(int(s[1:])*width_multiplier, activation='elu')\n",
    "    elif s[0] == 'o':\n",
    "        return keras.layers.Dense(int(s[1:])*width_multiplier, activation='linear')\n",
    "    else:\n",
    "        raise Exception('Unknown layer def: %s' % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TmzzoGdReGnM",
   "metadata": {
    "id": "TmzzoGdReGnM"
   },
   "source": [
    "### Model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803eca3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:28:04.250649Z",
     "start_time": "2023-01-20T00:28:04.206217Z"
    },
    "id": "803eca3b"
   },
   "outputs": [],
   "source": [
    "def build_model_from_string_def(strdef='i120_f_d4_d2_d1',width_multiplier=1):\n",
    "    layer_strings = strdef.split('_')\n",
    "    inputs = keras.layers.Input(shape=[int(layer_strings[0][1:]) * NFEATURES])\n",
    "    x = None\n",
    "    prev_conv_output_num_of_channels = None\n",
    "    return_sequences_rnn = None\n",
    "    for block,f in enumerate(layer_strings[1:-1]):\n",
    "        if x is None:\n",
    "            if ('lstm' in strdef) or ('g' in strdef):         \n",
    "                # these layers require 2D inputs and permutation\n",
    "                x = layers.Reshape((ndays+nwindows,NFEATURES))(inputs)\n",
    "                prev_conv_output_num_of_channels = NFEATURES\n",
    "                x = layers.Permute((2,1))(x)\n",
    "                return_sequences_rnn = layer_strings[block+2].startswith(('lstm','g','res','c1d'))\n",
    "            elif ('res' in strdef) or ('cld' in strdef):\n",
    "                # these layers require 2D inputs\n",
    "                x = layers.Reshape((ndays+nwindows,NFEATURES))(inputs)\n",
    "                prev_conv_output_num_of_channels = NFEATURES\n",
    "            else:\n",
    "                x = inputs\n",
    "\n",
    "\n",
    "        x = build_layer_from_string_def(f,width_multiplier,block,\n",
    "                                        force_identity_shortcut=(f.startswith('res') and prev_conv_output_num_of_channels==int(f[3:].split('x')[0])),\n",
    "                                        return_sequences_rnn=return_sequences_rnn)(x)\n",
    "        if f.startswith('lstm'):         \n",
    "            prev_conv_output_num_of_channels=int(f[4:])\n",
    "        elif f.startswith('res') or f.startswith('c1d'):\n",
    "            prev_conv_output_num_of_channels=int(f[3:].split('x')[0])\n",
    "\n",
    "\n",
    "    outputs = keras.layers.Dense(int(layer_strings[-1][1:])*width_multiplier, activation='linear')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_resnet_model(nhidden1=8, nhidden2=2, output_shape=1, act_func='sigmoid',\n",
    "                       filters=num_sheets-1, kernel_size=3, stride=1,\n",
    "                       num_res_blocks=1,input_dropout=0.):\n",
    "    inputs = layers.Input(shape=NFEATURES* (ndays + nwindows))\n",
    "    x = layers.Reshape((ndays+nwindows,NFEATURES))(inputs)\n",
    "    x = layers.Dropout(input_dropout)(x)\n",
    "    for ii in range(num_res_blocks - 1):\n",
    "        # TODO: think about conv filter numbers and kernel sizes\n",
    "        intermediate_features = layers.ZeroPadding1D(padding=1,name=\"padding%d_branch2a\" %ii)(x)\n",
    "        intermediate_features = layers.Conv1D(filters=NFEATURES,kernel_size=2,strides=1,use_bias=False,\n",
    "                          name=\"res%d_branch2a\" %ii)(intermediate_features)\n",
    "        intermediate_features = layers.BatchNormalization()(intermediate_features)\n",
    "        intermediate_features = layers.Activation(\"relu\", name=\"res%d_branch2a_relu\" % ii)(intermediate_features)\n",
    "\n",
    "        intermediate_features = layers.Conv1D(filters=NFEATURES,kernel_size=2,strides=1,use_bias=False,\n",
    "                          name=\"res%d_branch2b\" %ii)(intermediate_features)\n",
    "        intermediate_features = layers.BatchNormalization()(intermediate_features)\n",
    "        intermediate_features = layers.Activation(\"relu\", name=\"res%d_branch2b_relu\" % ii)(intermediate_features)\n",
    "\n",
    "        shortcut = x\n",
    "        x = layers.Add(name=\"res%d_add\" % ii)([intermediate_features, shortcut])\n",
    "\n",
    "    y = layers.ZeroPadding1D(padding=1,name=\"padding%d_branch2a\" % num_res_blocks)(x)\n",
    "    y = layers.Conv1D(filters,kernel_size,strides=stride,use_bias=False,\n",
    "                                name=\"res%d_branch2a\" % num_res_blocks)(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation(\"relu\", name=\"res%d_branch2a_relu\" % num_res_blocks)(y)\n",
    "\n",
    "    y = layers.ZeroPadding1D(padding=1,name=\"padding%d_branch2b\" % num_res_blocks)(y)\n",
    "    y = layers.Conv1D(filters,kernel_size,use_bias=False,\n",
    "                            name=\"res%d_branch2b\" % num_res_blocks)(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(nhidden1, activation=act_func)(y)\n",
    "\n",
    "    shortcut = inputs\n",
    "    shortcut = layers.Dense(nhidden1, activation=act_func)(shortcut)\n",
    "\n",
    "    y = layers.Add(name=\"res%d_add\" % num_res_blocks)([y, shortcut])\n",
    "    y = layers.Dropout(intermediate_dropout)(y)\n",
    "\n",
    "    y = layers.Activation(\"relu\",name=\"res_relu\")(y)\n",
    "\n",
    "\n",
    "    y = layers.Dense(nhidden2, activation=act_func)(y)\n",
    "    outputs= layers.Dense(output_shape, activation=keras.activations.linear,name='output')(y)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "def build_residual_lstm_model(nhidden1=8, nhidden2=2, output_shape=1,\n",
    "                              act_func='sigmoid',layer_type='lstm',\n",
    "                              conv_init=None,\n",
    "                              input_dropout=0.):\n",
    "    rnn_layer = layers.LSTM if layer_type == 'lstm' else layers.GRU\n",
    "    inputs = layers.Input(shape=NFEATURES*(ndays+nwindows))\n",
    "    x = layers.Reshape((ndays+nwindows,NFEATURES))(inputs)\n",
    "    x = layers.Dropout(input_dropout)(x)\n",
    "    x = layers.Permute((2,1))(x)\n",
    "\n",
    "    y = tf.keras.layers.Conv1D(ndays+nwindows,1, activation='relu',\n",
    "                            kernel_initializer=conv_init,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),\n",
    "                            trainable=False)(x)\n",
    "\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(nhidden1, activation=act_func)(y)\n",
    "    y = layers.Dropout(intermediate_dropout)(y)\n",
    "    y = layers.Dense(nhidden2, activation=act_func)(y)\n",
    "    y = layers.Dense(output_shape, activation=keras.activations.linear,name='mlp_output')(y)\n",
    "\n",
    "    shortcut = x\n",
    "    shortcut = layers.Dense(nhidden1, activation=act_func)(shortcut)\n",
    "    shortcut = rnn_layer(units = output_shape*2, activation=act_func,return_sequences=True)(shortcut)\n",
    "    shortcut = layers.Flatten()(shortcut)\n",
    "    shortcut = layers.Dense(output_shape, activation=keras.activations.linear,name='lstm_output')(shortcut)\n",
    "\n",
    "    outputs = layers.Add(name=\"res_add\")([y, shortcut])\n",
    "    # outputs = layers.Activation(\"relu\",name=\"res_relu\")(outputs)\n",
    "    outputs = layers.LeakyReLU(alpha=0.3,name=\"res_relu\")(outputs)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_transformer(head_size,\n",
    "                      num_heads,\n",
    "                      ff_dim,\n",
    "                      num_transformer_blocks,\n",
    "                      mlp_units,\n",
    "                      output_shape,\n",
    "                      dropout=0,\n",
    "                      mlp_dropout=0,\n",
    "                      input_dropout=0):\n",
    "    inputs = keras.Input(shape=NFEATURES*(ndays+nwindows))\n",
    "    x = layers.Reshape((ndays+nwindows,NFEATURES))(inputs)\n",
    "    x = layers.Dropout(input_dropout)(x)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(output_shape)(x)\n",
    "    outputs = layers.LeakyReLU(alpha=0.3,name=\"res_relu\")(outputs)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=initial_lr), loss=mse_loss_masked)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7244215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:28:04.266580Z",
     "start_time": "2023-01-20T00:28:04.252640Z"
    },
    "id": "b7244215"
   },
   "outputs": [],
   "source": [
    "# create folders in Google Drive to save results\n",
    "result_folders = ['models','results','images']\n",
    "for result_folder in result_folders:\n",
    "    if not os.path.exists(os.path.join(local_root_path, result_folder)):\n",
    "        os.makedirs(os.path.join(local_root_path, result_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H5l-Ym2VaoDh",
   "metadata": {
    "id": "H5l-Ym2VaoDh"
   },
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2383d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:37:52.944442Z",
     "start_time": "2023-01-20T00:28:04.268569Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "executionInfo": {
     "elapsed": 25457,
     "status": "ok",
     "timestamp": 1670875989934,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "2be2383d",
    "outputId": "e4ab2216-4e41-4be8-e13a-890d6f838a22"
   },
   "outputs": [],
   "source": [
    "full_model_str_def = 'i%d_'%(ndays + nwindows) +model_str_def\n",
    "start = time.time()\n",
    "model_path_prefix = \"mtl_%s_%s\" % (full_model_str_def, scenario_name)\n",
    "\n",
    "if train_models:\n",
    "\n",
    "    print('Training model %s for %d stations: ' % (full_model_str_def, len(output_stations)))\n",
    "\n",
    "    print([station.replace('target/','').replace('target','') for station in output_stations])\n",
    "\n",
    "\n",
    "    if os.path.exists(os.path.join(local_root_path, 'models', model_path_prefix +'.h5')):\n",
    "        loaded_model = annutils.load_model(os.path.join(local_root_path,'models', model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "        model = loaded_model.model\n",
    "        print('Ignored defined model arc and loaded pre-trained model from %s.h5' % os.path.join(local_root_path,'models', model_path_prefix))\n",
    "\n",
    "    if 'resnet' in model_str_def.lower():\n",
    "        model = build_resnet_model(nhidden1=num_neurons_multiplier[0]*yallc.shape[1], nhidden2=num_neurons_multiplier[-1]*yallc.shape[1], output_shape=yallc.shape[1],\n",
    "                                   num_res_blocks=num_res_blocks,\n",
    "                                   input_dropout=input_dropout)\n",
    "    elif ('residual_lstm' in model_str_def.lower()) or ('residual_gru' in model_str_def.lower()):\n",
    "        conv_init = tf.constant_initializer(annutils.conv_filter_generator(ndays=ndays,\n",
    "                                                                           window_size=window_size,\n",
    "                                                                           nwindows=nwindows))\n",
    "\n",
    "        model = build_residual_lstm_model(num_neurons_multiplier[0]*len(output_stations),\n",
    "                                 num_neurons_multiplier[-1]*len(output_stations),\n",
    "                                 output_shape=yallc.shape[1],\n",
    "                                 act_func='sigmoid',\n",
    "                                 layer_type=model_str_def.lower().split('_')[2],\n",
    "                                 conv_init=conv_init,\n",
    "                                 input_dropout=input_dropout)\n",
    "    elif 'transformer' in model_str_def.lower():\n",
    "        model = build_transformer(head_size=256,\n",
    "                                  num_heads=4,\n",
    "                                  ff_dim=4,\n",
    "                                  num_transformer_blocks=4,\n",
    "                                  mlp_units=[128],\n",
    "                                  output_shape=yallc.shape[1],\n",
    "                                  mlp_dropout=0.4,\n",
    "                                  dropout=0.25,\n",
    "                                  input_dropout=input_dropout)\n",
    "    else:\n",
    "        model = build_model_from_string_def(full_model_str_def,width_multiplier=yallc.shape[1])\n",
    "    display(model.summary())\n",
    "    history = model.fit(\n",
    "        xallc,\n",
    "        yallc,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        validation_data=(xallv, yallv),\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=50, mode=\"min\", restore_best_weights=True),\n",
    "            tensorboard_cb\n",
    "        ],\n",
    "        verbose=2,\n",
    "    )\n",
    "    if save_results:\n",
    "        model_savepath = os.path.join(local_root_path, 'models', model_path_prefix)\n",
    "        annutils.save_model(model_savepath, model, xscaler, yscaler)\n",
    "        print('Model saved to %s' % model_savepath)\n",
    "    print('Training time: %d min' % ((time.time()-start)/60)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19pX3dRUwLQ0",
   "metadata": {
    "id": "19pX3dRUwLQ0"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R0ix5jxYPi9k",
   "metadata": {
    "id": "R0ix5jxYPi9k"
   },
   "source": [
    "## Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ac231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:37:52.959378Z",
     "start_time": "2023-01-20T00:37:52.946433Z"
    },
    "id": "d96ac231"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "eval_metrics = ['MSE', 'Bias', 'R', 'RMSD', 'NSE']\n",
    "\n",
    "key_stations = ['RSAC064','CCWD_Rock','CHDMC006', 'CHSWP003','RSAC092','RSAN018']\n",
    "key_station_mappings = {'CCWD Rock': 'RSL',\n",
    "                        'Middle River Intake':'MUP',\n",
    "                        'Old River Hwy 4': 'OH4',\n",
    "                        'OLD MID':'OLD_MID'}\n",
    "\n",
    "\n",
    "def evaluate_sequences(target, pred, metrics,print_details=False):\n",
    "    assert len(target) == len(pred), 'Target and predicted sequence length must equal.'\n",
    "    valid_entries = target>0\n",
    "    sequence_length = np.sum(valid_entries)\n",
    "    # print('Total samples: %d, valid samples: %d' % (len(target), np.sum(valid_entries)))\n",
    "    if np.any(sequence_length == 0):\n",
    "        return {k: 0 for k in metrics}\n",
    "    target=target[valid_entries]\n",
    "    pred = pred[valid_entries]\n",
    "    SD_pred = np.sqrt( np.sum((pred-np.mean(pred)) ** 2) /(sequence_length-1))\n",
    "    SD_target = np.sqrt( np.sum((target-np.mean(target)) ** 2) /(sequence_length-1))\n",
    "\n",
    "    eval_results = defaultdict(float)\n",
    "    \n",
    "    for m in metrics:\n",
    "        if m =='MSE':\n",
    "            eval_results[m] = ((target - pred)**2).mean()\n",
    "        elif m =='Bias':\n",
    "            eval_results[m] = np.sum(pred - target)/np.sum(target) * 100\n",
    "        elif m == 'R':\n",
    "            eval_results[m] = np.sum(np.abs((pred-np.mean(pred)) * (target - np.mean(target)))) / (sequence_length * SD_pred * SD_target)\n",
    "        elif m == 'RMSD':\n",
    "            eval_results[m] = np.sqrt(np.sum( ( ( pred-np.mean(pred) ) * ( target - np.mean(target) ) ) ** 2 ) / sequence_length)\n",
    "        elif m == 'NSE':\n",
    "            eval_results[m] = 1 - np.sum( ( target - pred ) ** 2 ) / np.sum( (target - np.mean(target) ) ** 2 )\n",
    "    if print_details:\n",
    "        print('(sum(pred - mean(pred)) x (target - mean(target))) =  %.4f' % np.sum((pred-np.mean(pred)) * (target - np.mean(target))))\n",
    "        print('MSE =  %.4f' % eval_results[m])\n",
    "        print('sum(pred - target) = %.4f' % np.sum(pred - target))\n",
    "        print('sum(target) = %.4f' % np.sum(target))\n",
    "        print('target standard deviation = %.6f, prediction standard deviation =%.6f' %(SD_target,SD_pred))\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RN9HCL9YX7w0",
   "metadata": {
    "id": "RN9HCL9YX7w0"
   },
   "source": [
    "## Compute Numerical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P8lFDBnQFkJx",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:38:11.590597Z",
     "start_time": "2023-01-20T00:37:52.961368Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35025,
     "status": "ok",
     "timestamp": 1670876024956,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "P8lFDBnQFkJx",
    "outputId": "9ad24074-45aa-46ca-cc3e-c66fe4879945"
   },
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    full_results={}\n",
    "    range_results=defaultdict(defaultdict)\n",
    "\n",
    "\n",
    "    print('Testing MTL ANN for %d stations: ' % len(output_stations),end='')\n",
    "\n",
    "    model_path_prefix = \"mtl_%s_%s\" % (full_model_str_def, scenario_name)\n",
    "\n",
    "    annmodel = annutils.load_model(os.path.join(local_root_path,'models', model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "\n",
    "    train_pred = annmodel.model.predict(xallc).clip(0, 1)\n",
    "    val_pred = annmodel.model.predict(xallv).clip(0, 1)\n",
    "    test_pred = annmodel.model.predict(xallt).clip(0, 1)\n",
    "    observed_pred = annmodel.model.predict(x_extra_test).clip(0, 1)\n",
    "\n",
    "    val_target = yallv.to_numpy()\n",
    "    test_target = yallt.to_numpy()\n",
    "    observed_target = y_extra_test.to_numpy()\n",
    "\n",
    "    for ii, location in enumerate(output_stations):\n",
    "        if any([k.lower() in location.lower() for k in key_stations]):\n",
    "            print_details = print_detailed_key_station_metrics\n",
    "        else:\n",
    "            print_details = False\n",
    "        print(location)\n",
    "        \n",
    "        # compute training results\n",
    "        train_results = evaluate_sequences(yallc.iloc[:,ii],train_pred[:,ii], eval_metrics,\n",
    "                                        print_details=print_details)\n",
    "        train_results['R^2'] = r2_score(train_pred[:,ii], yallc.iloc[:,ii])\n",
    "        full_results['%s_train' %location] = train_results\n",
    "\n",
    "        # compute test results\n",
    "        test_results = evaluate_sequences(yallt.iloc[:,ii], test_pred[:,ii], eval_metrics,\n",
    "                                            print_details=print_details)\n",
    "        test_results['R^2'] = r2_score(test_pred[:,ii], yallt.iloc[:,ii])\n",
    "        full_results['%s_test' %location] = test_results\n",
    "\n",
    "        # compute validation results\n",
    "        val_results = evaluate_sequences(yallv.iloc[:,ii],val_pred[:,ii], eval_metrics,\n",
    "                                        print_details=print_details)\n",
    "        val_results['R^2'] = r2_score(val_pred[:,ii], yallv.iloc[:,ii])\n",
    "        full_results['%s_val' %location] = val_results\n",
    "\n",
    "        # compute results on observed data\n",
    "        observed_results = evaluate_sequences(y_extra_test.iloc[:,ii], observed_pred[:,ii], eval_metrics,\n",
    "                                            print_details=print_details)\n",
    "        observed_results['R^2'] = r2_score(observed_pred[:,ii], y_extra_test.iloc[:,ii])\n",
    "        full_results['%s_observed' %location] = observed_results\n",
    "\n",
    "        # compute results at different EC ranges on test sets (augmented)\n",
    "        for (lower_quantile, upper_quantile) in zip(percentiles,percentiles[1:]+[1,]):\n",
    "            if print_details:\n",
    "                print('#'*20, '%d%% - %d%%' % ((lower_quantile*100, upper_quantile*100)),'#'*20)\n",
    "            lower_threshold = np.quantile(test_target[:,ii], lower_quantile)\n",
    "            upper_threshold = np.quantile(test_target[:,ii], upper_quantile)\n",
    "            eval_results = evaluate_sequences(test_target[(test_target[:,ii] > lower_threshold) & (test_target[:,ii] <= upper_threshold),ii],\n",
    "                                            test_pred[(test_target[:,ii] > lower_threshold) & (test_target[:,ii] <= upper_threshold),ii],\n",
    "                                            eval_metrics,\n",
    "                                            print_details=print_details)\n",
    "            range_results[location][lower_quantile*100] = eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eDzCRYN2YC8Q",
   "metadata": {
    "id": "eDzCRYN2YC8Q"
   },
   "source": [
    "## Save Numerical Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RlOAfzjwW8IZ",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:38:11.606525Z",
     "start_time": "2023-01-20T00:38:11.592587Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1670876024957,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "RlOAfzjwW8IZ",
    "outputId": "75a0855d-f47e-4616-a9c1-c1edb81781b0"
   },
   "outputs": [],
   "source": [
    "if save_results:\n",
    "\n",
    "    # create a pickle file on Google Drive and write results \n",
    "    result_path_prefix = \"mtl_%s_%s\" % ('i%d_'%(ndays + nwindows) +model_str_def, scenario_name)\n",
    "    results_path = os.path.join(local_root_path,\"results/%s_full_results.pkl\" % (result_path_prefix))\n",
    "\n",
    "    f = open(results_path,\"wb\")\n",
    "    pickle.dump(full_results,f)\n",
    "    f.close()\n",
    "    print('Numerical results saved to %s' % results_path)\n",
    "\n",
    "    ### Uncomment below if you want to save results at different ranges ###\n",
    "    \n",
    "    # range_results_path = os.path.join(local_root_path,\"results/%s_ranged_results.pkl\" % (result_path_prefix))\n",
    "\n",
    "    # f = open(range_results_path,\"wb\")\n",
    "    # pickle.dump(range_results,f)\n",
    "    # f.close()\n",
    "    # print('Ranged numerical results saved to %s' % range_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dyqvpvChW2-2",
   "metadata": {
    "id": "dyqvpvChW2-2"
   },
   "source": [
    "## Generate Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5D4Xgu-oYTsf",
   "metadata": {
    "id": "5D4Xgu-oYTsf"
   },
   "source": [
    "### Exceedance Probability Plots and Time Series Plots for Key Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dKu1K_kbhEFO",
   "metadata": {
    "id": "dKu1K_kbhEFO"
   },
   "source": [
    "#### Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbxv9pxFsDzX",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:39:36.001193Z",
     "start_time": "2023-01-20T00:38:11.608517Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "executionInfo": {
     "elapsed": 27320,
     "status": "ok",
     "timestamp": 1670877058782,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "bbxv9pxFsDzX",
    "outputId": "83aaf037-e517-4d4a-deee-3f6852ed8c57"
   },
   "outputs": [],
   "source": [
    "test_data['baseline'] = \"dsm2_ann_inputs_base.xlsx\"\n",
    "test_data['observed'] = \"observed_data_daily.xlsx\"\n",
    "\n",
    "if save_results:\n",
    "    from matplotlib import ticker\n",
    "    import matplotlib.dates as mdates\n",
    "    from matplotlib.lines import Line2D\n",
    "    myfontsize=15\n",
    "    ncols=2\n",
    "    print('Testing MTL ANN for %d stations: ' % len(output_stations),end='')\n",
    "\n",
    "    print([station.replace('target/','').replace('target','') for station in output_stations],end='\\n')\n",
    "    model_path_prefix = \"mtl_%s_%s\" % ('i%d_'%(ndays + nwindows) +model_str_def, scenario_name)\n",
    "\n",
    "    annmodel = annutils.load_model(os.path.join(local_root_path,'models', model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "\n",
    "    for test_scenario, data_file in test_data.items():\n",
    "        fig_savepath = os.path.join(local_root_path,\"images/%s_%s_time_series.png\"% (model_path_prefix, test_scenario))\n",
    "        print('Scenario: %s' % test_scenario)\n",
    "        data_path = os.path.join(local_root_path,data_file)\n",
    "\n",
    "        dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "        df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "        dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "        dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "        dfouts = dfouts.rename(columns=name_mapping)[output_stations]\n",
    "        \n",
    "        fig_timeseries, ax_timeseries = plt.subplots(nrows=len(key_stations)//ncols,\n",
    "                                                                    ncols=ncols,\n",
    "                                                                    figsize=(6*ncols,3*len(key_stations)//ncols))\n",
    "        fig_timeseries.tight_layout(h_pad=3.5, w_pad=2)\n",
    "        ii = 0\n",
    "\n",
    "        ## test results\n",
    "        dfp=annutils.predict(annmodel.model, dfinps, annmodel.xscaler, annmodel.yscaler,columns=output_stations,\n",
    "                            ndays=ndays,window_size=window_size,nwindows=nwindows,verbose=0)\n",
    "        selected_key_stations = []\n",
    "        for location in output_stations:\n",
    "            if any([k.lower() in location.lower() for k in key_stations]):\n",
    "                selected_key_stations.append(location)\n",
    "\n",
    "        for location in selected_key_stations:\n",
    "            if any([k.lower() in location.lower() for k in key_stations]):\n",
    "                simplified_station_name = location.split('-')[0].replace('_',' ').replace('-',' ')\n",
    "                simplified_station_name = key_station_mappings.get(simplified_station_name) or simplified_station_name\n",
    "                y = dfouts.loc[:,location].copy()\n",
    "                y[y<0] = float('nan')\n",
    "\n",
    "                # Time Series Plots on Ground Truths vs. Model Predictions\n",
    "                ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(y.iloc[(ndays+nwindows*window_size-1):],'-',color='C0',alpha=0.8)\n",
    "                ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(dfp.loc[:,location].clip(0,y.iloc[(ndays+nwindows*window_size-1):]),'-',color='C3',alpha=0.8)\n",
    "                if ii == ncols:\n",
    "                    custom_lines = [Line2D([0], [0], color='C0'),\n",
    "                                    Line2D([0], [0], color='C3')]\n",
    "                    \n",
    "                    ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].legend(custom_lines,\n",
    "                                                                                    ['Target','Model Predictions'],\n",
    "                                                                                    bbox_to_anchor=(1.28, 2.76),\n",
    "                                                                                    fontsize=myfontsize)\n",
    "                \n",
    "\n",
    "                ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].set_title(simplified_station_name,fontsize=myfontsize)\n",
    "                ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].set_xticks(pd.date_range(start=dfp.index[0],\n",
    "                                                                                            end=dfp.index[-1],\n",
    "                                                                                            periods=5))\n",
    "                ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "                ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].set_xlim([dfinps.index[0], dfinps.index[-1]])\n",
    "                ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].tick_params(axis='x', labelsize=myfontsize)\n",
    "                ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].tick_params(axis='y', labelsize=myfontsize)\n",
    "                t = ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].text(dfinps.index[int(dfinps.shape[0]*0.4)],\n",
    "                            ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].get_ylim()[1]*0.58,\n",
    "                            '               Train       Test\\nR           %.3f       %.3f\\nNSE       %.3f       %.3f\\n%% Bias  %.3f%%   %.3f%%' % (full_results['%s_train' %location]['R'],\n",
    "                                                                    full_results['%s_test' %location]['R'],\n",
    "                                                                    full_results['%s_train' %location]['NSE'],\n",
    "                                                                    full_results['%s_test' %location]['NSE'],\n",
    "                                                                    full_results['%s_train' %location]['Bias'],\n",
    "                                                                    full_results['%s_test' %location]['Bias']),\n",
    "                            fontsize=myfontsize-2)\n",
    "                t.set_bbox(dict(facecolor='white', alpha=0.5, edgecolor='white'))\n",
    "                if ii >= len(key_stations) - ncols:\n",
    "                    ax_timeseries[ii//ncols,int(ii-(ii//ncols)*ncols)].set_xlabel('Time')\n",
    "                ii += 1\n",
    "        fig_timeseries.savefig(fig_savepath,bbox_inches='tight',dpi=300)\n",
    "        print('Figure saved as %s' % fig_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kV6L2gPvA-uC",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:39:36.017122Z",
     "start_time": "2023-01-20T00:39:36.004180Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670876875749,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "kV6L2gPvA-uC",
    "outputId": "337dcf07-12c6-406c-ed72-21505907e054"
   },
   "outputs": [],
   "source": [
    "pd.date_range(start=dfp.index[0],end=dfp.index[-1],freq='5A')#,periods=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rvdffeEUhNrV",
   "metadata": {
    "id": "rvdffeEUhNrV"
   },
   "source": [
    "#### Exceedance Prob Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S9D5zkmsT2Wk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:39:36.033053Z",
     "start_time": "2023-01-20T00:39:36.019113Z"
    },
    "id": "S9D5zkmsT2Wk"
   },
   "outputs": [],
   "source": [
    "def add_subplot_axes(ax,rect,facecolor='w'): # matplotlib 2.0+\n",
    "    fig = plt.gcf()\n",
    "    box = ax.get_position()\n",
    "    width = box.width\n",
    "    height = box.height\n",
    "    inax_position  = ax.transAxes.transform(rect[0:2])\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    infig_position = transFigure.transform(inax_position)    \n",
    "    x = infig_position[0]\n",
    "    y = infig_position[1]\n",
    "    width *= rect[2]\n",
    "    height *= rect[3]\n",
    "    subax = fig.add_axes([x,y,width,height],facecolor=facecolor)  # matplotlib 2.0+\n",
    "    x_labelsize = subax.get_xticklabels()[0].get_size()\n",
    "    y_labelsize = subax.get_yticklabels()[0].get_size()\n",
    "    x_labelsize *= rect[2]**0.5\n",
    "    y_labelsize *= rect[3]**0.5\n",
    "    subax.xaxis.set_tick_params(labelsize=x_labelsize)\n",
    "    subax.yaxis.set_tick_params(labelsize=y_labelsize)\n",
    "    return subax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sQQ3AzvvSjjE",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:41:08.558390Z",
     "start_time": "2023-01-20T00:39:36.036040Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 120902,
     "status": "ok",
     "timestamp": 1670876259176,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "sQQ3AzvvSjjE",
    "outputId": "58bb21db-c307-48e5-bc9a-22af82f8f436"
   },
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    myfontsize=15\n",
    "    ncols=2\n",
    "\n",
    "    print('Testing MTL ANN for %d stations: ' % len(output_stations),end='')\n",
    "\n",
    "    print([station.replace('target/','').replace('target','') for station in output_stations],end='\\n')\n",
    "    model_path_prefix = \"mtl_%s_%s\" % ('i%d_'%(ndays + nwindows) +model_str_def, scenario_name)\n",
    "\n",
    "    annmodel = annutils.load_model(os.path.join(local_root_path,'models', model_path_prefix),custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "    for test_scenario, data_file in test_data.items():\n",
    "        fig_savepath = os.path.join(local_root_path,\"images/%s_%s_exceedance_prob.png\"% (model_path_prefix, test_scenario))\n",
    "        print('Scenario: %s' % test_scenario)\n",
    "        data_path = os.path.join(local_root_path,data_file)\n",
    "\n",
    "        dflist = [pd.read_excel(data_path,i,index_col=0,parse_dates=True) for i in range(num_sheets)]\n",
    "\n",
    "        df_inpout = pd.concat(dflist[0:num_sheets],axis=1).dropna(axis=0)\n",
    "        dfinps = df_inpout.loc[:,~df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "        dfouts = df_inpout.loc[:,df_inpout.columns.isin(dflist[num_sheets-1].columns)]\n",
    "        dfouts = dfouts.rename(columns=name_mapping)[output_stations]\n",
    "        \n",
    "        ## training results\n",
    "        dfp=annutils.predict(annmodel.model, dfinps, annmodel.xscaler, annmodel.yscaler,columns=output_stations,\n",
    "                            ndays=ndays,window_size=window_size,nwindows=nwindows,verbose=0)\n",
    "        \n",
    "        selected_key_stations = []\n",
    "        fig_exceedance, ax_exceedance = plt.subplots(nrows=len(key_stations)//ncols,\n",
    "                                                            ncols=ncols,\n",
    "                                                            figsize=(6*ncols,4*len(key_stations)//ncols))\n",
    "        fig_exceedance.tight_layout(h_pad=5, w_pad=2)\n",
    "        ii = 0\n",
    "\n",
    "        for location in output_stations:\n",
    "            if any([k.lower() in location.lower() for k in key_stations]):\n",
    "                selected_key_stations.append(location)\n",
    "\n",
    "        for location in selected_key_stations:\n",
    "            simplified_station_name = location.split('-')[0].replace('_',' ').replace('-',' ')\n",
    "            simplified_station_name = key_station_mappings.get(simplified_station_name) or simplified_station_name\n",
    "            y = dfouts.loc[:,location].copy()\n",
    "            y[y<0] = float('nan')\n",
    "\n",
    "            # Combined exceedance probability plots\n",
    "            fig_exceedance, ax_exceedance\n",
    "            test_y_sorted = np.sort(y.iloc[(ndays+nwindows*window_size-1):].dropna().to_numpy())\n",
    "            test_pred_sorted = np.sort(dfp.loc[:,location].clip(0,dfouts.loc[:,location].max()).to_numpy())\n",
    "\n",
    "            # calculate the proportional values of samples\n",
    "            p = 1. * np.arange(len(test_y_sorted)-1,-1,-1) / (len(test_y_sorted) - 1)\n",
    "            p_pred = 1. * np.arange(len(test_pred_sorted)-1,-1,-1) / (len(test_pred_sorted) - 1)\n",
    "\n",
    "            ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].tick_params(axis='x', labelsize=myfontsize)\n",
    "            ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].tick_params(axis='y', labelsize=myfontsize)\n",
    "\n",
    "            # plot the sorted data:\n",
    "            ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(p, test_y_sorted)\n",
    "            ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].plot(p_pred, test_pred_sorted,'--',color='C3')\n",
    "            ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].ticklabel_format(axis='y',style='sci',scilimits=(0,0))\n",
    "            if int(ii-(ii//ncols)*ncols) == 0:\n",
    "                ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].set_ylabel('EC ' + r\"$(\\mu S/cm)$\",fontsize=myfontsize)\n",
    "            ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].yaxis.get_offset_text().set_fontsize(myfontsize)\n",
    "            ylims = ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].get_ylim()\n",
    "            if ii >= len(key_stations) - ncols:\n",
    "                ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].set_xlabel('Exceedance Probability ' + r\"$(\\%)$\",fontsize=myfontsize)\n",
    "                text_yloc = ylims[0]-(ylims[1]-ylims[0])*0.37\n",
    "            else:\n",
    "                text_yloc = ylims[0]-(ylims[1]-ylims[0])*0.2\n",
    "\n",
    "            ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].text(0.48,\n",
    "                                                                            text_yloc,\n",
    "                                                                            '(%s)'%chr(97+ii),weight='bold',fontsize=myfontsize)\n",
    "            if ii == ncols:\n",
    "                plt.legend(['Target','Model Predictions'],fontsize=myfontsize,bbox_to_anchor=(-0.3, 2))\n",
    "            ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)].set_title(simplified_station_name,fontsize=myfontsize)\n",
    "            \n",
    "            subax1 = add_subplot_axes(ax_exceedance[ii//ncols,int(ii-(ii//ncols)*ncols)],[0.4,0.5,0.58,0.44]) # xloc, yloc, xwidth, ywidth\n",
    "            subax1.plot(y.iloc[(ndays+nwindows*window_size-1):])\n",
    "            subax1.plot(dfp.loc[:,location].clip(0,dfouts.loc[:,location].max()),'--',color='C3',alpha=0.8)\n",
    "            subax1.set_xticks(pd.date_range(start=dfp.index[0],\n",
    "                                            end=dfp.index[-1],\n",
    "                                            periods=5))\n",
    "            subax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            plt.xticks(fontsize=myfontsize-3)\n",
    "            min_x = int(y.iloc[(ndays+nwindows*window_size-1):].min()//100*100)\n",
    "            max_x = int((y.iloc[(ndays+nwindows*window_size-1):].max()//100+1)*100)\n",
    "            subax1.set_yticks(np.linspace(min_x,max_x,(max_x-min_x)//((max_x-min_x) // 3//100*100)))\n",
    "            plt.yticks(fontsize=myfontsize-3)\n",
    "\n",
    "            subax1.yaxis.get_offset_text().set_fontsize(myfontsize-3)\n",
    "            ii += 1\n",
    "\n",
    "        fig_exceedance.savefig(fig_savepath,bbox_inches='tight',dpi=300)\n",
    "        print('Figure saved as %s' % fig_savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z30TDstuYpO5",
   "metadata": {
    "id": "z30TDstuYpO5"
   },
   "source": [
    "### Station-wise Heatmap Plots of $r^2$, Bias, RSR and Bias at Different EC Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E-F7hULi-6TB",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-20T00:41:10.225956Z",
     "start_time": "2023-01-20T00:41:08.561298Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "executionInfo": {
     "elapsed": 3489,
     "status": "ok",
     "timestamp": 1670876590600,
     "user": {
      "displayName": "Siyu Qi",
      "userId": "06573567475482842475"
     },
     "user_tz": 480
    },
    "id": "E-F7hULi-6TB",
    "outputId": "c8b4253f-0ea0-4e27-d340-9a40abe9967c"
   },
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    fig_savepath = os.path.join(local_root_path,\"images/%s_Range_Performance.png\"% (model_path_prefix))\n",
    "    stations = []\n",
    "    simplified_station_names = []\n",
    "    for s in list(range_results.keys()):\n",
    "        new_s = s.split('-')[0].replace('_',' ').replace('-',' ').split(':')[0]\n",
    "        if new_s in key_station_mappings:\n",
    "            new_s = key_station_mappings[new_s]\n",
    "        stations.append(s)\n",
    "        simplified_station_names.append(new_s)\n",
    "    # plot percentile results for key stations\n",
    "    myfontsize=14\n",
    "    # train and test results plot in different figures\n",
    "    plot_metrics=['r^2','Bias','RSR','NSE']\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=len(plot_metrics),figsize=(15,8))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "    for ii, metric in enumerate(plot_metrics):\n",
    "        cmap='viridis'\n",
    "        plot_metric = metric\n",
    "        if metric=='r^2':\n",
    "            plot_metric = 'R'\n",
    "        elif metric=='RSR':\n",
    "            plot_metric = 'NSE'\n",
    "            cmap='viridis_r'\n",
    "        elif metric =='Bias':\n",
    "            # cmap = Pbias_map\n",
    "            cmap = 'RdYlBu_r'\n",
    "        to_plot=np.zeros([len(output_stations),len(percentiles)])\n",
    "        for x_loc, station in enumerate(output_stations):\n",
    "            for y_loc, percentile_range in enumerate(percentiles):\n",
    "                if metric=='r^2':\n",
    "                    to_plot[x_loc,y_loc] = range_results[station][int(percentile_range*100)][plot_metric]**2\n",
    "                elif metric=='RSR':\n",
    "                    to_plot[x_loc,y_loc] = np.sqrt(1-range_results[station][int(percentile_range*100)][plot_metric])\n",
    "                else:\n",
    "                    to_plot[x_loc,y_loc] = range_results[station][int(percentile_range*100)][plot_metric]\n",
    "\n",
    "        current_plot = ax[ii].pcolor(to_plot,cmap=cmap)\n",
    "        current_plot.set_clim(current_plot.get_clim()[0],max(current_plot.get_clim()[1],1))\n",
    "        cbar = plt.colorbar(current_plot,ax=ax[ii],aspect=30,fraction=0.1, pad=0.04)\n",
    "        cbar.ax.tick_params(labelsize=myfontsize-1) \n",
    "\n",
    "        if metric =='Bias':\n",
    "            ax[ii].set_title('Percent Bias (%)',fontsize=myfontsize+2)\n",
    "        else:\n",
    "            ax[ii].set_title(r'${}$'.format(metric),fontsize=myfontsize+2)\n",
    "        if ii == 0:\n",
    "            ax[ii].set_ylabel('Study Location',fontsize=myfontsize+2)\n",
    "            ax[ii].set_yticks(np.arange(len(output_stations))+0.5)\n",
    "            ax[ii].set_yticklabels(simplified_station_names,fontsize=myfontsize-3)\n",
    "        else:\n",
    "            ax[ii].set_yticks([])\n",
    "\n",
    "        ax[ii].set_xlabel('Range',fontsize=myfontsize+2)\n",
    "        ax[ii].set_xticks(np.arange(len(percentiles))+0.5)\n",
    "        ax[ii].xaxis.set_tick_params(size=0)\n",
    "        ax[ii].set_xticklabels(['0-%d%%'%(percentiles[1]*100),'%d-%d%%'%(percentiles[1]*100,percentiles[2]*100),'%d-100%%'%(percentiles[2]*100)],\n",
    "                            fontsize=myfontsize-3)\n",
    "\n",
    "        ax[ii].text(1.4, ax[ii].get_ylim()[0]-3.5,\n",
    "                    '(%s)'%chr(97+ii),weight='bold',fontsize=myfontsize)\n",
    "    plt.savefig(fig_savepath,bbox_inches='tight',dpi=300)\n",
    "    print('Figure saved as %s' % fig_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa0b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "119HsOdPrPOJ-WHBwakDUqTVpjklGRVGg",
     "timestamp": 1652220006613
    },
    {
     "file_id": "1OZ9foQyQP5P9hfWyEjEg0lTJW3s8sXTs",
     "timestamp": 1647365592417
    },
    {
     "file_id": "1xVpgVH4RbKZ3-vjcRePmqVU_4kjv8zZs",
     "timestamp": 1646788842444
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:Salinity_DWR]",
   "language": "python",
   "name": "conda-env-Salinity_DWR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "3d3652577a7c35b42d007c0caebd8ec483b346ab09486faf41fb90f43ee21a27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
