{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T18:54:32.146680300Z",
     "start_time": "2023-06-27T18:54:32.134679300Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomkovic\\Miniconda3\\envs\\Salinity_DWR\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "local_root_path = \".\"\n",
    "\n",
    "sys.path.append(local_root_path)\n",
    "import annutils\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# excel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T18:54:37.535209100Z",
     "start_time": "2023-06-27T18:54:37.514451800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# find all the .xlsx files in the local_root_path dir\n",
    "# excel_files = [f for f in os.listdir(local_root_path) if f.endswith(\".xlsx\") and not f.startswith(\"~$\")]\n",
    "# excel_files = [r\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\modified_bc\\anninputs\\perturbhist\\dsm2_ann_inputs_perturbhist.xlsx\"]\n",
    "excel_files = [r\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\historical\\anninputs\\dsm2_ann_inputs_historical.xlsx\"]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup stations\n",
    "In order to turn the excel sheets into inputs we can predict on we need to setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T18:54:40.283853600Z",
     "start_time": "2023-06-27T18:54:40.275836Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_sheets = 9\n",
    "\n",
    "observed_stations_ordered_by_median = ['RSMKL008', 'RSAN032', 'RSAN037', 'RSAC092', 'SLTRM004', 'ROLD024',\n",
    "                                       'CHVCT000', 'RSAN018', 'CHSWP003', 'CHDMC006', 'SLDUT007', 'RSAN072',\n",
    "                                       'OLD_MID', 'RSAN058', 'ROLD059', 'RSAN007', 'RSAC081', 'SLMZU025',\n",
    "                                       'RSAC075', 'SLMZU011', 'SLSUS012', 'SLCBN002', 'RSAC064']\n",
    "\n",
    "output_stations = ['CHDMC006-CVP INTAKE', 'CHSWP003-CCFB_INTAKE', 'CHVCT000-VICTORIA INTAKE',\n",
    "                   'OLD_MID-OLD RIVER NEAR MIDDLE RIVER', 'ROLD024-OLD RIVER AT BACON ISLAND',\n",
    "                   'ROLD059-OLD RIVER AT TRACY BLVD', 'RSAC064-SACRAMENTO R AT PORT CHICAGO',\n",
    "                   'RSAC075-MALLARDISLAND', 'RSAC081-COLLINSVILLE', 'RSAC092-EMMATON',\n",
    "                   'RSAC101-SACRAMENTO R AT RIO VISTA', 'RSAN007-ANTIOCH', 'RSAN018-JERSEYPOINT',\n",
    "                   'RSAN032-SACRAMENTO R AT SAN ANDREAS LANDING', 'RSAN037-SAN JOAQUIN R AT PRISONERS POINT',\n",
    "                   'RSAN058-ROUGH AND READY ISLAND', 'RSAN072-SAN JOAQUIN R AT BRANDT BRIDGE',\n",
    "                   'RSMKL008-S FORK MOKELUMNE AT TERMINOUS', 'SLCBN002-CHADBOURNE SLOUGH NR SUNRISE DUCK CLUB',\n",
    "                   'SLDUT007-DUTCH SLOUGH', 'SLMZU011-MONTEZUMA SL AT BELDONS LANDING',\n",
    "                   'SLMZU025-MONTEZUMA SL AT NATIONAL STEEL', 'SLSUS012-SUISUN SL NEAR VOLANTI SL',\n",
    "                   'SLTRM004-THREE MILE SLOUGH NR SAN JOAQUIN R', 'SSS-STEAMBOAT SL', 'CCW-MIDDLE RIVER INTAKE',\n",
    "                   'OH4-OLD R @ HWY 4', 'SLRCK005-CCWD_Rock', 'MRU-MIDDLE RIVER AT UNDINE ROAD', 'HLL-HOLLAND TRACT',\n",
    "                   'BET-PIPER SLOUGH @ BETHEL TRACT', 'GES-SACRAMENTO R BELOW GEORGIANA SLOUGH',\n",
    "                   'NMR: N FORK MOKELUMNE R NEAR WALNUT GROVE', 'IBS-CORDELIA SLOUGH @ IBIS CLUB',\n",
    "                   'GYS-GOODYEAR SLOUGH AT MORROW ISLAND CLUB', 'BKS-SLBAR002-North Bay Aqueduct/Barker Sl']\n",
    "\n",
    "output_stations, name_mapping = annutils.read_output_stations(output_stations, observed_stations_ordered_by_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mse_loss_masked def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T18:54:46.785824200Z",
     "start_time": "2023-06-27T18:54:46.781823Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mse_loss_masked(y_true, y_pred):\n",
    "    squared_diff = tf.reduce_sum(tf.math.squared_difference(y_pred[y_true > 0], y_true[y_true > 0]))\n",
    "    return squared_diff / (tf.reduce_sum(tf.cast(y_true > 0, tf.float32)) + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T18:54:50.822869600Z",
     "start_time": "2023-06-27T18:54:50.817511900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(location, df_input, output_columns):\n",
    "    model=keras.models.load_model('%s.h5'%location,custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "    xscaler,yscaler=joblib.load('%s_xyscaler.dump'%location)\n",
    "    return predict_with_model(model, xscaler, yscaler, df_input, output_columns)\n",
    "\n",
    "def predict_with_model(model, xscaler, yscaler, df_input, output_columns):\n",
    "    dfx = pd.DataFrame(xscaler.transform(df_input), df_input.index, columns=df_input.columns)\n",
    "\n",
    "    yyp=model.predict(dfx, verbose=True)\n",
    "    predicted_y = yscaler.inverse_transform(yyp)\n",
    "    return pd.DataFrame(predicted_y, index=df_input.index, columns=output_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T21:39:09.020533Z",
     "start_time": "2023-06-27T21:34:40.150531Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 4years_cal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file: D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\historical\\anninputs\\dsm2_ann_inputs_historical.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file: mtl_i118_lstm8_f_o1.h5\n",
      "Location: .\\Experiments\\4years_cal\\models\\mtl_i118_lstm8_f_o1\n",
      "Xscaler Min[0]: 4305.58333396911\n",
      "Xscaler Max[0]: 82406.1875\n",
      "362/362 [==============================] - 2s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file: mtl_i118_residual_lstm_8_2.h5\n",
      "Location: .\\Experiments\\4years_cal\\models\\mtl_i118_residual_lstm_8_2\n",
      "Xscaler Min[0]: 4305.58333396911\n",
      "Xscaler Max[0]: 82406.1875\n",
      "362/362 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.09s/it]\n",
      "100%|██████████| 1/1 [00:21<00:00, 21.80s/it]\n",
      "100%|██████████| 1/1 [00:21<00:00, 21.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiments = [\"4years_cal\"]  #\"colab\",\"6years\", \"6yearsAugmented\", \"4years\",\"4years_DCC\",\"4years_SacLag\",\"4years_SacMag\",\"colab_wo2015\"\n",
    "\n",
    "for experiment in tqdm(experiments):\n",
    "    print(\"Experiment: %s\" % experiment)\n",
    "    experiment_dir = os.path.join(local_root_path, \"Experiments\", experiment)\n",
    "\n",
    "    ndays = 118\n",
    "    window_size = 0\n",
    "    nwindows = 0\n",
    "\n",
    "    compression_opts = dict(method='zip', archive_name='out.csv')\n",
    "\n",
    "    model_dir = os.path.join(experiment_dir, \"models\")\n",
    "    model_files = [f for f in os.listdir(model_dir) if f.endswith(\".h5\")]\n",
    "\n",
    "\n",
    "    for data_file in tqdm(excel_files):\n",
    "        print(\"Data file: %s\" % data_file)\n",
    "        data_path = os.path.join(local_root_path,data_file)\n",
    "        dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "\n",
    "        dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "        dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "\n",
    "        # NEED TO RENAME THE SILLY DCC COLUMNS\n",
    "        experiment = '4years_perturbhist'\n",
    "        dcc_sub_part_f = 'DSP_PERTURBHIST_202308'\n",
    "        dcc_part_f = 'DWR-DMS-DSM2'\n",
    "        # needs to combine the DCC gate operation DSS codes into one column.\n",
    "        dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "        dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "        dfinps.columns = [s.replace('01JAN2007 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "        dfinps.columns = [s.replace('01JAN2007 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "        dfinps.columns = [s.replace('01JAN1953 - 01JAN2022', '01JAN1953 - 01JAN2020') for s in dfinps.columns] # for colab_simple\n",
    "        dfinps.columns = [s.replace('01JAN1953 - 01JAN2022', '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "        dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "        dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "\n",
    "        #get the name of the file without the extension\n",
    "        # file_name = os.path.splitext(data_file)[0]\n",
    "        file_name = os.path.splitext(os.path.basename(data_file))[0]\n",
    "\n",
    "        dirs = [\"input\", \"target\", \"prediction\"]\n",
    "        for dir in dirs:\n",
    "            os.makedirs(os.path.join(\"Experiments\", experiment, \"results\", dir), exist_ok=True)\n",
    "\n",
    "        input_file = os.path.join(\"Experiments\", experiment, \"results\", \"input\", file_name + \".csv\")\n",
    "        dfinps.to_csv(input_file, compression=compression_opts)\n",
    "\n",
    "        # read_in = pd.read_csv(input_file, compression=compression_opts, index_col=0)\n",
    "\n",
    "        target_file = os.path.join(\"Experiments\", experiment, \"results\", \"target\", file_name + \"_target.csv\")\n",
    "        dfouts.to_csv(target_file, compression=compression_opts)\n",
    "\n",
    "        for model_file in tqdm(model_files):\n",
    "            print(\"Model file: %s\" % model_file)\n",
    "            model_name = os.path.splitext(model_file)[0]\n",
    "\n",
    "            model_prediction_dir = os.path.join(experiment_dir, \"results\", \"prediction\", model_name)\n",
    "            os.makedirs(model_prediction_dir, exist_ok=True)\n",
    "\n",
    "            location = os.path.join(model_dir, model_name)\n",
    "            print(\"Location: %s\" % location)\n",
    "            #prediction = predict(location, dfinps, dfouts.columns)\n",
    "            model=keras.models.load_model('%s.h5'%location,custom_objects={\"mse_loss_masked\": mse_loss_masked})\n",
    "            xscaler,yscaler=joblib.load('%s_xyscaler.dump'%location)\n",
    "            print(\"Xscaler Min[0]: %s\" % xscaler.min_val[0])\n",
    "            print(\"Xscaler Max[0]: %s\" % xscaler.max_val[0])\n",
    "            scaled_input = xscaler.transform(dfinps)\n",
    "            dfx = pd.DataFrame(scaled_input, dfinps.index, columns=dfinps.columns)\n",
    "\n",
    "            yyp=model.predict(dfx, verbose=True)\n",
    "            predicted_y = yscaler.inverse_transform(yyp)\n",
    "            prediction = pd.DataFrame(predicted_y, index=dfinps.index, columns=dfouts.columns)\n",
    "            prediction_file = os.path.join(model_prediction_dir, file_name + \".csv\")\n",
    "            prediction.to_csv(prediction_file, compression=compression_opts)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
