{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T19:08:50.721848600Z",
     "start_time": "2023-06-27T19:08:50.707849500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "tf.config.list_physical_devices('GPU')\n",
    "local_root_path = \".\"\n",
    "sys.path.append(local_root_path)\n",
    "import annutils\n",
    "import importlib\n",
    "importlib.reload(annutils)\n",
    "dsp_home = \"F:/projects/ann_dsp\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T19:09:04.144975100Z",
     "start_time": "2023-06-27T19:09:04.136366Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHDMC006', 'CHSWP003', 'CHVCT000', 'OLD_MID', 'ROLD024', 'ROLD059', 'RSAC064', 'RSAC075', 'RSAC081', 'RSAC092', 'RSAN007', 'RSAN018', 'RSAN032', 'RSAN037', 'RSAN058', 'RSAN072', 'RSMKL008', 'SLCBN002', 'SLDUT007', 'SLMZU011', 'SLMZU025', 'SLSUS012', 'SLTRM004']\n",
      "{'CHDMC006-CVP INTAKE': 'CHDMC006', 'CHSWP003-CCFB_INTAKE': 'CHSWP003', 'CHVCT000-VICTORIA INTAKE': 'CHVCT000', 'OLD_MID-OLD RIVER NEAR MIDDLE RIVER': 'OLD_MID', 'ROLD024-OLD RIVER AT BACON ISLAND': 'ROLD024', 'ROLD059-OLD RIVER AT TRACY BLVD': 'ROLD059', 'RSAC064-SACRAMENTO R AT PORT CHICAGO': 'RSAC064', 'RSAC075-MALLARDISLAND': 'RSAC075', 'RSAC081-COLLINSVILLE': 'RSAC081', 'RSAC092-EMMATON': 'RSAC092', 'RSAN007-ANTIOCH': 'RSAN007', 'RSAN018-JERSEYPOINT': 'RSAN018', 'RSAN032-SACRAMENTO R AT SAN ANDREAS LANDING': 'RSAN032', 'RSAN037-SAN JOAQUIN R AT PRISONERS POINT': 'RSAN037', 'RSAN058-ROUGH AND READY ISLAND': 'RSAN058', 'RSAN072-SAN JOAQUIN R AT BRANDT BRIDGE': 'RSAN072', 'RSMKL008-S FORK MOKELUMNE AT TERMINOUS': 'RSMKL008', 'SLCBN002-CHADBOURNE SLOUGH NR SUNRISE DUCK CLUB': 'SLCBN002', 'SLDUT007-DUTCH SLOUGH': 'SLDUT007', 'SLMZU011-MONTEZUMA SL AT BELDONS LANDING': 'SLMZU011', 'SLMZU025-MONTEZUMA SL AT NATIONAL STEEL': 'SLMZU025', 'SLSUS012-SUISUN SL NEAR VOLANTI SL': 'SLSUS012', 'SLTRM004-THREE MILE SLOUGH NR SAN JOAQUIN R': 'SLTRM004'}\n"
     ]
    }
   ],
   "source": [
    "compression_opts = dict(method='zip', archive_name='out.csv')\n",
    "#compression_opts = None\n",
    "\n",
    "\n",
    "# Make a dir named Experiments\n",
    "if not os.path.exists(\"Experiments\"):\n",
    "    os.mkdir(\"Experiments\")\n",
    "\n",
    "num_sheets = 9\n",
    "\n",
    "observed_stations_ordered_by_median = ['RSMKL008', 'RSAN032', 'RSAN037', 'RSAC092', 'SLTRM004', 'ROLD024',\n",
    "                                       'CHVCT000', 'RSAN018', 'CHSWP003', 'CHDMC006', 'SLDUT007', 'RSAN072',\n",
    "                                       'OLD_MID', 'RSAN058', 'ROLD059', 'RSAN007', 'RSAC081', 'SLMZU025',\n",
    "                                       'RSAC075', 'SLMZU011', 'SLSUS012', 'SLCBN002', 'RSAC064']\n",
    "\n",
    "output_stations = ['CHDMC006-CVP INTAKE', 'CHSWP003-CCFB_INTAKE', 'CHVCT000-VICTORIA INTAKE',\n",
    "                   'OLD_MID-OLD RIVER NEAR MIDDLE RIVER', 'ROLD024-OLD RIVER AT BACON ISLAND',\n",
    "                   'ROLD059-OLD RIVER AT TRACY BLVD', 'RSAC064-SACRAMENTO R AT PORT CHICAGO',\n",
    "                   'RSAC075-MALLARDISLAND', 'RSAC081-COLLINSVILLE', 'RSAC092-EMMATON',\n",
    "                   'RSAC101-SACRAMENTO R AT RIO VISTA', 'RSAN007-ANTIOCH', 'RSAN018-JERSEYPOINT',\n",
    "                   'RSAN032-SACRAMENTO R AT SAN ANDREAS LANDING', 'RSAN037-SAN JOAQUIN R AT PRISONERS POINT',\n",
    "                   'RSAN058-ROUGH AND READY ISLAND', 'RSAN072-SAN JOAQUIN R AT BRANDT BRIDGE',\n",
    "                   'RSMKL008-S FORK MOKELUMNE AT TERMINOUS', 'SLCBN002-CHADBOURNE SLOUGH NR SUNRISE DUCK CLUB',\n",
    "                   'SLDUT007-DUTCH SLOUGH', 'SLMZU011-MONTEZUMA SL AT BELDONS LANDING',\n",
    "                   'SLMZU025-MONTEZUMA SL AT NATIONAL STEEL', 'SLSUS012-SUISUN SL NEAR VOLANTI SL',\n",
    "                   'SLTRM004-THREE MILE SLOUGH NR SAN JOAQUIN R', 'SSS-STEAMBOAT SL', 'CCW-MIDDLE RIVER INTAKE',\n",
    "                   'OH4-OLD R @ HWY 4', 'SLRCK005-CCWD_Rock', 'MRU-MIDDLE RIVER AT UNDINE ROAD', 'HLL-HOLLAND TRACT',\n",
    "                   'BET-PIPER SLOUGH @ BETHEL TRACT', 'GES-SACRAMENTO R BELOW GEORGIANA SLOUGH',\n",
    "                   'NMR: N FORK MOKELUMNE R NEAR WALNUT GROVE', 'IBS-CORDELIA SLOUGH @ IBIS CLUB',\n",
    "                   'GYS-GOODYEAR SLOUGH AT MORROW ISLAND CLUB', 'BKS-SLBAR002-North Bay Aqueduct/Barker Sl']\n",
    "\n",
    "output_stations, name_mapping = annutils.read_output_stations(output_stations, observed_stations_ordered_by_median)\n",
    "print(output_stations)\n",
    "print(name_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Experiment:6 Years\n",
    "This is just the base data for the 6 selected years.\n",
    " If we do this right the training data should be approx 6 * 365 =~ 2190 rows of training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T21:54:20.627479800Z",
     "start_time": "2023-06-20T21:54:20.583475800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dir named 6years\n",
    "experiment_name = \"6years\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2008-10-1','2009-9-30'),\n",
    "    ('2010-10-1','2011-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30'),\n",
    "    ('2016-10-1','2017-9-30')\n",
    "]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_historical.xlsx\"]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in input_files:\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    if not os.path.exists(data_path):\n",
    "        raise ValueError(f\"data_path {datapath} does not exist\")\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 8 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T21:54:29.021533900Z",
     "start_time": "2023-06-20T21:54:20.630477200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "df_plus = annutils.create_antecedent_inputs(X_df,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "# df_plus should now have 118 * 8 = 944 input features\n",
    "\n",
    "# synchronize trims off the na values so the row numbers go from 10920 to 10803\n",
    "df_X2, df_Y2 = annutils.synchronize(df_plus, Y_df)\n",
    "\n",
    "train_X = annutils.include(df_X2, picked_training_years)\n",
    "train_Y = annutils.include(df_Y2, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(df_X2, picked_training_years)\n",
    "test_Y = annutils.exclude(df_Y2, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years\n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compiling inputs for 4years_DCC experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years\n",
    "# experiment_names = [\"4years\",\"4years_DCC\",\"4years_SacLag\",\"4years_SacMag\"]\n",
    "experiment_names = [\"4years_DCC\"]\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "\n",
    "input_files_dict = {\"4years\":[\"dsm2_ann_inputs_historical.xlsx\"],\n",
    "                    \"4years_DCC\":[\"dsm2_ann_inputs_historical.xlsx\",\n",
    "                                  \"dsm2_ann_inputs_dcc0.xlsx\",\n",
    "                                  \"dsm2_ann_inputs_dcc1.xlsx\"],\n",
    "                                #   \"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "                                #   \"dsm2_ann_inputs_rsacminus20pct.xlsx\"],\n",
    "                    \"4years_SacLag\":[\"dsm2_ann_inputs_historical.xlsx\",\n",
    "                                     \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "                                     \"dsm2_ann_inputs_rsanminus15day.xlsx\"],\n",
    "                    \"4years_SacMag\":[\"dsm2_ann_inputs_historical.xlsx\",\n",
    "                                     \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "                                     \"dsm2_ann_inputs_rsanminus20pct.xlsx\"]\n",
    "}\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "for experiment_name in experiment_names:\n",
    "    if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "        os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "    X_df= None\n",
    "    Y_df= None\n",
    "\n",
    "    input_files = input_files_dict[experiment_name]\n",
    "\n",
    "    for data_file in tqdm(input_files):\n",
    "        data_path = data_file #os.path.join(local_root_path,data_file)\n",
    "        dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "        dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "        dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "        X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "        Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "    # now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "    train_X = annutils.include(X_df, picked_training_years)\n",
    "    train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "    test_X = annutils.exclude(X_df, picked_training_years)\n",
    "    test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "    train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "    train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "    test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "    test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "    \n",
    "    print(f\"Finished compiling inputs for {experiment_name} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years (calendar years) \n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data and using jan 1 - dec 31 instead of water year limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:26<00:00, 26.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compiling inputs for 4years_cal experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_cal\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2008-1-1','2008-12-31'),\n",
    "    ('2010-1-1','2010-12-31'),\n",
    "    ('2012-1-1','2012-12-31'),\n",
    "    ('2014-1-1','2014-12-31')\n",
    "]\n",
    "\n",
    "picked_training_years = [\n",
    "                         ('2007-2-1','2008-3-31'),\n",
    "                         ('2010-01-1','2012-1-1'),\n",
    "                         ('2012-01-1','2012-12-31'),  # reduced\n",
    "                         ('2014-01-1','2015-5-31')]  # reduced\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_historical.xlsx\"]\n",
    "\n",
    "experiment = '4years_cal'\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years (calendar years) plus augmented data\n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data and using jan 1 - dec 31 instead of water year limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\projects\\\\delta_salinity\\\\model\\\\dsm2\\\\DSP_DSM2_202307\\\\modified_bc\\\\anninputs\\\\hiexplonf2\\\\dsm2_ann_inputs_hiexplonf2.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\projects\\ann_dsp\\scripts\\rma_ann_repo\\CreateDatasets.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/projects/ann_dsp/scripts/rma_ann_repo/CreateDatasets.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m data_file \u001b[39min\u001b[39;00m tqdm(aug_input_files):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/projects/ann_dsp/scripts/rma_ann_repo/CreateDatasets.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     data_path \u001b[39m=\u001b[39m data_file\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/projects/ann_dsp/scripts/rma_ann_repo/CreateDatasets.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     dfinps, dfouts \u001b[39m=\u001b[39m annutils\u001b[39m.\u001b[39;49mread_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/projects/ann_dsp/scripts/rma_ann_repo/CreateDatasets.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     dfinps \u001b[39m=\u001b[39m annutils\u001b[39m.\u001b[39mcreate_antecedent_inputs(dfinps,ndays\u001b[39m=\u001b[39mndays,window_size\u001b[39m=\u001b[39mwindow_size,nwindows\u001b[39m=\u001b[39mnwindows)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/projects/ann_dsp/scripts/rma_ann_repo/CreateDatasets.ipynb#X14sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     dfinps, dfouts \u001b[39m=\u001b[39m annutils\u001b[39m.\u001b[39msynchronize(dfinps, dfouts)\n",
      "File \u001b[1;32mf:\\projects\\ann_dsp\\scripts\\rma_ann_repo\\annutils.py:473\u001b[0m, in \u001b[0;36mread_and_split\u001b[1;34m(data_path, num_sheets, observed_stations)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_and_split\u001b[39m(data_path, num_sheets, observed_stations):\n\u001b[1;32m--> 473\u001b[0m     dflist \u001b[39m=\u001b[39m [read_excel_sheet(data_path, i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_sheets)]\n\u001b[0;32m    475\u001b[0m     df_inputs_and_outputs \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dflist[\u001b[39m0\u001b[39m:num_sheets], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdropna(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    476\u001b[0m     last_sheet_columns \u001b[39m=\u001b[39m dflist[num_sheets \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcolumns\n",
      "File \u001b[1;32mf:\\projects\\ann_dsp\\scripts\\rma_ann_repo\\annutils.py:473\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_and_split\u001b[39m(data_path, num_sheets, observed_stations):\n\u001b[1;32m--> 473\u001b[0m     dflist \u001b[39m=\u001b[39m [read_excel_sheet(data_path, i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_sheets)]\n\u001b[0;32m    475\u001b[0m     df_inputs_and_outputs \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dflist[\u001b[39m0\u001b[39m:num_sheets], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdropna(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    476\u001b[0m     last_sheet_columns \u001b[39m=\u001b[39m dflist[num_sheets \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcolumns\n",
      "File \u001b[1;32mf:\\projects\\ann_dsp\\scripts\\rma_ann_repo\\annutils.py:442\u001b[0m, in \u001b[0;36mread_excel_sheet\u001b[1;34m(data_path, sheet_index)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mread_pickle(pickle_path)\n\u001b[0;32m    441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m     retval \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(data_path, sheet_index, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, parse_dates\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    443\u001b[0m     \u001b[39m# create cache folder if it doesn't exist\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(pickle_path)):\n",
      "File \u001b[1;32mc:\\Users\\eli\\Miniconda3\\envs\\anntraining\\lib\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    479\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\eli\\Miniconda3\\envs\\anntraining\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1496\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1497\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m   1498\u001b[0m     )\n\u001b[0;32m   1499\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1501\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1502\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1503\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\eli\\Miniconda3\\envs\\anntraining\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1369\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1371\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1372\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1373\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1374\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1375\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eli\\Miniconda3\\envs\\anntraining\\lib\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    869\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    871\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\projects\\\\delta_salinity\\\\model\\\\dsm2\\\\DSP_DSM2_202307\\\\modified_bc\\\\anninputs\\\\hiexplonf2\\\\dsm2_ann_inputs_hiexplonf2.xlsx'"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_cal\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2008-1-1','2008-12-31'),\n",
    "    ('2010-1-1','2010-12-31'),\n",
    "    ('2012-1-1','2012-12-31'),\n",
    "    ('2014-1-1','2014-12-31')\n",
    "]\n",
    "\n",
    "aug_data = [('2014-1-1','2014-12-31')]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_historical.xlsx\"]\n",
    "aug_input_files = [r\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\modified_bc\\anninputs\\hiexplonf2\\dsm2_ann_inputs_hiexplonf2.xlsx\"]\n",
    "\n",
    "experiment = '4years_cal_hiexplonf2'\n",
    "dcc_sub_part_f = 'DSP_HIEXPLONF2_202308'\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "# Add augmented data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(aug_input_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "x_aug_train = annutils.include(X_df, aug_data)\n",
    "y_aug_train = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# needs to combine the DCC gate operation DSS codes into one column.\n",
    "x_aug_train.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in x_aug_train.columns]\n",
    "y_aug_train.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in y_aug_train.columns]\n",
    "x_aug_train.columns = [s.replace('01JAN2013 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in x_aug_train.columns]\n",
    "y_aug_train.columns = [s.replace('01JAN2013 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in y_aug_train.columns]\n",
    "x_aug_train.columns = [s.replace('1DAY', 'IR-YEAR') for s in x_aug_train.columns]\n",
    "y_aug_train.columns = [s.replace('1DAY', 'IR-YEAR') for s in y_aug_train.columns]\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(x_aug_train.columns))}')\n",
    "\n",
    "train_X = pd.concat([train_X, x_aug_train], axis=0)\n",
    "train_Y = pd.concat([train_Y, y_aug_train], axis=0)\n",
    "\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years (calendar years) but perturbed\n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data and using jan 1 - dec 31 instead of water year limits\n",
    "But the input data to DSM2 has been perturbed from the baseline using perturbhist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for 4years_perturbhist experiment\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_aug_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m test_Y \u001b[39m=\u001b[39m annutils\u001b[39m.\u001b[39minclude(Y_df, test_data)\n\u001b[0;32m     54\u001b[0m \u001b[39m# needs to combine the DCC gate operation DSS codes into one column.\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m x_aug_train\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39mreplace(dcc_sub_part_f, dcc_part_f) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m x_aug_train\u001b[39m.\u001b[39mcolumns]\n\u001b[0;32m     56\u001b[0m y_aug_train\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39mreplace(dcc_sub_part_f, dcc_part_f) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m y_aug_train\u001b[39m.\u001b[39mcolumns]\n\u001b[0;32m     57\u001b[0m x_aug_train\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m01JAN2013 - 01JAN2014\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m01JAN1953 - 01JAN2020\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m x_aug_train\u001b[39m.\u001b[39mcolumns]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_aug_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "\n",
    "aug_data = [('2008-1-1','2008-12-31'),\n",
    "    ('2010-1-1','2010-12-31'),\n",
    "    ('2012-1-1','2012-12-31'),\n",
    "    ('2014-1-1','2014-12-31')]\n",
    "\n",
    "aug_input_files = [r\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\modified_bc\\anninputs\\perturbhist\\dsm2_ann_inputs_perturbhist.xlsx\"]\n",
    "\n",
    "test_files = [\"dsm2_ann_inputs_historical.xlsx\"]\n",
    "test_data = aug_data\n",
    "\n",
    "experiment = '4years_perturbhist'\n",
    "dcc_sub_part_f = 'DSP_PERTURBHIST_202308'\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "# Add augmented data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(aug_input_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, aug_data)\n",
    "train_Y = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = annutils.include(X_df, test_data)\n",
    "test_Y = annutils.include(Y_df, test_data)\n",
    "\n",
    "# needs to combine the DCC gate operation DSS codes into one column.\n",
    "train_X.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in train_X.columns]\n",
    "train_Y.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in train_Y.columns]\n",
    "train_X.columns = [s.replace('01JAN2007 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in train_X.columns]\n",
    "train_Y.columns = [s.replace('01JAN2007 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in train_Y.columns]\n",
    "train_X.columns = [s.replace('1DAY', 'IR-YEAR') for s in train_X.columns]\n",
    "train_Y.columns = [s.replace('1DAY', 'IR-YEAR') for s in train_Y.columns]\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years plus augmented data\n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data and the augmented inputs/outputs from DSM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dir named 4years\n",
    "# experiment_names = [\"4years\",\"4years_DCC\",\"4years_SacLag\",\"4years_SacMag\"]\n",
    "experiment_names = [\"4years_hiexplonf\"]\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "\n",
    "aug_data = [('2013-11-1','2014-6-1')]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_historical.xlsx\"]\n",
    "aug_input_files = [r\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\modified_bc\\anninputs\\hiexplonf\\dsm2_ann_inputs_hiexplonf.xlsx\"]\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "experiment_name = '4years_hiexplonf'\n",
    "    \n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "# Add augmented data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(aug_input_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "x_aug_train = annutils.include(X_df, aug_data)\n",
    "y_aug_train = annutils.include(Y_df, aug_data)\n",
    "\n",
    "train_X = pd.concat([train_X, x_aug_train], axis=0)\n",
    "train_Y = pd.concat([train_Y, y_aug_train], axis=0)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment_name} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Perturbed Latinhypercube\n",
    "\n",
    "This has 7 years using 2008, 2010, 2012, 2014 with variations of perturbed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_split 1990-01-01 00:00:00\n",
      "read_split 2013-09-03 00:00:00\n",
      "read_split 2009-09-03 00:00:00\n",
      "read_split 2006-09-03 00:00:00\n",
      "read_split 2013-09-03 00:00:00\n",
      "read_split 2011-09-03 00:00:00\n",
      "read_split 2009-09-03 00:00:00\n",
      "read_split 2006-09-03 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:27<00:00, 27.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcc_op_lag1   dcc_op_lag1\n",
      "dcc_op_lag2   dcc_op_lag2\n",
      "dcc_op_lag3   dcc_op_lag3\n",
      "dcc_op_lag4   dcc_op_lag4\n",
      "dcc_op_lag5   dcc_op_lag5\n",
      "dcc_op_lag6   dcc_op_lag6\n",
      "dcc_op_lag7   dcc_op_lag7\n",
      "dcc_op_lag8   dcc_op_lag8\n",
      "dcc_op_lag9   dcc_op_lag9\n",
      "dcc_op_lag10   dcc_op_lag10\n",
      "dcc_op_lag11   dcc_op_lag11\n",
      "dcc_op_lag12   dcc_op_lag12\n",
      "dcc_op_lag13   dcc_op_lag13\n",
      "dcc_op_lag14   dcc_op_lag14\n",
      "dcc_op_lag15   dcc_op_lag15\n",
      "dcc_op_lag16   dcc_op_lag16\n",
      "dcc_op_lag17   dcc_op_lag17\n",
      "dcc_op_lag18   dcc_op_lag18\n",
      "dcc_op_lag19   dcc_op_lag19\n",
      "dcc_op_lag20   dcc_op_lag20\n",
      "dcc_op_lag21   dcc_op_lag21\n",
      "dcc_op_lag22   dcc_op_lag22\n",
      "dcc_op_lag23   dcc_op_lag23\n",
      "dcc_op_lag24   dcc_op_lag24\n",
      "dcc_op_lag25   dcc_op_lag25\n",
      "dcc_op_lag26   dcc_op_lag26\n",
      "dcc_op_lag27   dcc_op_lag27\n",
      "dcc_op_lag28   dcc_op_lag28\n",
      "dcc_op_lag29   dcc_op_lag29\n",
      "dcc_op_lag30   dcc_op_lag30\n",
      "dcc_op_lag31   dcc_op_lag31\n",
      "dcc_op_lag32   dcc_op_lag32\n",
      "dcc_op_lag33   dcc_op_lag33\n",
      "dcc_op_lag34   dcc_op_lag34\n",
      "dcc_op_lag35   dcc_op_lag35\n",
      "dcc_op_lag36   dcc_op_lag36\n",
      "dcc_op_lag37   dcc_op_lag37\n",
      "dcc_op_lag38   dcc_op_lag38\n",
      "dcc_op_lag39   dcc_op_lag39\n",
      "dcc_op_lag40   dcc_op_lag40\n",
      "dcc_op_lag41   dcc_op_lag41\n",
      "dcc_op_lag42   dcc_op_lag42\n",
      "dcc_op_lag43   dcc_op_lag43\n",
      "dcc_op_lag44   dcc_op_lag44\n",
      "dcc_op_lag45   dcc_op_lag45\n",
      "dcc_op_lag46   dcc_op_lag46\n",
      "dcc_op_lag47   dcc_op_lag47\n",
      "dcc_op_lag48   dcc_op_lag48\n",
      "dcc_op_lag49   dcc_op_lag49\n",
      "dcc_op_lag50   dcc_op_lag50\n",
      "dcc_op_lag51   dcc_op_lag51\n",
      "dcc_op_lag52   dcc_op_lag52\n",
      "dcc_op_lag53   dcc_op_lag53\n",
      "dcc_op_lag54   dcc_op_lag54\n",
      "dcc_op_lag55   dcc_op_lag55\n",
      "dcc_op_lag56   dcc_op_lag56\n",
      "dcc_op_lag57   dcc_op_lag57\n",
      "dcc_op_lag58   dcc_op_lag58\n",
      "dcc_op_lag59   dcc_op_lag59\n",
      "dcc_op_lag60   dcc_op_lag60\n",
      "dcc_op_lag61   dcc_op_lag61\n",
      "dcc_op_lag62   dcc_op_lag62\n",
      "dcc_op_lag63   dcc_op_lag63\n",
      "dcc_op_lag64   dcc_op_lag64\n",
      "dcc_op_lag65   dcc_op_lag65\n",
      "dcc_op_lag66   dcc_op_lag66\n",
      "dcc_op_lag67   dcc_op_lag67\n",
      "dcc_op_lag68   dcc_op_lag68\n",
      "dcc_op_lag69   dcc_op_lag69\n",
      "dcc_op_lag70   dcc_op_lag70\n",
      "dcc_op_lag71   dcc_op_lag71\n",
      "dcc_op_lag72   dcc_op_lag72\n",
      "dcc_op_lag73   dcc_op_lag73\n",
      "dcc_op_lag74   dcc_op_lag74\n",
      "dcc_op_lag75   dcc_op_lag75\n",
      "dcc_op_lag76   dcc_op_lag76\n",
      "dcc_op_lag77   dcc_op_lag77\n",
      "dcc_op_lag78   dcc_op_lag78\n",
      "dcc_op_lag79   dcc_op_lag79\n",
      "dcc_op_lag80   dcc_op_lag80\n",
      "dcc_op_lag81   dcc_op_lag81\n",
      "dcc_op_lag82   dcc_op_lag82\n",
      "dcc_op_lag83   dcc_op_lag83\n",
      "dcc_op_lag84   dcc_op_lag84\n",
      "dcc_op_lag85   dcc_op_lag85\n",
      "dcc_op_lag86   dcc_op_lag86\n",
      "dcc_op_lag87   dcc_op_lag87\n",
      "dcc_op_lag88   dcc_op_lag88\n",
      "dcc_op_lag89   dcc_op_lag89\n",
      "dcc_op_lag90   dcc_op_lag90\n",
      "dcc_op_lag91   dcc_op_lag91\n",
      "dcc_op_lag92   dcc_op_lag92\n",
      "dcc_op_lag93   dcc_op_lag93\n",
      "dcc_op_lag94   dcc_op_lag94\n",
      "dcc_op_lag95   dcc_op_lag95\n",
      "dcc_op_lag96   dcc_op_lag96\n",
      "dcc_op_lag97   dcc_op_lag97\n",
      "dcc_op_lag98   dcc_op_lag98\n",
      "dcc_op_lag99   dcc_op_lag99\n",
      "dcc_op_lag100   dcc_op_lag100\n",
      "dcc_op_lag101   dcc_op_lag101\n",
      "dcc_op_lag102   dcc_op_lag102\n",
      "dcc_op_lag103   dcc_op_lag103\n",
      "dcc_op_lag104   dcc_op_lag104\n",
      "dcc_op_lag105   dcc_op_lag105\n",
      "dcc_op_lag106   dcc_op_lag106\n",
      "dcc_op_lag107   dcc_op_lag107\n",
      "dcc_op_lag108   dcc_op_lag108\n",
      "dcc_op_lag109   dcc_op_lag109\n",
      "dcc_op_lag110   dcc_op_lag110\n",
      "dcc_op_lag111   dcc_op_lag111\n",
      "dcc_op_lag112   dcc_op_lag112\n",
      "dcc_op_lag113   dcc_op_lag113\n",
      "dcc_op_lag114   dcc_op_lag114\n",
      "dcc_op_lag115   dcc_op_lag115\n",
      "dcc_op_lag116   dcc_op_lag116\n",
      "dcc_op_lag117   dcc_op_lag117\n",
      "dcc_op_lag118   dcc_op_lag118\n",
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for latinhypercube_7 experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "# Eli go here\n",
    "experiment = 'latinhypercube_7'\n",
    "\n",
    "# Eli should be historical\n",
    "test_files = [\"dsm2_ann_inputs_historical.xlsx\"]\n",
    "test_files = [r\"F:\\projects\\ann_dsp\\model\\dsm2\\DSP_DSM2_202307\\latinhypercube_7\\anninputs\\lathypcub_0\\dsm2_ann_inputs_lathypcub_0.xlsx\"]\n",
    "test_files = [r\"F:\\projects\\ann_dsp\\model\\dsm2\\DSP_DSM2_202307\\historical\\anninputs\\dsm2_ann_inputs_historical.xlsx\"]\n",
    "test_data = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "numbers = { 0: 1997,\n",
    "            1: 2014,\n",
    "            2: 2010,\n",
    "            3: 2008,\n",
    "            4: 2014,\n",
    "            5: 2012,\n",
    "            6: 2010,\n",
    "            7: 2008\n",
    "}\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "#picked_training_years = [\n",
    "#                         ('2007-2-1','2008-8-31'),\n",
    "#                         ('2010-01-1','2012-1-1'),\n",
    "#                         ('2012-01-1','2012-12-31'),  # reduced\n",
    "#                         ('2014-01-1','2015-5-31')]  # reduced\n",
    "# runyear is nominal. This gives the specifics\n",
    "aug_windows = { 1997: ('1997-5-1','1997-9-1'),\n",
    "               2008: ('2007-2-1','2008-8-31'),\n",
    "               2010: ('2010-01-1','2011-12-31'),\n",
    "               2012: ('2012-01-1','2012-12-31'),\n",
    "               2014: ('2014-01-1','2015-5-31')\n",
    "               }\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "for case_num, yearran in numbers.items():\n",
    "\n",
    "    #aug_data = [(f'{str(yearran)}-1-1',f'{str(yearran)}-12-31')]\n",
    "    aug_data = [aug_windows[yearran]]\n",
    "    \n",
    "    # Eli\n",
    "    aug_input_file = fr\"{dsp_home}\\model\\dsm2\\DSP_DSM2_202307\\latinhypercube_7\\anninputs\\lathypcub_{str(case_num)}\\dsm2_ann_inputs_lathypcub_{str(case_num)}.xlsx\"\n",
    "\n",
    "    if case_num in (0,3,7):\n",
    "        gate_per = '01JAN1953 - 01JAN2022'\n",
    "        dcc_sub_part_f = 'DWR-DMS-DSM2'\n",
    "    else:\n",
    "        gate_per = f'01JAN{str(yearran-1)} - 01JAN{str(yearran)}'\n",
    "        dcc_sub_part_f = f'DSP_LATHYPCUB_{str(case_num)}_202309'\n",
    "\n",
    "    dfinps, dfouts = annutils.read_and_split(aug_input_file, num_sheets, observed_stations_ordered_by_median)\n",
    "    print(\"read_split\",dfinps.first_valid_index())\n",
    "    # This is where indexiing may cause issue\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "\n",
    "\n",
    "    # needs to combine the DCC gate operation DSS codes into one column.\n",
    "    dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfouts.columns]\n",
    "    \n",
    "    dfinps = annutils.include(dfinps, aug_data)\n",
    "    dfouts = annutils.include(dfouts, aug_data)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = X_df #annutils.include(X_df, aug_data)\n",
    "train_Y = Y_df # annY_df #annutils.include(dfouts, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file #os.path.join(local_root_path,'Experiments',data_file)\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"local root: \",local_root_path)\n",
    "        raise ValueError(f\"Data path {data_path} does not exist\")\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    dfinps = annutils.include(dfinps, test_data)\n",
    "    dfouts = annutils.include(dfouts, test_data)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = X_df #annutils.include(X_df, test_data)\n",
    "test_Y = Y_df #annutils.include(Y_df, test_data)\n",
    "\n",
    "xx= list(train_X.columns.copy())\n",
    "yy = list(test_X.columns.copy())\n",
    "#xx.sort()\n",
    "#yy.sort()\n",
    "for xxx,yyy in zip(xx,yy): \n",
    "    if \"dcc\" in xxx or \"dcc\" in yyy or \"128\" in xxx or \"128\" in yyy: \n",
    "        print(xxx,\" \",yyy)\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts,float_format=\"%.2f\")\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts,float_format=\"%.2f\")\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts,float_format=\"%.2f\")\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts,float_format=\"%.2f\")\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Perturbed Latinhypercube with tidal shift\n",
    "\n",
    "This has 7 years using 2008, 2010, 2012, 2014 with variations of perturbed data but always has a perturbed tidal cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for lathypcub_tideshift experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "experiment = 'lathypcub_tideshift'\n",
    "\n",
    "test_files = [\"dsm2_ann_inputs_historical.xlsx\"]\n",
    "test_data = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "numbers = {1: 2014,\n",
    "            2: 2010,\n",
    "            3: 2008,\n",
    "            4: 2014,\n",
    "            5: 2012,\n",
    "            6: 2010,\n",
    "            7: 2008\n",
    "}\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "for case_num, yearran in numbers.items():\n",
    "\n",
    "    aug_data = [(f'{str(yearran)}-1-1',f'{str(yearran)}-12-31')]\n",
    "    \n",
    "    aug_input_file = fr\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\lathypcub_tideshift\\anninputs\\lathypcub_tideshift_{str(case_num)}\\dsm2_ann_inputs_lathypcub_tideshift_{str(case_num)}.xlsx\"\n",
    "\n",
    "    if case_num == 3 or case_num == 7:\n",
    "        gate_per = '01JAN1953 - 01JAN2022'\n",
    "        dcc_sub_part_f = 'DWR-DMS-DSM2'\n",
    "    else:\n",
    "        gate_per = f'01JAN{str(yearran-1)} - 01JAN{str(yearran)}'\n",
    "        dcc_sub_part_f = f'DSP_LATHYPCUB_{str(case_num)}_202309'\n",
    "\n",
    "    dfinps, dfouts = annutils.read_and_split(aug_input_file, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    \n",
    "    # needs to combine the DCC gate operation DSS codes into one column.\n",
    "    dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfouts.columns]\n",
    "    \n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, aug_data)\n",
    "train_Y = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = annutils.include(X_df, test_data)\n",
    "test_Y = annutils.include(Y_df, test_data)\n",
    "\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Perturbed Latinhypercube no tidal shift\n",
    "\n",
    "This has 7 years using 2008, 2010, 2012, 2014 with variations of perturbed data but never has a perturbed tidal cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:27<00:00, 27.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 236\n",
      "Finished compiling inputs for lathypcub_regtide experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "experiment = 'lathypcub_regtide'\n",
    "\n",
    "test_files = [\"dsm2_ann_inputs_historical.xlsx\"]\n",
    "#test_files = [r\"F:\\projects\\ann_dsp\\model\\dsm2\\DSP_DSM2_202307\\historical\\anninputs\\dsm2_ann_inputs_historical.xlsx\"]\n",
    "test_data = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "numbers = {1: 2014,\n",
    "            2: 2010,\n",
    "            3: 2008,\n",
    "            4: 2014,\n",
    "            5: 2012,\n",
    "            6: 2010,\n",
    "            7: 2008\n",
    "}\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "for case_num, yearran in numbers.items():\n",
    "\n",
    "    aug_data = [(f'{str(yearran)}-1-1',f'{str(yearran)}-12-31')]\n",
    "    \n",
    "    aug_input_file = fr\"{dsp_home}\\model\\dsm2\\DSP_DSM2_202307\\lathypcub_regtide\\anninputs\\lathypcub_regtide_{str(case_num)}\\dsm2_ann_inputs_lathypcub_regtide_{str(case_num)}.xlsx\"\n",
    "\n",
    "    if case_num == 3 or case_num == 7:\n",
    "        gate_per = '01JAN1953 - 01JAN2022'\n",
    "        dcc_sub_part_f = 'DWR-DMS-DSM2'\n",
    "    else:\n",
    "        gate_per = f'01JAN{str(yearran-1)} - 01JAN{str(yearran)}'\n",
    "        dcc_sub_part_f = f'DSP_LATHYPCUB_{str(case_num)}_202309'\n",
    "\n",
    "    dfinps, dfouts = annutils.read_and_split(aug_input_file, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    \n",
    "    # needs to combine the DCC gate operation DSS codes into one column.\n",
    "    dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfouts.columns]\n",
    "    \n",
    "    dfinps = annutils.include(dfinps, aug_data)\n",
    "    dfouts = annutils.include(dfouts, aug_data)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = X_df #annutils.include(X_df, aug_data)\n",
    "train_Y = Y_df # annY_df #annutils.include(dfouts, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = annutils.include(X_df, test_data)\n",
    "test_Y = annutils.include(Y_df, test_data)\n",
    "\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Perturbed Latinhypercube 7 day tidal shift\n",
    "\n",
    "This has 7 years using 2008, 2010, 2012, 2014 with variations of perturbed data but always has a 7 day tidal shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for lathypcub_7tide experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "experiment = 'lathypcub_7tide'\n",
    "\n",
    "test_files = [\"dsm2_ann_inputs_historical.xlsx\"]\n",
    "test_data = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "numbers = {1: 2014,\n",
    "            2: 2010,\n",
    "            3: 2008,\n",
    "            4: 2014,\n",
    "            5: 2012,\n",
    "            6: 2010,\n",
    "            7: 2008\n",
    "}\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "for case_num, yearran in numbers.items():\n",
    "\n",
    "    aug_data = [(f'{str(yearran)}-1-1',f'{str(yearran)}-12-31')]\n",
    "    \n",
    "    aug_input_file = fr\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\lathypcub_7tide\\anninputs\\lathypcub_7tide_{str(case_num)}\\dsm2_ann_inputs_lathypcub_7tide_{str(case_num)}.xlsx\"\n",
    "\n",
    "    if case_num == 3 or case_num == 7:\n",
    "        gate_per = '01JAN1953 - 01JAN2022'\n",
    "        dcc_sub_part_f = 'DWR-DMS-DSM2'\n",
    "    else:\n",
    "        gate_per = f'01JAN{str(yearran-1)} - 01JAN{str(yearran)}'\n",
    "        dcc_sub_part_f = f'DSP_LATHYPCUB_{str(case_num)}_202309'\n",
    "\n",
    "    dfinps, dfouts = annutils.read_and_split(aug_input_file, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    \n",
    "    # needs to combine the DCC gate operation DSS codes into one column.\n",
    "    dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfouts.columns]\n",
    "    \n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, aug_data)\n",
    "train_Y = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = annutils.include(X_df, test_data)\n",
    "test_Y = annutils.include(Y_df, test_data)\n",
    "\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiment: 6 Years with Augmented data\n",
    "This is the same as the previous experiment but adds in the augmented data.\n",
    "The augmented data:\n",
    "    sac + 15 days\n",
    "    sac - 15 days\n",
    "    sjr + 15 days\n",
    "    sjr - 15 days\n",
    "    sac + 20%\n",
    "    sac - 20%\n",
    "    sjr + 20%\n",
    "    sjr - 20%\n",
    "\n",
    " If we do this right the training data should be approx 6 * 365 * 9 =~ 19710 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T21:55:46.479784500Z",
     "start_time": "2023-06-20T21:54:29.030535500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 6years\n",
    "experiment_name = \"6yearsAugmented\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2008-10-1','2009-1-31'),\n",
    "    ('2009-12-1','2011-6-30'),\n",
    "    ('2011-11-1','2012-12-31'),\n",
    "    ('2013-10-1','2014-12-31'),\n",
    "    ('2016-10-1','2017-12-31')\n",
    "]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_historical.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsanplus20pct.xlsx\"\n",
    "               ]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Colab with Ryan's method\n",
    "This is the same as the 6 years + augmented data but done with the list of \"picked training years\" in the colab script\n",
    "The augmented data:\n",
    "    sac + 15 days\n",
    "    sac - 15 days\n",
    "    sjr + 15 days\n",
    "    sjr - 15 days\n",
    "    sac + 20%\n",
    "    sac - 20%\n",
    "    sjr + 20%\n",
    "    sjr - 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:06<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named colab_simple\n",
    "experiment_name = \"colab_simple\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [('1990-10-1','1991-9-30'),\n",
    "                         ('1992-10-1','1995-9-30'),\n",
    "                         ('1996-10-1','1998-9-30'),\n",
    "                         ('1999-10-1','2003-9-30'),\n",
    "                         ('2004-10-1','2006-9-30'),\n",
    "                         ('2007-10-1','2010-9-30'),\n",
    "                         ('2011-10-1','2013-9-30'),\n",
    "                         ('2014-10-1','2016-9-30'),\n",
    "                         ('2017-10-1','2019-9-30'),]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_historical.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "               #\"dsm2_ann_inputs_rsanplus20pct.xlsx\"\n",
    "               ]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Colab with Ryan's method - minus 2015\n",
    "This is the same as the 6 years + augmented data but done with the list of \"picked training years\" in the colab script\n",
    "The augmented data:\n",
    "    sac + 15 days\n",
    "    sac - 15 days\n",
    "    sjr + 15 days\n",
    "    sjr - 15 days\n",
    "    sac + 20%\n",
    "    sac - 20%\n",
    "    sjr + 20%\n",
    "    sjr - 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named colab_simple_wo2015\n",
    "experiment_name = \"colab_simple_wo2015\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [('1990-10-1','1991-9-30'),\n",
    "                         ('1992-10-1','1995-9-30'),\n",
    "                         ('1996-10-1','1998-9-30'),\n",
    "                         ('1999-10-1','2003-9-30'),\n",
    "                         ('2004-10-1','2006-9-30'),\n",
    "                         ('2007-10-1','2010-9-30'),\n",
    "                         ('2011-10-1','2013-9-30'),\n",
    "                         ('2015-10-1','2016-9-30'),\n",
    "                         ('2017-10-1','2019-9-30'),]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_historical.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus20pct.xlsx\"]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiment: Colab standard\n",
    "This is how the Colab notebook builds the datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T19:31:10.098753200Z",
     "start_time": "2023-06-27T19:31:04.109758600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_name = \"colab\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "train_data = [\"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus20pct.xlsx\",\n",
    "              ]\n",
    "\n",
    "test_data = {'dcc0': \"dsm2_ann_inputs_dcc0.xlsx\",\n",
    "             'smscg1': \"dsm2_ann_inputs_smscg1.xlsx\",\n",
    "             'dcc1': \"dsm2_ann_inputs_dcc1.xlsx\",\n",
    "             'smscg0': \"dsm2_ann_inputs_smscg0.xlsx\"}\n",
    "\n",
    "extra_data = {'observed': \"observed_data_daily.xlsx\"}\n",
    "which_part_for_test = 'last'\n",
    "extra_data_test_ratio = 0.3\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "\n",
    "\n",
    "def read_training_data(train_data):\n",
    "    x_train = None\n",
    "    y_train = None\n",
    "    for data_file in tqdm(train_data):\n",
    "        data_path = os.path.join(local_root_path, data_file)\n",
    "        dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "\n",
    "        # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "        if x_train is None:\n",
    "            (x_train, y_train), (_, _)  = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows  )\n",
    "\n",
    "\n",
    "        else:\n",
    "            (xc, yc), (_, _) = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows )\n",
    "\n",
    "            x_train = pd.concat([x_train, xc], axis=0)\n",
    "            y_train = pd.concat([y_train, yc], axis=0)\n",
    "            del xc, yc\n",
    "    return x_train, y_train\n",
    "\n",
    "train_X, train_Y = read_training_data(train_data)\n",
    "test_X, test_Y = read_training_data(test_data.values())\n",
    "\n",
    "\n",
    "######### Read extra observed dataset ###############\n",
    "for data_file in tqdm(extra_data.values()):\n",
    "    data_path = os.path.join(local_root_path, data_file)\n",
    "\n",
    "    # print(\"Starting read_excel calls:\", data_path)\n",
    "    dflist = [annutils.read_excel_sheet(data_path, i) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets], axis=1).dropna(axis=0)\n",
    "    col_mask = df_inpout.columns.isin(dflist[num_sheets - 1].columns)\n",
    "    dfinps = df_inpout.loc[:, ~col_mask]\n",
    "    dfouts = df_inpout.loc[:, col_mask]\n",
    "    # dfouts = dfouts[output_stations]  # out_stations is None here...\n",
    "\n",
    "    start_year = max(dfinps.index[0].year, dfouts.index[0].year)\n",
    "    end_year = min(dfinps.index[-1].year, dfouts.index[-1].year)\n",
    "\n",
    "    if which_part_for_test == 'last':\n",
    "        calib_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "    elif which_part_for_test == 'first':\n",
    "        calib_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "        valid_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'middle':\n",
    "        calib_slice = [slice(str(start_year),\n",
    "                             str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)))),\n",
    "                       slice(str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year) + 1)),\n",
    "                             str(end_year))]\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)) + 1),\n",
    "                            str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'manual' and picked_training_years is not None:\n",
    "        calib_slice = [slice(str(start_year), str(end_year)) for (start_year, end_year) in picked_training_years]\n",
    "        valid_slice = [slice(start_year, end_year) for ((_, start_year), (end_year, _)) in\n",
    "                       zip([(None, '1989-10-1'), ] + picked_training_years,\n",
    "                           picked_training_years + [('2020-9-30', None), ])]\n",
    "    else:\n",
    "        raise Exception('Unknown data splitting method')\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (x_extra_train, y_extra_train), (x_extra_test, y_extra_test) = \\\n",
    "        annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                      [dfouts],\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays, window_size=window_size, nwindows=nwindows,\n",
    "                                      )\n",
    "\n",
    "    train_X = pd.concat([train_X, x_extra_train], axis=0)\n",
    "    train_Y = pd.concat([train_Y, y_extra_train], axis=0)\n",
    "    test_X = pd.concat([test_X, x_extra_test], axis=0)\n",
    "    test_Y = pd.concat([test_Y, y_extra_test], axis=0)\n",
    "\n",
    "print(\"Done\")\n",
    "# takes about 2minutes...\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Colab reduced\n",
    "This is how the Colab notebook builds the datasets. But the years have been rearranged and thinned so that fewer are used and the test/prediction years tally better with the other methods.\n",
    "\n",
    "Name experiment: colab_reduced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 11570 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 1/4 [00:00<00:02,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:01<00:01,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:02<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"colab_reduced\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [('1990-10-1','1992-2-28'),  # expanded\n",
    "                        # ('1992-10-1','1994-2-28'),  # reduced\n",
    "                         ('1996-10-1','1998-2-28'),  # reduced\n",
    "                         ('2000-2-1','2003-4-30'),\n",
    "                         ('2004-10-1','2005-12-31'),  # reduced\n",
    "                         ('2007-10-1','2008-12-31'),\n",
    "                         ('2010-01-1','2011-05-31'),\n",
    "                         ('2012-01-1','2012-12-31'),  # reduced\n",
    "                         ('2014-03-1','2015-5-31'),\n",
    "                         ('2017-10-1','2019-2-28')]  # reduced\n",
    "\n",
    "train_data = [\"dsm2_ann_inputs_historical.xlsx\",\n",
    "              #\"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "              #\"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "              #\"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "              #\"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "              #\"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "              #\"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "              #\"dsm2_ann_inputs_rsanplus20pct.xlsx\",\n",
    "              ]\n",
    "\n",
    "test_data = {'dcc0': \"dsm2_ann_inputs_dcc0.xlsx\",\n",
    "             'smscg1': \"dsm2_ann_inputs_smscg1.xlsx\",\n",
    "             'dcc1': \"dsm2_ann_inputs_dcc1.xlsx\",\n",
    "             'smscg0': \"dsm2_ann_inputs_smscg0.xlsx\"}\n",
    "\n",
    "extra_data = {'observed': \"observed_data_daily.xlsx\"}\n",
    "which_part_for_test = 'manual'\n",
    "extra_data_test_ratio = 0.3\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "\n",
    "\n",
    "def read_training_data(train_data):\n",
    "    x_train = None\n",
    "    y_train = None\n",
    "    for data_file in tqdm(train_data):\n",
    "        data_path = os.path.join(local_root_path, data_file)\n",
    "        dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "\n",
    "        # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "        if x_train is None:\n",
    "            (x_train, y_train), (_, _)  = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows  )\n",
    "\n",
    "\n",
    "        else:\n",
    "            (xc, yc), (_, _) = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows )\n",
    "\n",
    "            x_train = pd.concat([x_train, xc], axis=0)\n",
    "            y_train = pd.concat([y_train, yc], axis=0)\n",
    "            del xc, yc\n",
    "    return x_train, y_train\n",
    "\n",
    "train_X, train_Y = read_training_data(train_data)\n",
    "test_X, test_Y = read_training_data(test_data.values())\n",
    "\n",
    "\n",
    "######### Read extra observed dataset ###############\n",
    "for data_file in tqdm(extra_data.values()):\n",
    "    data_path = os.path.join(local_root_path, data_file)\n",
    "\n",
    "    # print(\"Starting read_excel calls:\", data_path)\n",
    "    dflist = [annutils.read_excel_sheet(data_path, i) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets], axis=1).dropna(axis=0)\n",
    "    col_mask = df_inpout.columns.isin(dflist[num_sheets - 1].columns)\n",
    "    dfinps = df_inpout.loc[:, ~col_mask]\n",
    "    dfouts = df_inpout.loc[:, col_mask]\n",
    "    # dfouts = dfouts[output_stations]  # out_stations is None here...\n",
    "\n",
    "    start_year = max(dfinps.index[0].year, dfouts.index[0].year)\n",
    "    end_year = min(dfinps.index[-1].year, dfouts.index[-1].year)\n",
    "\n",
    "    if which_part_for_test == 'last':\n",
    "        calib_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "    elif which_part_for_test == 'first':\n",
    "        calib_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "        valid_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'middle':\n",
    "        calib_slice = [slice(str(start_year),\n",
    "                             str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)))),\n",
    "                       slice(str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year) + 1)),\n",
    "                             str(end_year))]\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)) + 1),\n",
    "                            str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'manual' and picked_training_years is not None:\n",
    "        calib_slice = [slice(str(start_year), str(end_year)) for (start_year, end_year) in picked_training_years]\n",
    "        valid_slice = [slice(start_year, end_year) for ((_, start_year), (end_year, _)) in\n",
    "                       zip([(None, '1989-10-1'), ] + picked_training_years,\n",
    "                           picked_training_years + [('2020-9-30', None), ])]\n",
    "    else:\n",
    "        raise Exception('Unknown data splitting method')\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (x_extra_train, y_extra_train), (x_extra_test, y_extra_test) = \\\n",
    "        annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                      [dfouts],\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays, window_size=window_size, nwindows=nwindows,\n",
    "                                      )\n",
    "\n",
    "    train_X = pd.concat([train_X, x_extra_train], axis=0)\n",
    "    train_Y = pd.concat([train_Y, y_extra_train], axis=0)\n",
    "    test_X = pd.concat([test_X, x_extra_test], axis=0)\n",
    "    test_Y = pd.concat([test_Y, y_extra_test], axis=0)\n",
    "\n",
    "print(\"Writing\")\n",
    "# takes about 2minutes...\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
