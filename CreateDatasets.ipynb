{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-27T19:08:50.721848600Z",
     "start_time": "2023-06-27T19:08:50.707849500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T21:54:20.516479800Z",
     "start_time": "2023-06-20T21:54:20.503479300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "local_root_path = \".\"\n",
    "sys.path.append(local_root_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T19:08:53.891169700Z",
     "start_time": "2023-06-27T19:08:53.872170500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import annutils"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T19:08:55.636896600Z",
     "start_time": "2023-06-27T19:08:55.612281500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'annutils' from 'J:\\\\Workspaces\\\\git\\\\SalinityMLWorkshop_DMS_UCD\\\\annutils.py'>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(annutils)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T19:30:24.915182500Z",
     "start_time": "2023-06-27T19:30:24.893221500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Basic Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip', archive_name='out.csv')\n",
    "\n",
    "\n",
    "# Make a dir named Experiments\n",
    "if not os.path.exists(\"Experiments\"):\n",
    "    os.mkdir(\"Experiments\")\n",
    "\n",
    "num_sheets = 9\n",
    "\n",
    "observed_stations_ordered_by_median = ['RSMKL008', 'RSAN032', 'RSAN037', 'RSAC092', 'SLTRM004', 'ROLD024',\n",
    "                                       'CHVCT000', 'RSAN018', 'CHSWP003', 'CHDMC006', 'SLDUT007', 'RSAN072',\n",
    "                                       'OLD_MID', 'RSAN058', 'ROLD059', 'RSAN007', 'RSAC081', 'SLMZU025',\n",
    "                                       'RSAC075', 'SLMZU011', 'SLSUS012', 'SLCBN002', 'RSAC064']\n",
    "\n",
    "output_stations = ['CHDMC006-CVP INTAKE', 'CHSWP003-CCFB_INTAKE', 'CHVCT000-VICTORIA INTAKE',\n",
    "                   'OLD_MID-OLD RIVER NEAR MIDDLE RIVER', 'ROLD024-OLD RIVER AT BACON ISLAND',\n",
    "                   'ROLD059-OLD RIVER AT TRACY BLVD', 'RSAC064-SACRAMENTO R AT PORT CHICAGO',\n",
    "                   'RSAC075-MALLARDISLAND', 'RSAC081-COLLINSVILLE', 'RSAC092-EMMATON',\n",
    "                   'RSAC101-SACRAMENTO R AT RIO VISTA', 'RSAN007-ANTIOCH', 'RSAN018-JERSEYPOINT',\n",
    "                   'RSAN032-SACRAMENTO R AT SAN ANDREAS LANDING', 'RSAN037-SAN JOAQUIN R AT PRISONERS POINT',\n",
    "                   'RSAN058-ROUGH AND READY ISLAND', 'RSAN072-SAN JOAQUIN R AT BRANDT BRIDGE',\n",
    "                   'RSMKL008-S FORK MOKELUMNE AT TERMINOUS', 'SLCBN002-CHADBOURNE SLOUGH NR SUNRISE DUCK CLUB',\n",
    "                   'SLDUT007-DUTCH SLOUGH', 'SLMZU011-MONTEZUMA SL AT BELDONS LANDING',\n",
    "                   'SLMZU025-MONTEZUMA SL AT NATIONAL STEEL', 'SLSUS012-SUISUN SL NEAR VOLANTI SL',\n",
    "                   'SLTRM004-THREE MILE SLOUGH NR SAN JOAQUIN R', 'SSS-STEAMBOAT SL', 'CCW-MIDDLE RIVER INTAKE',\n",
    "                   'OH4-OLD R @ HWY 4', 'SLRCK005-CCWD_Rock', 'MRU-MIDDLE RIVER AT UNDINE ROAD', 'HLL-HOLLAND TRACT',\n",
    "                   'BET-PIPER SLOUGH @ BETHEL TRACT', 'GES-SACRAMENTO R BELOW GEORGIANA SLOUGH',\n",
    "                   'NMR: N FORK MOKELUMNE R NEAR WALNUT GROVE', 'IBS-CORDELIA SLOUGH @ IBIS CLUB',\n",
    "                   'GYS-GOODYEAR SLOUGH AT MORROW ISLAND CLUB', 'BKS-SLBAR002-North Bay Aqueduct/Barker Sl']\n",
    "\n",
    "output_stations, name_mapping = annutils.read_output_stations(output_stations, observed_stations_ordered_by_median)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T19:09:04.144975100Z",
     "start_time": "2023-06-27T19:09:04.136366Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment:6 Years\n",
    "This is just the base data for the 6 selected years.\n",
    " If we do this right the training data should be approx 6 * 365 =~ 2190 rows of training data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Make a dir named 6years\n",
    "experiment_name = \"6years\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2008-10-1','2009-9-30'),\n",
    "    ('2010-10-1','2011-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30'),\n",
    "    ('2016-10-1','2017-9-30')\n",
    "]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in input_files:\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 8 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T21:54:20.627479800Z",
     "start_time": "2023-06-20T21:54:20.583475800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "df_plus = annutils.create_antecedent_inputs(X_df,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "# df_plus should now have 118 * 8 = 944 input features\n",
    "\n",
    "# synchronize trims off the na values so the row numbers go from 10920 to 10803\n",
    "df_X2, df_Y2 = annutils.synchronize(df_plus, Y_df)\n",
    "\n",
    "train_X = annutils.include(df_X2, picked_training_years)\n",
    "train_Y = annutils.include(df_Y2, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(df_X2, picked_training_years)\n",
    "test_Y = annutils.exclude(df_Y2, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T21:54:29.021533900Z",
     "start_time": "2023-06-20T21:54:20.630477200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment: 6 Years with Augmented data\n",
    "This is the same as the previous experiment but adds in the augmented data.\n",
    "The augmented data:\n",
    "    sac + 15 days\n",
    "    sac - 15 days\n",
    "    sjr + 15 days\n",
    "    sjr - 15 days\n",
    "    sac + 20%\n",
    "    sac - 20%\n",
    "    sjr + 20%\n",
    "    sjr - 20%\n",
    "\n",
    " If we do this right the training data should be approx 6 * 365 * 9 =~ 19710 rows of data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Make a dir named 6years\n",
    "experiment_name = \"6yearsAugmented\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2008-10-1','2009-9-30'),\n",
    "    ('2010-10-1','2011-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30'),\n",
    "    ('2016-10-1','2017-9-30')\n",
    "]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_base.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus20pct.xlsx\"]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T21:55:46.479784500Z",
     "start_time": "2023-06-20T21:54:29.030535500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment: Colab standard\n",
    "This is how the Colab notebook builds the datasets.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36478c129dc044c2a44fff200803f84f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "161a8a009b184de1a4b500e7dc5df063"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9d15a72c4c0465095def57375b0801d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"colab\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "train_data = [\"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus20pct.xlsx\",\n",
    "              ]\n",
    "\n",
    "test_data = {'dcc0': \"dsm2_ann_inputs_dcc0.xlsx\",\n",
    "             'smscg1': \"dsm2_ann_inputs_smscg1.xlsx\",\n",
    "             'dcc1': \"dsm2_ann_inputs_dcc1.xlsx\",\n",
    "             'smscg0': \"dsm2_ann_inputs_smscg0.xlsx\"}\n",
    "\n",
    "extra_data = {'observed': \"observed_data_daily.xlsx\"}\n",
    "which_part_for_test = 'last'\n",
    "extra_data_test_ratio = 0.3\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "\n",
    "\n",
    "def read_training_data(train_data):\n",
    "    x_train = None\n",
    "    y_train = None\n",
    "    for data_file in tqdm(train_data):\n",
    "        data_path = os.path.join(local_root_path, data_file)\n",
    "        dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "\n",
    "        # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "        if x_train is None:\n",
    "            (x_train, y_train), (_, _)  = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows  )\n",
    "\n",
    "\n",
    "        else:\n",
    "            (xc, yc), (_, _) = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows )\n",
    "\n",
    "            x_train = pd.concat([x_train, xc], axis=0)\n",
    "            y_train = pd.concat([y_train, yc], axis=0)\n",
    "            del xc, yc\n",
    "    return x_train, y_train\n",
    "\n",
    "train_X, train_Y = read_training_data(train_data)\n",
    "test_X, test_Y = read_training_data(test_data.values())\n",
    "\n",
    "\n",
    "######### Read extra observed dataset ###############\n",
    "for data_file in tqdm(extra_data.values()):\n",
    "    data_path = os.path.join(local_root_path, data_file)\n",
    "\n",
    "    # print(\"Starting read_excel calls:\", data_path)\n",
    "    dflist = [annutils.read_excel_sheet(data_path, i) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets], axis=1).dropna(axis=0)\n",
    "    col_mask = df_inpout.columns.isin(dflist[num_sheets - 1].columns)\n",
    "    dfinps = df_inpout.loc[:, ~col_mask]\n",
    "    dfouts = df_inpout.loc[:, col_mask]\n",
    "    # dfouts = dfouts[output_stations]  # out_stations is None here...\n",
    "\n",
    "    start_year = max(dfinps.index[0].year, dfouts.index[0].year)\n",
    "    end_year = min(dfinps.index[-1].year, dfouts.index[-1].year)\n",
    "\n",
    "    if which_part_for_test == 'last':\n",
    "        calib_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "    elif which_part_for_test == 'first':\n",
    "        calib_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "        valid_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'middle':\n",
    "        calib_slice = [slice(str(start_year),\n",
    "                             str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)))),\n",
    "                       slice(str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year) + 1)),\n",
    "                             str(end_year))]\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)) + 1),\n",
    "                            str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'manual' and picked_training_years is not None:\n",
    "        calib_slice = [slice(str(start_year), str(end_year)) for (start_year, end_year) in picked_training_years]\n",
    "        valid_slice = [slice(start_year, end_year) for ((_, start_year), (end_year, _)) in\n",
    "                       zip([(None, '1989-10-1'), ] + picked_training_years,\n",
    "                           picked_training_years + [('2020-9-30', None), ])]\n",
    "    else:\n",
    "        raise Exception('Unknown data splitting method')\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (x_extra_train, y_extra_train), (x_extra_test, y_extra_test) = \\\n",
    "        annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                      [dfouts],\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays, window_size=window_size, nwindows=nwindows,\n",
    "                                      )\n",
    "\n",
    "    train_X = pd.concat([train_X, x_extra_train], axis=0)\n",
    "    train_Y = pd.concat([train_Y, y_extra_train], axis=0)\n",
    "    test_X = pd.concat([test_X, x_extra_test], axis=0)\n",
    "    test_Y = pd.concat([test_Y, y_extra_test], axis=0)\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T19:31:10.098753200Z",
     "start_time": "2023-06-27T19:31:04.109758600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# takes about 2minutes...\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T19:33:21.078075Z",
     "start_time": "2023-06-27T19:31:38.968677Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
