{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T19:08:50.721848600Z",
     "start_time": "2023-06-27T19:08:50.707849500Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'annutils' from 'd:\\\\projects\\\\delta_salinity\\\\scripts\\\\rma_ann_repo\\\\annutils.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "tf.config.list_physical_devices('GPU')\n",
    "local_root_path = \".\"\n",
    "sys.path.append(local_root_path)\n",
    "import annutils\n",
    "import importlib\n",
    "importlib.reload(annutils)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T19:09:04.144975100Z",
     "start_time": "2023-06-27T19:09:04.136366Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip', archive_name='out.csv')\n",
    "\n",
    "\n",
    "# Make a dir named Experiments\n",
    "if not os.path.exists(\"Experiments\"):\n",
    "    os.mkdir(\"Experiments\")\n",
    "\n",
    "num_sheets = 9\n",
    "\n",
    "observed_stations_ordered_by_median = ['RSMKL008', 'RSAN032', 'RSAN037', 'RSAC092', 'SLTRM004', 'ROLD024',\n",
    "                                       'CHVCT000', 'RSAN018', 'CHSWP003', 'CHDMC006', 'SLDUT007', 'RSAN072',\n",
    "                                       'OLD_MID', 'RSAN058', 'ROLD059', 'RSAN007', 'RSAC081', 'SLMZU025',\n",
    "                                       'RSAC075', 'SLMZU011', 'SLSUS012', 'SLCBN002', 'RSAC064']\n",
    "\n",
    "output_stations = ['CHDMC006-CVP INTAKE', 'CHSWP003-CCFB_INTAKE', 'CHVCT000-VICTORIA INTAKE',\n",
    "                   'OLD_MID-OLD RIVER NEAR MIDDLE RIVER', 'ROLD024-OLD RIVER AT BACON ISLAND',\n",
    "                   'ROLD059-OLD RIVER AT TRACY BLVD', 'RSAC064-SACRAMENTO R AT PORT CHICAGO',\n",
    "                   'RSAC075-MALLARDISLAND', 'RSAC081-COLLINSVILLE', 'RSAC092-EMMATON',\n",
    "                   'RSAC101-SACRAMENTO R AT RIO VISTA', 'RSAN007-ANTIOCH', 'RSAN018-JERSEYPOINT',\n",
    "                   'RSAN032-SACRAMENTO R AT SAN ANDREAS LANDING', 'RSAN037-SAN JOAQUIN R AT PRISONERS POINT',\n",
    "                   'RSAN058-ROUGH AND READY ISLAND', 'RSAN072-SAN JOAQUIN R AT BRANDT BRIDGE',\n",
    "                   'RSMKL008-S FORK MOKELUMNE AT TERMINOUS', 'SLCBN002-CHADBOURNE SLOUGH NR SUNRISE DUCK CLUB',\n",
    "                   'SLDUT007-DUTCH SLOUGH', 'SLMZU011-MONTEZUMA SL AT BELDONS LANDING',\n",
    "                   'SLMZU025-MONTEZUMA SL AT NATIONAL STEEL', 'SLSUS012-SUISUN SL NEAR VOLANTI SL',\n",
    "                   'SLTRM004-THREE MILE SLOUGH NR SAN JOAQUIN R', 'SSS-STEAMBOAT SL', 'CCW-MIDDLE RIVER INTAKE',\n",
    "                   'OH4-OLD R @ HWY 4', 'SLRCK005-CCWD_Rock', 'MRU-MIDDLE RIVER AT UNDINE ROAD', 'HLL-HOLLAND TRACT',\n",
    "                   'BET-PIPER SLOUGH @ BETHEL TRACT', 'GES-SACRAMENTO R BELOW GEORGIANA SLOUGH',\n",
    "                   'NMR: N FORK MOKELUMNE R NEAR WALNUT GROVE', 'IBS-CORDELIA SLOUGH @ IBIS CLUB',\n",
    "                   'GYS-GOODYEAR SLOUGH AT MORROW ISLAND CLUB', 'BKS-SLBAR002-North Bay Aqueduct/Barker Sl']\n",
    "\n",
    "output_stations, name_mapping = annutils.read_output_stations(output_stations, observed_stations_ordered_by_median)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Experiment:6 Years\n",
    "This is just the base data for the 6 selected years.\n",
    " If we do this right the training data should be approx 6 * 365 =~ 2190 rows of training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T21:54:20.627479800Z",
     "start_time": "2023-06-20T21:54:20.583475800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dir named 6years\n",
    "experiment_name = \"6years\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2008-10-1','2009-9-30'),\n",
    "    ('2010-10-1','2011-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30'),\n",
    "    ('2016-10-1','2017-9-30')\n",
    "]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in input_files:\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 8 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T21:54:29.021533900Z",
     "start_time": "2023-06-20T21:54:20.630477200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "df_plus = annutils.create_antecedent_inputs(X_df,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "# df_plus should now have 118 * 8 = 944 input features\n",
    "\n",
    "# synchronize trims off the na values so the row numbers go from 10920 to 10803\n",
    "df_X2, df_Y2 = annutils.synchronize(df_plus, Y_df)\n",
    "\n",
    "train_X = annutils.include(df_X2, picked_training_years)\n",
    "train_Y = annutils.include(df_Y2, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(df_X2, picked_training_years)\n",
    "test_Y = annutils.exclude(df_Y2, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years\n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compiling inputs for 4years_DCC experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years\n",
    "# experiment_names = [\"4years\",\"4years_DCC\",\"4years_SacLag\",\"4years_SacMag\"]\n",
    "experiment_names = [\"4years_DCC\"]\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "\n",
    "input_files_dict = {\"4years\":[\"dsm2_ann_inputs_base.xlsx\"],\n",
    "                    \"4years_DCC\":[\"dsm2_ann_inputs_base.xlsx\",\n",
    "                                  \"dsm2_ann_inputs_dcc0.xlsx\",\n",
    "                                  \"dsm2_ann_inputs_dcc1.xlsx\"],\n",
    "                                #   \"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "                                #   \"dsm2_ann_inputs_rsacminus20pct.xlsx\"],\n",
    "                    \"4years_SacLag\":[\"dsm2_ann_inputs_base.xlsx\",\n",
    "                                     \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "                                     \"dsm2_ann_inputs_rsanminus15day.xlsx\"],\n",
    "                    \"4years_SacMag\":[\"dsm2_ann_inputs_base.xlsx\",\n",
    "                                     \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "                                     \"dsm2_ann_inputs_rsanminus20pct.xlsx\"]\n",
    "}\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "for experiment_name in experiment_names:\n",
    "    if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "        os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "    X_df= None\n",
    "    Y_df= None\n",
    "\n",
    "    input_files = input_files_dict[experiment_name]\n",
    "\n",
    "    for data_file in tqdm(input_files):\n",
    "        data_path = os.path.join(local_root_path,data_file)\n",
    "        dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "        dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "        dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "        X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "        Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "    # now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "    train_X = annutils.include(X_df, picked_training_years)\n",
    "    train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "    test_X = annutils.exclude(X_df, picked_training_years)\n",
    "    test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "    train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "    train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "    test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "    test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "    \n",
    "    print(f\"Finished compiling inputs for {experiment_name} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years (calendar years) \n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data and using jan 1 - dec 31 instead of water year limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compiling inputs for 4years_cal experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_cal\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2008-1-1','2008-12-31'),\n",
    "    ('2010-1-1','2010-12-31'),\n",
    "    ('2012-1-1','2012-12-31'),\n",
    "    ('2014-1-1','2014-12-31')\n",
    "]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "\n",
    "experiment = '4years_cal'\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years (calendar years) plus augmented data\n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data and using jan 1 - dec 31 instead of water year limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for 4years_cal_hiexplonf2 experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_cal\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2008-1-1','2008-12-31'),\n",
    "    ('2010-1-1','2010-12-31'),\n",
    "    ('2012-1-1','2012-12-31'),\n",
    "    ('2014-1-1','2014-12-31')\n",
    "]\n",
    "\n",
    "aug_data = [('2014-1-1','2014-12-31')]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "aug_input_files = [r\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\modified_bc\\anninputs\\hiexplonf2\\dsm2_ann_inputs_hiexplonf2.xlsx\"]\n",
    "\n",
    "experiment = '4years_cal_hiexplonf2'\n",
    "dcc_sub_part_f = 'DSP_HIEXPLONF2_202308'\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "# Add augmented data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(aug_input_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "x_aug_train = annutils.include(X_df, aug_data)\n",
    "y_aug_train = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# needs to combine the DCC gate operation DSS codes into one column.\n",
    "x_aug_train.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in x_aug_train.columns]\n",
    "y_aug_train.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in y_aug_train.columns]\n",
    "x_aug_train.columns = [s.replace('01JAN2013 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in x_aug_train.columns]\n",
    "y_aug_train.columns = [s.replace('01JAN2013 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in y_aug_train.columns]\n",
    "x_aug_train.columns = [s.replace('1DAY', 'IR-YEAR') for s in x_aug_train.columns]\n",
    "y_aug_train.columns = [s.replace('1DAY', 'IR-YEAR') for s in y_aug_train.columns]\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(x_aug_train.columns))}')\n",
    "\n",
    "train_X = pd.concat([train_X, x_aug_train], axis=0)\n",
    "train_Y = pd.concat([train_Y, y_aug_train], axis=0)\n",
    "\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years (calendar years) but perturbed\n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data and using jan 1 - dec 31 instead of water year limits\n",
    "But the input data to DSM2 has been perturbed from the baseline using perturbhist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for 4years_perturbhist experiment\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_aug_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m test_Y \u001b[39m=\u001b[39m annutils\u001b[39m.\u001b[39minclude(Y_df, test_data)\n\u001b[0;32m     54\u001b[0m \u001b[39m# needs to combine the DCC gate operation DSS codes into one column.\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m x_aug_train\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39mreplace(dcc_sub_part_f, dcc_part_f) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m x_aug_train\u001b[39m.\u001b[39mcolumns]\n\u001b[0;32m     56\u001b[0m y_aug_train\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39mreplace(dcc_sub_part_f, dcc_part_f) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m y_aug_train\u001b[39m.\u001b[39mcolumns]\n\u001b[0;32m     57\u001b[0m x_aug_train\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m01JAN2013 - 01JAN2014\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m01JAN1953 - 01JAN2020\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m x_aug_train\u001b[39m.\u001b[39mcolumns]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_aug_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "\n",
    "aug_data = [('2008-1-1','2008-12-31'),\n",
    "    ('2010-1-1','2010-12-31'),\n",
    "    ('2012-1-1','2012-12-31'),\n",
    "    ('2014-1-1','2014-12-31')]\n",
    "\n",
    "aug_input_files = [r\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\modified_bc\\anninputs\\perturbhist\\dsm2_ann_inputs_perturbhist.xlsx\"]\n",
    "\n",
    "test_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "test_data = aug_data\n",
    "\n",
    "experiment = '4years_perturbhist'\n",
    "dcc_sub_part_f = 'DSP_PERTURBHIST_202308'\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "# Add augmented data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(aug_input_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, aug_data)\n",
    "train_Y = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = annutils.include(X_df, test_data)\n",
    "test_Y = annutils.include(Y_df, test_data)\n",
    "\n",
    "# needs to combine the DCC gate operation DSS codes into one column.\n",
    "train_X.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in train_X.columns]\n",
    "train_Y.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in train_Y.columns]\n",
    "train_X.columns = [s.replace('01JAN2007 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in train_X.columns]\n",
    "train_Y.columns = [s.replace('01JAN2007 - 01JAN2014', '01JAN1953 - 01JAN2020') for s in train_Y.columns]\n",
    "train_X.columns = [s.replace('1DAY', 'IR-YEAR') for s in train_X.columns]\n",
    "train_Y.columns = [s.replace('1DAY', 'IR-YEAR') for s in train_Y.columns]\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirement: 4 years plus augmented data\n",
    "\n",
    "This is the same as 6 years but with only 4 years in the data and the augmented inputs/outputs from DSM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m nwindows\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     21\u001b[0m experiment_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m4years_hiexplonf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m\"\u001b[39m\u001b[39mExperiments/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m experiment_name):\n\u001b[0;32m     24\u001b[0m     os\u001b[39m.\u001b[39mmkdir(\u001b[39m\"\u001b[39m\u001b[39mExperiments/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m experiment_name)\n\u001b[0;32m     26\u001b[0m X_df\u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years\n",
    "# experiment_names = [\"4years\",\"4years_DCC\",\"4years_SacLag\",\"4years_SacMag\"]\n",
    "experiment_names = [\"4years_hiexplonf\"]\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "\n",
    "aug_data = [('2013-11-1','2014-6-1')]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "aug_input_files = [r\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\modified_bc\\anninputs\\hiexplonf\\dsm2_ann_inputs_hiexplonf.xlsx\"]\n",
    "\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "experiment_name = '4years_hiexplonf'\n",
    "    \n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "# Add augmented data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(aug_input_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "x_aug_train = annutils.include(X_df, aug_data)\n",
    "y_aug_train = annutils.include(Y_df, aug_data)\n",
    "\n",
    "train_X = pd.concat([train_X, x_aug_train], axis=0)\n",
    "train_Y = pd.concat([train_Y, y_aug_train], axis=0)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment_name} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Perturbed Latinhypercube\n",
    "\n",
    "This has 7 years using 2008, 2010, 2012, 2014 with variations of perturbed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for latinhypercube experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "experiment = 'latinhypercube'\n",
    "\n",
    "test_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "test_data = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "numbers = {1: 2014,\n",
    "            2: 2010,\n",
    "            3: 2008,\n",
    "            4: 2014,\n",
    "            5: 2012,\n",
    "            6: 2010,\n",
    "            7: 2008\n",
    "}\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "for case_num, yearran in numbers.items():\n",
    "\n",
    "    aug_data = [(f'{str(yearran)}-1-1',f'{str(yearran)}-12-31')]\n",
    "\n",
    "    aug_input_file = fr\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\latinhypercube\\anninputs\\lathypcub_{str(case_num)}\\dsm2_ann_inputs_lathypcub_{str(case_num)}.xlsx\"\n",
    "\n",
    "    if case_num == 3 or case_num == 7:\n",
    "        gate_per = '01JAN1953 - 01JAN2022'\n",
    "        dcc_sub_part_f = 'DWR-DMS-DSM2'\n",
    "    else:\n",
    "        gate_per = f'01JAN{str(yearran-1)} - 01JAN{str(yearran)}'\n",
    "        dcc_sub_part_f = f'DSP_LATHYPCUB_{str(case_num)}_202309'\n",
    "\n",
    "    dfinps, dfouts = annutils.read_and_split(aug_input_file, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    \n",
    "    # needs to combine the DCC gate operation DSS codes into one column.\n",
    "    dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfouts.columns]\n",
    "    \n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, aug_data)\n",
    "train_Y = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = annutils.include(X_df, test_data)\n",
    "test_Y = annutils.include(Y_df, test_data)\n",
    "\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Perturbed Latinhypercube with tidal shift\n",
    "\n",
    "This has 7 years using 2008, 2010, 2012, 2014 with variations of perturbed data but always has a perturbed tidal cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for lathypcub_tideshift experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "experiment = 'lathypcub_tideshift'\n",
    "\n",
    "test_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "test_data = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "numbers = {1: 2014,\n",
    "            2: 2010,\n",
    "            3: 2008,\n",
    "            4: 2014,\n",
    "            5: 2012,\n",
    "            6: 2010,\n",
    "            7: 2008\n",
    "}\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "for case_num, yearran in numbers.items():\n",
    "\n",
    "    aug_data = [(f'{str(yearran)}-1-1',f'{str(yearran)}-12-31')]\n",
    "    \n",
    "    aug_input_file = fr\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\lathypcub_tideshift\\anninputs\\lathypcub_tideshift_{str(case_num)}\\dsm2_ann_inputs_lathypcub_tideshift_{str(case_num)}.xlsx\"\n",
    "\n",
    "    if case_num == 3 or case_num == 7:\n",
    "        gate_per = '01JAN1953 - 01JAN2022'\n",
    "        dcc_sub_part_f = 'DWR-DMS-DSM2'\n",
    "    else:\n",
    "        gate_per = f'01JAN{str(yearran-1)} - 01JAN{str(yearran)}'\n",
    "        dcc_sub_part_f = f'DSP_LATHYPCUB_{str(case_num)}_202309'\n",
    "\n",
    "    dfinps, dfouts = annutils.read_and_split(aug_input_file, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    \n",
    "    # needs to combine the DCC gate operation DSS codes into one column.\n",
    "    dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfouts.columns]\n",
    "    \n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, aug_data)\n",
    "train_Y = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = annutils.include(X_df, test_data)\n",
    "test_Y = annutils.include(Y_df, test_data)\n",
    "\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Perturbed Latinhypercube no tidal shift\n",
    "\n",
    "This has 7 years using 2008, 2010, 2012, 2014 with variations of perturbed data but never has a perturbed tidal cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for lathypcub_regtide experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "experiment = 'lathypcub_regtide'\n",
    "\n",
    "test_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "test_data = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "numbers = {1: 2014,\n",
    "            2: 2010,\n",
    "            3: 2008,\n",
    "            4: 2014,\n",
    "            5: 2012,\n",
    "            6: 2010,\n",
    "            7: 2008\n",
    "}\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "for case_num, yearran in numbers.items():\n",
    "\n",
    "    aug_data = [(f'{str(yearran)}-1-1',f'{str(yearran)}-12-31')]\n",
    "    \n",
    "    aug_input_file = fr\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\lathypcub_regtide\\anninputs\\lathypcub_regtide_{str(case_num)}\\dsm2_ann_inputs_lathypcub_regtide_{str(case_num)}.xlsx\"\n",
    "\n",
    "    if case_num == 3 or case_num == 7:\n",
    "        gate_per = '01JAN1953 - 01JAN2022'\n",
    "        dcc_sub_part_f = 'DWR-DMS-DSM2'\n",
    "    else:\n",
    "        gate_per = f'01JAN{str(yearran-1)} - 01JAN{str(yearran)}'\n",
    "        dcc_sub_part_f = f'DSP_LATHYPCUB_{str(case_num)}_202309'\n",
    "\n",
    "    dfinps, dfouts = annutils.read_and_split(aug_input_file, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    \n",
    "    # needs to combine the DCC gate operation DSS codes into one column.\n",
    "    dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfouts.columns]\n",
    "    \n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, aug_data)\n",
    "train_Y = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = annutils.include(X_df, test_data)\n",
    "test_Y = annutils.include(Y_df, test_data)\n",
    "\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Perturbed Latinhypercube 7 day tidal shift\n",
    "\n",
    "This has 7 years using 2008, 2010, 2012, 2014 with variations of perturbed data but always has a 7 day tidal shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "d:\\projects\\delta_salinity\\scripts\\rma_ann_repo\\annutils.py:502: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  return pd.concat([df.loc[start:end] for start, end in start_and_end])\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping columns: 0\n",
      "Finished compiling inputs for lathypcub_7tide experiment\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 4years_perturbhist\n",
    "experiment = 'lathypcub_7tide'\n",
    "\n",
    "test_files = [\"dsm2_ann_inputs_base.xlsx\"]\n",
    "test_data = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2009-10-1','2010-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30')\n",
    "]\n",
    "numbers = {1: 2014,\n",
    "            2: 2010,\n",
    "            3: 2008,\n",
    "            4: 2014,\n",
    "            5: 2012,\n",
    "            6: 2010,\n",
    "            7: 2008\n",
    "}\n",
    "ndays=118\n",
    "window_size=0\n",
    "nwindows=0\n",
    "\n",
    "if not os.path.exists(\"Experiments/\" + experiment):\n",
    "    os.mkdir(\"Experiments/\" + experiment)\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "dcc_part_f = 'DWR-DMS-DSM2'\n",
    "\n",
    "for case_num, yearran in numbers.items():\n",
    "\n",
    "    aug_data = [(f'{str(yearran)}-1-1',f'{str(yearran)}-12-31')]\n",
    "    \n",
    "    aug_input_file = fr\"D:\\projects\\delta_salinity\\model\\dsm2\\DSP_DSM2_202307\\lathypcub_7tide\\anninputs\\lathypcub_7tide_{str(case_num)}\\dsm2_ann_inputs_lathypcub_7tide_{str(case_num)}.xlsx\"\n",
    "\n",
    "    if case_num == 3 or case_num == 7:\n",
    "        gate_per = '01JAN1953 - 01JAN2022'\n",
    "        dcc_sub_part_f = 'DWR-DMS-DSM2'\n",
    "    else:\n",
    "        gate_per = f'01JAN{str(yearran-1)} - 01JAN{str(yearran)}'\n",
    "        dcc_sub_part_f = f'DSP_LATHYPCUB_{str(case_num)}_202309'\n",
    "\n",
    "    dfinps, dfouts = annutils.read_and_split(aug_input_file, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    \n",
    "    # needs to combine the DCC gate operation DSS codes into one column.\n",
    "    dfinps.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(dcc_sub_part_f, dcc_part_f) for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace(gate_per, '01JAN1953 - 01JAN2020') for s in dfouts.columns]\n",
    "    dfinps.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfinps.columns]\n",
    "    dfouts.columns = [s.replace('1DAY', 'IR-YEAR') for s in dfouts.columns]\n",
    "    \n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "train_X = annutils.include(X_df, aug_data)\n",
    "train_Y = annutils.include(Y_df, aug_data)\n",
    "\n",
    "# Add test data\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "for data_file in tqdm(test_files):\n",
    "    data_path = data_file\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "test_X = annutils.include(X_df, test_data)\n",
    "test_Y = annutils.include(Y_df, test_data)\n",
    "\n",
    "\n",
    "print(f'non-overlapping columns: {len(set(train_X.columns) ^ set(test_X.columns))}')\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n",
    "print(f\"Finished compiling inputs for {experiment} experiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiment: 6 Years with Augmented data\n",
    "This is the same as the previous experiment but adds in the augmented data.\n",
    "The augmented data:\n",
    "    sac + 15 days\n",
    "    sac - 15 days\n",
    "    sjr + 15 days\n",
    "    sjr - 15 days\n",
    "    sac + 20%\n",
    "    sac - 20%\n",
    "    sjr + 20%\n",
    "    sjr - 20%\n",
    "\n",
    " If we do this right the training data should be approx 6 * 365 * 9 =~ 19710 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T21:55:46.479784500Z",
     "start_time": "2023-06-20T21:54:29.030535500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:08<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named 6years\n",
    "experiment_name = \"6yearsAugmented\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [\n",
    "    ('2007-10-1','2008-9-30'),\n",
    "    ('2008-10-1','2009-9-30'),\n",
    "    ('2010-10-1','2011-9-30'),\n",
    "    ('2011-10-1','2012-9-30'),\n",
    "    ('2013-10-1','2014-9-30'),\n",
    "    ('2016-10-1','2017-9-30')\n",
    "]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_base.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus20pct.xlsx\"]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Colab with Ryan's method\n",
    "This is the same as the 6 years + augmented data but done with the list of \"picked training years\" in the colab script\n",
    "The augmented data:\n",
    "    sac + 15 days\n",
    "    sac - 15 days\n",
    "    sjr + 15 days\n",
    "    sjr - 15 days\n",
    "    sac + 20%\n",
    "    sac - 20%\n",
    "    sjr + 20%\n",
    "    sjr - 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:06<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named colab_simple\n",
    "experiment_name = \"colab_simple\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [('1990-10-1','1991-9-30'),\n",
    "                         ('1992-10-1','1995-9-30'),\n",
    "                         ('1996-10-1','1998-9-30'),\n",
    "                         ('1999-10-1','2003-9-30'),\n",
    "                         ('2004-10-1','2006-9-30'),\n",
    "                         ('2007-10-1','2010-9-30'),\n",
    "                         ('2011-10-1','2013-9-30'),\n",
    "                         ('2014-10-1','2016-9-30'),\n",
    "                         ('2017-10-1','2019-9-30'),]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_base.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus20pct.xlsx\"]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Colab with Ryan's method - minus 2015\n",
    "This is the same as the 6 years + augmented data but done with the list of \"picked training years\" in the colab script\n",
    "The augmented data:\n",
    "    sac + 15 days\n",
    "    sac - 15 days\n",
    "    sjr + 15 days\n",
    "    sjr - 15 days\n",
    "    sac + 20%\n",
    "    sac - 20%\n",
    "    sjr + 20%\n",
    "    sjr - 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make a dir named colab_simple_wo2015\n",
    "experiment_name = \"colab_simple_wo2015\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [('1990-10-1','1991-9-30'),\n",
    "                         ('1992-10-1','1995-9-30'),\n",
    "                         ('1996-10-1','1998-9-30'),\n",
    "                         ('1999-10-1','2003-9-30'),\n",
    "                         ('2004-10-1','2006-9-30'),\n",
    "                         ('2007-10-1','2010-9-30'),\n",
    "                         ('2011-10-1','2013-9-30'),\n",
    "                         ('2015-10-1','2016-9-30'),\n",
    "                         ('2017-10-1','2019-9-30'),]\n",
    "\n",
    "input_files = [\"dsm2_ann_inputs_base.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "               \"dsm2_ann_inputs_rsanplus20pct.xlsx\"]\n",
    "\n",
    "X_df= None\n",
    "Y_df= None\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "for data_file in tqdm(input_files):\n",
    "    data_path = os.path.join(local_root_path,data_file)\n",
    "    dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "    dfinps = annutils.create_antecedent_inputs(dfinps,ndays=ndays,window_size=window_size,nwindows=nwindows)\n",
    "    dfinps, dfouts = annutils.synchronize(dfinps, dfouts)\n",
    "    X_df = pd.concat([X_df, dfinps], axis=0)\n",
    "    Y_df = pd.concat([Y_df, dfouts], axis=0)\n",
    "\n",
    "# now X_df should have 118 * 8 = 944 input features and Y_df should have 23 target salinity values\n",
    "\n",
    "\n",
    "train_X = annutils.include(X_df, picked_training_years)\n",
    "train_Y = annutils.include(Y_df, picked_training_years)\n",
    "\n",
    "test_X = annutils.exclude(X_df, picked_training_years)\n",
    "test_Y = annutils.exclude(Y_df, picked_training_years)\n",
    "\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiment: Colab standard\n",
    "This is how the Colab notebook builds the datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T19:31:10.098753200Z",
     "start_time": "2023-06-27T19:31:04.109758600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36478c129dc044c2a44fff200803f84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161a8a009b184de1a4b500e7dc5df063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d15a72c4c0465095def57375b0801d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"colab\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "train_data = [\"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus20pct.xlsx\",\n",
    "              ]\n",
    "\n",
    "test_data = {'dcc0': \"dsm2_ann_inputs_dcc0.xlsx\",\n",
    "             'smscg1': \"dsm2_ann_inputs_smscg1.xlsx\",\n",
    "             'dcc1': \"dsm2_ann_inputs_dcc1.xlsx\",\n",
    "             'smscg0': \"dsm2_ann_inputs_smscg0.xlsx\"}\n",
    "\n",
    "extra_data = {'observed': \"observed_data_daily.xlsx\"}\n",
    "which_part_for_test = 'last'\n",
    "extra_data_test_ratio = 0.3\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "\n",
    "\n",
    "def read_training_data(train_data):\n",
    "    x_train = None\n",
    "    y_train = None\n",
    "    for data_file in tqdm(train_data):\n",
    "        data_path = os.path.join(local_root_path, data_file)\n",
    "        dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "\n",
    "        # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "        if x_train is None:\n",
    "            (x_train, y_train), (_, _)  = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows  )\n",
    "\n",
    "\n",
    "        else:\n",
    "            (xc, yc), (_, _) = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows )\n",
    "\n",
    "            x_train = pd.concat([x_train, xc], axis=0)\n",
    "            y_train = pd.concat([y_train, yc], axis=0)\n",
    "            del xc, yc\n",
    "    return x_train, y_train\n",
    "\n",
    "train_X, train_Y = read_training_data(train_data)\n",
    "test_X, test_Y = read_training_data(test_data.values())\n",
    "\n",
    "\n",
    "######### Read extra observed dataset ###############\n",
    "for data_file in tqdm(extra_data.values()):\n",
    "    data_path = os.path.join(local_root_path, data_file)\n",
    "\n",
    "    # print(\"Starting read_excel calls:\", data_path)\n",
    "    dflist = [annutils.read_excel_sheet(data_path, i) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets], axis=1).dropna(axis=0)\n",
    "    col_mask = df_inpout.columns.isin(dflist[num_sheets - 1].columns)\n",
    "    dfinps = df_inpout.loc[:, ~col_mask]\n",
    "    dfouts = df_inpout.loc[:, col_mask]\n",
    "    # dfouts = dfouts[output_stations]  # out_stations is None here...\n",
    "\n",
    "    start_year = max(dfinps.index[0].year, dfouts.index[0].year)\n",
    "    end_year = min(dfinps.index[-1].year, dfouts.index[-1].year)\n",
    "\n",
    "    if which_part_for_test == 'last':\n",
    "        calib_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "    elif which_part_for_test == 'first':\n",
    "        calib_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "        valid_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'middle':\n",
    "        calib_slice = [slice(str(start_year),\n",
    "                             str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)))),\n",
    "                       slice(str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year) + 1)),\n",
    "                             str(end_year))]\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)) + 1),\n",
    "                            str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'manual' and picked_training_years is not None:\n",
    "        calib_slice = [slice(str(start_year), str(end_year)) for (start_year, end_year) in picked_training_years]\n",
    "        valid_slice = [slice(start_year, end_year) for ((_, start_year), (end_year, _)) in\n",
    "                       zip([(None, '1989-10-1'), ] + picked_training_years,\n",
    "                           picked_training_years + [('2020-9-30', None), ])]\n",
    "    else:\n",
    "        raise Exception('Unknown data splitting method')\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (x_extra_train, y_extra_train), (x_extra_test, y_extra_test) = \\\n",
    "        annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                      [dfouts],\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays, window_size=window_size, nwindows=nwindows,\n",
    "                                      )\n",
    "\n",
    "    train_X = pd.concat([train_X, x_extra_train], axis=0)\n",
    "    train_Y = pd.concat([train_Y, y_extra_train], axis=0)\n",
    "    test_X = pd.concat([test_X, x_extra_test], axis=0)\n",
    "    test_Y = pd.concat([test_Y, y_extra_test], axis=0)\n",
    "\n",
    "print(\"Done\")\n",
    "# takes about 2minutes...\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Colab minus 2015\n",
    "This is how the Colab notebook builds the datasets. But without the year 2015\n",
    "\n",
    "Name experiment: colab_wo2015\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:00<00:05,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:01<00:05,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:02<00:04,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:03<00:04,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:05<00:03,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:06<00:02,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:07<00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n",
      " 25%|██▌       | 1/4 [00:00<00:02,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:01<00:01,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:02<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selecting 10803 samples for training, 0 for test\n",
      "Randomly selecting 10803 samples for training, 0 for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.02s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"colab_wo2015\"\n",
    "if not os.path.exists(\"Experiments/\" + experiment_name):\n",
    "    os.mkdir(\"Experiments/\" + experiment_name)\n",
    "\n",
    "picked_training_years = [('1990-10-1','1991-9-30'),\n",
    "                         ('1992-10-1','1995-9-30'),\n",
    "                         ('1996-10-1','1998-9-30'),\n",
    "                         ('1999-10-1','2003-9-30'),\n",
    "                         ('2004-10-1','2006-9-30'),\n",
    "                         ('2007-10-1','2010-9-30'),\n",
    "                         ('2011-10-1','2013-9-30'),\n",
    "                         ('2015-10-1','2016-9-30'),\n",
    "                         ('2017-10-1','2019-9-30'),]\n",
    "\n",
    "train_data = [\"dsm2_ann_inputs_rsacminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsacplus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanminus20pct.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus15day.xlsx\",\n",
    "              \"dsm2_ann_inputs_rsanplus20pct.xlsx\",\n",
    "              ]\n",
    "\n",
    "test_data = {'dcc0': \"dsm2_ann_inputs_dcc0.xlsx\",\n",
    "             'smscg1': \"dsm2_ann_inputs_smscg1.xlsx\",\n",
    "             'dcc1': \"dsm2_ann_inputs_dcc1.xlsx\",\n",
    "             'smscg0': \"dsm2_ann_inputs_smscg0.xlsx\"}\n",
    "\n",
    "extra_data = {'observed': \"observed_data_daily.xlsx\"}\n",
    "which_part_for_test = 'manual'\n",
    "extra_data_test_ratio = 0.3\n",
    "\n",
    "ndays = 118\n",
    "window_size = 0\n",
    "nwindows = 0\n",
    "\n",
    "\n",
    "\n",
    "def read_training_data(train_data):\n",
    "    x_train = None\n",
    "    y_train = None\n",
    "    for data_file in tqdm(train_data):\n",
    "        data_path = os.path.join(local_root_path, data_file)\n",
    "        dfinps, dfouts = annutils.read_and_split(data_path, num_sheets, observed_stations_ordered_by_median)\n",
    "\n",
    "        # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "        if x_train is None:\n",
    "            (x_train, y_train), (_, _)  = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows  )\n",
    "\n",
    "\n",
    "        else:\n",
    "            (xc, yc), (_, _) = \\\n",
    "                annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                              [dfouts],\n",
    "                                              train_frac=1,\n",
    "                                              ndays=ndays, window_size=window_size, nwindows=nwindows )\n",
    "\n",
    "            x_train = pd.concat([x_train, xc], axis=0)\n",
    "            y_train = pd.concat([y_train, yc], axis=0)\n",
    "            del xc, yc\n",
    "    return x_train, y_train\n",
    "\n",
    "train_X, train_Y = read_training_data(train_data)\n",
    "test_X, test_Y = read_training_data(test_data.values())\n",
    "\n",
    "\n",
    "######### Read extra observed dataset ###############\n",
    "for data_file in tqdm(extra_data.values()):\n",
    "    data_path = os.path.join(local_root_path, data_file)\n",
    "\n",
    "    # print(\"Starting read_excel calls:\", data_path)\n",
    "    dflist = [annutils.read_excel_sheet(data_path, i) for i in range(num_sheets)]\n",
    "\n",
    "    df_inpout = pd.concat(dflist[0:num_sheets], axis=1).dropna(axis=0)\n",
    "    col_mask = df_inpout.columns.isin(dflist[num_sheets - 1].columns)\n",
    "    dfinps = df_inpout.loc[:, ~col_mask]\n",
    "    dfouts = df_inpout.loc[:, col_mask]\n",
    "    # dfouts = dfouts[output_stations]  # out_stations is None here...\n",
    "\n",
    "    start_year = max(dfinps.index[0].year, dfouts.index[0].year)\n",
    "    end_year = min(dfinps.index[-1].year, dfouts.index[-1].year)\n",
    "\n",
    "    if which_part_for_test == 'last':\n",
    "        calib_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "    elif which_part_for_test == 'first':\n",
    "        calib_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year)) + 1),\n",
    "                            str(end_year))\n",
    "        valid_slice = slice(str(start_year),\n",
    "                            str(int(start_year + (1 - extra_data_test_ratio) * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'middle':\n",
    "        calib_slice = [slice(str(start_year),\n",
    "                             str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)))),\n",
    "                       slice(str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year) + 1)),\n",
    "                             str(end_year))]\n",
    "        valid_slice = slice(str(int(start_year + (1 - extra_data_test_ratio) / 2 * (end_year - start_year)) + 1),\n",
    "                            str(int(start_year + (1 + extra_data_test_ratio) / 2 * (end_year - start_year))))\n",
    "    elif which_part_for_test == 'manual' and picked_training_years is not None:\n",
    "        calib_slice = [slice(str(start_year), str(end_year)) for (start_year, end_year) in picked_training_years]\n",
    "        valid_slice = [slice(start_year, end_year) for ((_, start_year), (end_year, _)) in\n",
    "                       zip([(None, '1989-10-1'), ] + picked_training_years,\n",
    "                           picked_training_years + [('2020-9-30', None), ])]\n",
    "    else:\n",
    "        raise Exception('Unknown data splitting method')\n",
    "\n",
    "    # create tuple of calibration and validation sets and the xscaler and yscaler on the combined inputs\n",
    "    (x_extra_train, y_extra_train), (x_extra_test, y_extra_test) = \\\n",
    "        annutils.create_training_sets_no_scaling([dfinps],\n",
    "                                      [dfouts],\n",
    "                                      calib_slice=calib_slice,\n",
    "                                      valid_slice=valid_slice,\n",
    "                                      ndays=ndays, window_size=window_size, nwindows=nwindows,\n",
    "                                      )\n",
    "\n",
    "    train_X = pd.concat([train_X, x_extra_train], axis=0)\n",
    "    train_Y = pd.concat([train_Y, y_extra_train], axis=0)\n",
    "    test_X = pd.concat([test_X, x_extra_test], axis=0)\n",
    "    test_Y = pd.concat([test_Y, y_extra_test], axis=0)\n",
    "\n",
    "print(\"Done\")\n",
    "# takes about 2minutes...\n",
    "train_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_X.csv\"), compression=compression_opts)\n",
    "train_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"train_Y.csv\"), compression=compression_opts)\n",
    "test_X.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_X.csv\"), compression=compression_opts)\n",
    "test_Y.to_csv(os.path.join(\"Experiments\", experiment_name, \"test_Y.csv\"), compression=compression_opts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
